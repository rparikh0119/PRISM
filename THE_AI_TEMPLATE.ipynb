{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a17b7a-afb4-4b3c-9e7c-eb3fb1b17522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA COMPATIBILITY CONFIGURATION\n",
      "============================================================\n",
      "✓ CUDA environment variables configured\n",
      "✓ Warning filters applied\n",
      "\n",
      "IMPORTANT: Do not skip this cell or move it!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA COMPATIBILITY CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Critical: Set CUDA environment variables BEFORE importing torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA operations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # Memory management\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'  # Disable device-side assertions\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"✓ CUDA environment variables configured\")\n",
    "print(\"✓ Warning filters applied\")\n",
    "print(\"\\nIMPORTANT: Do not skip this cell or move it!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0203ae1a-38bb-4524-8dbc-fe1ee06bb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING CUDA-COMPATIBLE PYTORCH\n",
      "============================================================\n",
      "\n",
      "1. Removing old PyTorch installations...\n",
      "Found existing installation: torch 2.7.1\n",
      "Uninstalling torch-2.7.1:\n",
      "  Successfully uninstalled torch-2.7.1\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "2. Installing PyTorch with CUDA 12.8 support...\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251016%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251016%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.8.0.dev20251016%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.24 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nvshmem_cu12-3.3.24-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting pytorch-triton==3.5.0+git7416ffcb (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.5.0%2Bgit7416ffcb-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251016%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (905.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.2/905.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nvshmem_cu12-3.3.24-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.5.0%2Bgit7416ffcb-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251016%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.8.0.dev20251016%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "Successfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.24 nvidia-nvtx-cu12-12.8.90 pytorch-triton-3.5.0+git7416ffcb torch-2.10.0.dev20251016+cu128 torchaudio-2.8.0.dev20251016+cu128 torchvision-0.25.0.dev20251016+cu128\n",
      "\n",
      "✓ PyTorch installation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: INSTALL/UPDATE CUDA-COMPATIBLE PYTORCH\n",
    "# Install PyTorch with CUDA 12.8 support for Blackwell GPUs\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING CUDA-COMPATIBLE PYTORCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uninstall existing PyTorch versions\n",
    "print(\"\\n1. Removing old PyTorch installations...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install PyTorch nightly with CUDA 12.8 (supports Blackwell sm_120)\n",
    "print(\"\\n2. Installing PyTorch with CUDA 12.8 support...\")\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "print(\"\\n✓ PyTorch installation complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4e4221-b897-4930-829a-52c595581f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMPORTING CORE AI LIBRARIES\n",
      "============================================================\n",
      "✓ Core libraries imported successfully\n",
      "✓ PyTorch configured for NVIDIA Blackwell GPU\n",
      "✓ PyTorch version: 2.10.0.dev20251016+cu128\n",
      "✓ NumPy version: 1.26.4\n",
      "✓ Pandas version: 2.2.3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTING CORE AI LIBRARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    print(\"✓ Core libraries imported successfully\")\n",
    "    \n",
    "    # Configure PyTorch for Blackwell GPU stability\n",
    "    if torch.cuda.is_available():\n",
    "        # Disable TF32 for better Blackwell compatibility\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "        \n",
    "        # Disable benchmark mode for deterministic behavior\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"✓ PyTorch configured for NVIDIA Blackwell GPU\")\n",
    "    else:\n",
    "        print(\"ℹ️ No GPU detected - running in CPU mode\")\n",
    "    \n",
    "    print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"✓ NumPy version: {np.__version__}\")\n",
    "    print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify Cell 2 completed successfully\")\n",
    "    print(\"2. Restart kernel: Kernel → Restart Kernel\")\n",
    "    print(\"3. Re-run from Cell 1\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05ba40f-c185-42fd-863e-20f4688d7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GPU COMPREHENSIVE TESTING\n",
      "============================================================\n",
      "\n",
      "1. Testing CUDA availability...\n",
      "✓ CUDA is available\n",
      "\n",
      "2. GPU Hardware Information:\n",
      "  • Device name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n",
      "  • Device count: 1\n",
      "  • Current device: 0\n",
      "  • Compute capability: 12.0\n",
      "  ✓ Blackwell architecture detected (sm_120)\n",
      "\n",
      "3. GPU Memory:\n",
      "  • Total memory: 95.59 GB\n",
      "  • Allocated: 0.00 GB\n",
      "  • Reserved: 0.00 GB\n",
      "  • Available: 95.59 GB\n",
      "\n",
      "4. Testing basic GPU operations...\n",
      "  ✓ Matrix multiplication successful\n",
      "\n",
      "5. Testing advanced GPU operations...\n",
      "  ✓ Softmax successful\n",
      "  ✓ Convolution successful\n",
      "\n",
      "============================================================\n",
      "GPU TEST SUMMARY\n",
      "============================================================\n",
      "✓ GPU detected and functional\n",
      "✓ Ready for AI model training and inference\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU COMPREHENSIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_gpu():\n",
    "    \"\"\"Comprehensive GPU testing with detailed diagnostics\"\"\"\n",
    "    \n",
    "    # Test 1: CUDA Availability\n",
    "    print(\"\\n1. Testing CUDA availability...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"❌ CUDA not available\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  • GPU drivers not installed (requires 528.89+)\")\n",
    "        print(\"  • CUDA toolkit missing\")\n",
    "        print(\"  • GPU hardware not detected\")\n",
    "        print(\"\\nYou can continue in CPU mode, but training will be slower.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✓ CUDA is available\")\n",
    "    \n",
    "    # Test 2: GPU Information\n",
    "    print(\"\\n2. GPU Hardware Information:\")\n",
    "    print(f\"  • Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  • Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  • Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Test 3: Compute Capability\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"  • Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] >= 12:  # Blackwell is sm_120+\n",
    "        print(\"  ✓ Blackwell architecture detected (sm_120)\")\n",
    "    elif capability[0] >= 9:\n",
    "        print(\"  ✓ Hopper/Ada Lovelace architecture\")\n",
    "    elif capability[0] >= 8:\n",
    "        print(\"  ✓ Ampere architecture\")\n",
    "    else:\n",
    "        print(f\"  ⚠️ Older GPU architecture (sm_{capability[0]}{capability[1]})\")\n",
    "    \n",
    "    # Test 4: Memory\n",
    "    print(\"\\n3. GPU Memory:\")\n",
    "    try:\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "        \n",
    "        print(f\"  • Total memory: {total_memory:.2f} GB\")\n",
    "        print(f\"  • Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  • Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  • Available: {total_memory - reserved:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not read memory info: {e}\")\n",
    "    \n",
    "    # Test 5: Basic Operations\n",
    "    print(\"\\n4. Testing basic GPU operations...\")\n",
    "    try:\n",
    "        # Simple matrix multiplication\n",
    "        x = torch.randn(1000, 1000, device='cuda')\n",
    "        y = torch.randn(1000, 1000, device='cuda')\n",
    "        z = torch.matmul(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  ✓ Matrix multiplication successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, z\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ GPU operation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 6: Advanced Operations\n",
    "    print(\"\\n5. Testing advanced GPU operations...\")\n",
    "    try:\n",
    "        # Softmax\n",
    "        x = torch.randn(100, 100, device='cuda')\n",
    "        y = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        # Convolution\n",
    "        conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "        img = torch.randn(1, 3, 64, 64, device='cuda')\n",
    "        out = conv(img)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  ✓ Softmax successful\")\n",
    "        print(\"  ✓ Convolution successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, conv, img, out\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Advanced operations warning: {e}\")\n",
    "        print(\"  (This may not affect basic model training)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run GPU tests\n",
    "gpu_available = test_gpu()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if gpu_available:\n",
    "    print(\"✓ GPU detected and functional\")\n",
    "    print(\"✓ Ready for AI model training and inference\")\n",
    "else:\n",
    "    print(\"ℹ️ Running in CPU mode\")\n",
    "    print(\"• You can still develop and test models\")\n",
    "    print(\"• Training will be slower without GPU\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58546d14-4bbc-4fc6-8a6f-620df0307bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING AI FRAMEWORK DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "Installing packages (this may take 3-5 minutes)...\n",
      "\n",
      "Packages to install:\n",
      "  • mlflow\n",
      "  • tensorflow\n",
      "  • gradio\n",
      "  • transformers\n",
      "  • datasets\n",
      "  • accelerate\n",
      "  • safetensors\n",
      "\n",
      "✓ All framework dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING AI FRAMEWORK DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nInstalling packages (this may take 3-5 minutes)...\")\n",
    "\n",
    "# Core ML frameworks\n",
    "packages = [\n",
    "    \"mlflow\",           # Model registry and deployment\n",
    "    \"tensorflow\",       # TensorFlow support\n",
    "    \"gradio\",          # Web UI creation\n",
    "    \"transformers\",    # Hugging Face models\n",
    "    \"datasets\",        # Hugging Face datasets\n",
    "    \"accelerate\",      # Training optimization\n",
    "    \"safetensors\",     # Safe model serialization\n",
    "]\n",
    "\n",
    "print(\"\\nPackages to install:\")\n",
    "for pkg in packages:\n",
    "    print(f\"  • {pkg}\")\n",
    "\n",
    "# Uncomment to actually install (commented for safety in template)\n",
    "# for pkg in packages:\n",
    "#     !pip install -q {pkg}\n",
    "\n",
    "print(\"\\n✓ All framework dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6bdfcd-507e-412d-9b3d-8e4760f451e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING REGISTER_MODEL NOTEBOOK\n",
      "============================================================\n",
      "✓ Created: Register_Model.ipynb\n",
      "\n",
      "Next steps:\n",
      "1. Open Register_Model.ipynb\n",
      "2. Update configuration with your model details\n",
      "3. Run all cells to register your model\n",
      "4. Check HP AI Studio Deployments tab\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING REGISTER_MODEL NOTEBOOK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def create_register_notebook():\n",
    "    \"\"\"Create Register_Model.ipynb for MLflow model registration\"\"\"\n",
    "    \n",
    "    notebook = {\n",
    "        \"cells\": [],\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\n",
    "                \"display_name\": \"Python 3\",\n",
    "                \"language\": \"python\",\n",
    "                \"name\": \"python3\"\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"name\": \"python\",\n",
    "                \"version\": \"3.10.0\"\n",
    "            }\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 4\n",
    "    }\n",
    "    \n",
    "    # Cell 1: Instructions\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"markdown\",\n",
    "        \"metadata\": {},\n",
    "        \"source\": [\n",
    "            \"# Model Registration for HP AI Studio\\n\",\n",
    "            \"\\n\",\n",
    "            \"This notebook registers your trained model with MLflow for deployment in HP AI Studio.\\n\",\n",
    "            \"\\n\",\n",
    "            \"## Instructions:\\n\",\n",
    "            \"1. Update the configuration section with your model details\\n\",\n",
    "            \"2. Run all cells in order\\n\",\n",
    "            \"3. Verify model appears in HP AI Studio Deployments tab\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 2: Configuration\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Configuration - Update these values\\n\",\n",
    "            \"MODEL_NAME = 'my-ai-model'\\n\",\n",
    "            \"MODEL_VERSION = '1.0.0'\\n\",\n",
    "            \"MODEL_PATH = './models/my_model'\\n\",\n",
    "            \"MODEL_DESCRIPTION = 'Description of your AI model'\\n\",\n",
    "            \"MLFLOW_TRACKING_URI = './mlruns'\\n\",\n",
    "            \"EXPERIMENT_NAME = 'ai-560-student-projects'\\n\",\n",
    "            \"STUDENT_NAME = 'Your Name'\\n\",\n",
    "            \"PROJECT_TITLE = 'Your Project Title'\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Configuration loaded for: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Student: {STUDENT_NAME}')\\n\",\n",
    "            \"print(f'Project: {PROJECT_TITLE}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 3: Import libraries\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"import mlflow\\n\",\n",
    "            \"import mlflow.pyfunc\\n\",\n",
    "            \"from mlflow.models.signature import ModelSignature\\n\",\n",
    "            \"from mlflow.types.schema import Schema, ColSpec\\n\",\n",
    "            \"from mlflow.types import DataType\\n\",\n",
    "            \"import pandas as pd\\n\",\n",
    "            \"import torch\\n\",\n",
    "            \"from datetime import datetime\\n\",\n",
    "            \"import json\\n\",\n",
    "            \"from pathlib import Path\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Libraries imported successfully')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 4: Model wrapper class\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"class CustomModelWrapper(mlflow.pyfunc.PythonModel):\\n\",\n",
    "            \"    \\\"\\\"\\\"Wrapper class for MLflow model deployment\\\"\\\"\\\"\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def load_context(self, context):\\n\",\n",
    "            \"        \\\"\\\"\\\"Load model and dependencies\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your model loading code here\\n\",\n",
    "            \"        # Example: self.model = torch.load(context.artifacts['model_path'])\\n\",\n",
    "            \"        print('Model loaded successfully')\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def predict(self, context, model_input):\\n\",\n",
    "            \"        \\\"\\\"\\\"Run inference\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your prediction code here\\n\",\n",
    "            \"        # Example: return self.model(model_input)\\n\",\n",
    "            \"        return {'output': 'Model prediction would go here'}\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model wrapper class defined')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 5: Define signature\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Define model signature\\n\",\n",
    "            \"input_schema = Schema([ColSpec(DataType.string, 'input')])\\n\",\n",
    "            \"output_schema = Schema([ColSpec(DataType.string, 'output')])\\n\",\n",
    "            \"signature = ModelSignature(inputs=input_schema, outputs=output_schema)\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Create example input\\n\",\n",
    "            \"input_example = pd.DataFrame({'input': ['example input data']})\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model signature defined')\\n\",\n",
    "            \"print(f'Input schema: {input_schema}')\\n\",\n",
    "            \"print(f'Output schema: {output_schema}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 6: Register model\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Set MLflow tracking\\n\",\n",
    "            \"mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\\n\",\n",
    "            \"mlflow.set_experiment(EXPERIMENT_NAME)\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Registering model: {MODEL_NAME}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Start MLflow run\\n\",\n",
    "            \"with mlflow.start_run(run_name=f\\\"{MODEL_NAME}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\\\") as run:\\n\",\n",
    "            \"    # Log parameters\\n\",\n",
    "            \"    mlflow.log_param('model_version', MODEL_VERSION)\\n\",\n",
    "            \"    mlflow.log_param('student_name', STUDENT_NAME)\\n\",\n",
    "            \"    mlflow.log_param('project_title', PROJECT_TITLE)\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    # Log model\\n\",\n",
    "            \"    mlflow.pyfunc.log_model(\\n\",\n",
    "            \"        artifact_path='model',\\n\",\n",
    "            \"        python_model=CustomModelWrapper(),\\n\",\n",
    "            \"        signature=signature,\\n\",\n",
    "            \"        input_example=input_example,\\n\",\n",
    "            \"        registered_model_name=MODEL_NAME\\n\",\n",
    "            \"    )\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    print(f'✓ Model registered: {MODEL_NAME}')\\n\",\n",
    "            \"    print(f'✓ Run ID: {run.info.run_id}')\\n\",\n",
    "            \"    print(f'✓ Check HP AI Studio Deployments tab')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 7: Verification\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Verify registration\\n\",\n",
    "            \"client = mlflow.tracking.MlflowClient()\\n\",\n",
    "            \"model_versions = client.search_model_versions(f\\\"name='{MODEL_NAME}'\\\")\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Model: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Versions registered: {len(model_versions)}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"for mv in model_versions:\\n\",\n",
    "            \"    print(f\\\"\\\\nVersion: {mv.version}\\\")\\n\",\n",
    "            \"    print(f\\\"Stage: {mv.current_stage}\\\")\\n\",\n",
    "            \"    print(f\\\"Status: {mv.status}\\\")\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save notebook\n",
    "    notebook_path = Path(\"Register_Model.ipynb\")\n",
    "    with open(notebook_path, 'w') as f:\n",
    "        json.dump(notebook, f, indent=2)\n",
    "    \n",
    "    return notebook_path\n",
    "\n",
    "# Create the notebook\n",
    "try:\n",
    "    notebook_path = create_register_notebook()\n",
    "    print(f\"✓ Created: {notebook_path}\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Open Register_Model.ipynb\")\n",
    "    print(\"2. Update configuration with your model details\")\n",
    "    print(\"3. Run all cells to register your model\")\n",
    "    print(\"4. Check HP AI Studio Deployments tab\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating notebook: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2a60c9-4ea5-4721-aa2b-aa450163f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HUGGING FACE AUTHENTICATION\n",
      "============================================================\n",
      "\n",
      "Why authenticate with Hugging Face?\n",
      "  • Access to 500,000+ pre-trained models\n",
      "  • Download datasets for training\n",
      "  • Use gated models (Llama, Stable Diffusion, etc.)\n",
      "  • Share your trained models (optional)\n",
      "\n",
      "• No existing Hugging Face login found\n",
      "\n",
      "------------------------------------------------------------\n",
      "HOW TO GET YOUR HUGGING FACE TOKEN:\n",
      "------------------------------------------------------------\n",
      "1. Go to: https://huggingface.co/settings/tokens\n",
      "2. Click 'Create new token'\n",
      "3. Name it: 'HP-AI-Studio-Student'\n",
      "4. Select: 'Read' access (or 'Write' if you'll publish models)\n",
      "5. Click 'Create token'\n",
      "6. Copy the token (it looks like: hf_xxxxxxxxxxxxxxxxxxxxx)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to authenticate now? (y/n):  y\n",
      "\n",
      "Paste your Hugging Face token here:  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\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Authenticating...\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "\n",
      "✓ Successfully authenticated as: Riya119\n",
      "✓ You can now access Hugging Face models and datasets\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HUGGING FACE AUTHENTICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def authenticate_huggingface():\n",
    "    \"\"\"Interactive Hugging Face authentication\"\"\"\n",
    "    \n",
    "    print(\"\\nWhy authenticate with Hugging Face?\")\n",
    "    print(\"  • Access to 500,000+ pre-trained models\")\n",
    "    print(\"  • Download datasets for training\")\n",
    "    print(\"  • Use gated models (Llama, Stable Diffusion, etc.)\")\n",
    "    print(\"  • Share your trained models (optional)\")\n",
    "    \n",
    "    # Check if already authenticated\n",
    "    try:\n",
    "        from huggingface_hub import whoami\n",
    "        user_info = whoami()\n",
    "        print(f\"\\n✓ Already logged in as: {user_info['name']}\")\n",
    "        response = input(\"\\nContinue with this account? (y/n): \").lower()\n",
    "        if response == 'y':\n",
    "            print(\"✓ Using existing authentication\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"\\n• No existing Hugging Face login found\")\n",
    "    \n",
    "    # Get authentication token\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"HOW TO GET YOUR HUGGING FACE TOKEN:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"1. Go to: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Click 'Create new token'\")\n",
    "    print(\"3. Name it: 'HP-AI-Studio-Student'\")\n",
    "    print(\"4. Select: 'Read' access (or 'Write' if you'll publish models)\")\n",
    "    print(\"5. Click 'Create token'\")\n",
    "    print(\"6. Copy the token (it looks like: hf_xxxxxxxxxxxxxxxxxxxxx)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    choice = input(\"\\nDo you want to authenticate now? (y/n): \").lower()\n",
    "    \n",
    "    if choice == 'y':\n",
    "        try:\n",
    "            # Import login function\n",
    "            from huggingface_hub import login\n",
    "            \n",
    "            # Get token from user\n",
    "            token = input(\"\\nPaste your Hugging Face token here: \").strip()\n",
    "            \n",
    "            # Validate token format\n",
    "            if not token.startswith('hf_'):\n",
    "                print(\"\\n⚠️ Warning: Token should start with 'hf_'\")\n",
    "                confirm = input(\"Continue anyway? (y/n): \").lower()\n",
    "                if confirm != 'y':\n",
    "                    print(\"Authentication cancelled\")\n",
    "                    return False\n",
    "            \n",
    "            # Attempt login\n",
    "            print(\"\\nAuthenticating...\")\n",
    "            login(token=token, add_to_git_credential=True)\n",
    "            \n",
    "            # Verify authentication\n",
    "            from huggingface_hub import whoami\n",
    "            user_info = whoami()\n",
    "            \n",
    "            print(f\"\\n✓ Successfully authenticated as: {user_info['name']}\")\n",
    "            print(\"✓ You can now access Hugging Face models and datasets\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Authentication failed: {e}\")\n",
    "            print(\"\\nTroubleshooting:\")\n",
    "            print(\"  1. Verify token is correct\")\n",
    "            print(\"  2. Check token has required permissions\")\n",
    "            print(\"  3. Try creating a new token\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"\\nℹ️ Skipping authentication\")\n",
    "        print(\"You can authenticate later by running:\")\n",
    "        print(\"  from huggingface_hub import login\")\n",
    "        print(\"  login()\")\n",
    "        return False\n",
    "\n",
    "# Run authentication\n",
    "hf_authenticated = authenticate_huggingface()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf5151d-fe33-4593-8c22-c6d321d31965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎉 SETUP COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Your HP AI Studio environment is configured and ready.\n",
      "All core dependencies are installed and tested.\n",
      "\n",
      "✓ GPU: Detected and functional\n",
      "✓ Hugging Face: Authenticated\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS FOR YOUR AI PROJECT:\n",
      "============================================================\n",
      "\n",
      "1. DEVELOP YOUR MODEL\n",
      "   - Load datasets using Hugging Face datasets library\n",
      "   - Fine-tune models or train from scratch\n",
      "   - Test and evaluate your model performance\n",
      "\n",
      "2. SAVE YOUR MODEL\n",
      "   - Use torch.save() for PyTorch models\n",
      "   - Save tokenizers and configurations\n",
      "   - Document model architecture and parameters\n",
      "\n",
      "3. REGISTER FOR DEPLOYMENT\n",
      "   - Open Register_Model.ipynb\n",
      "   - Update configuration with your model details\n",
      "   - Run all cells to register with MLflow\n",
      "   - Check HP AI Studio Deployments tab\n",
      "\n",
      "4. CREATE YOUR INTERFACE\n",
      "   - Use Gradio for interactive UIs\n",
      "   - Build REST APIs with FastAPI\n",
      "   - Integrate with existing applications\n",
      "\n",
      "5. DOCUMENT YOUR WORK\n",
      "   - Keep a development journal\n",
      "   - Screenshot important results\n",
      "   - Record process and iterations\n",
      "   - Prepare portfolio presentation\n",
      "\n",
      "============================================================\n",
      "HELPFUL RESOURCES:\n",
      "============================================================\n",
      "  • HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\n",
      "  • Hugging Face: https://huggingface.co/\n",
      "  • MLflow Documentation: https://mlflow.org/docs/latest/\n",
      "  • PyTorch Tutorials: https://pytorch.org/tutorials/\n",
      "  • Gradio Documentation: https://gradio.app/docs/\n",
      "\n",
      "============================================================\n",
      "REMEMBER:\n",
      "============================================================\n",
      "  • Save your work frequently (Ctrl+S)\n",
      "  • Document your process in your project journal\n",
      "  • Test on small datasets before full training\n",
      "  • Ask for help in office hours if needed\n",
      "  • Clear GPU memory: torch.cuda.empty_cache()\n",
      "\n",
      "✓ You're ready to begin your AI project!\n",
      "  Good luck with your creative AI development!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYour HP AI Studio environment is configured and ready.\")\n",
    "print(\"All core dependencies are installed and tested.\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(\"\\n✓ GPU: Detected and functional\")\n",
    "else:\n",
    "    print(\"\\nℹ️ GPU: Not detected (using CPU mode)\")\n",
    "\n",
    "if hf_authenticated:\n",
    "    print(\"✓ Hugging Face: Authenticated\")\n",
    "else:\n",
    "    print(\"ℹ️ Hugging Face: Not authenticated (optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS FOR YOUR AI PROJECT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. DEVELOP YOUR MODEL\")\n",
    "print(\"   - Load datasets using Hugging Face datasets library\")\n",
    "print(\"   - Fine-tune models or train from scratch\")\n",
    "print(\"   - Test and evaluate your model performance\")\n",
    "\n",
    "print(\"\\n2. SAVE YOUR MODEL\")\n",
    "print(\"   - Use torch.save() for PyTorch models\")\n",
    "print(\"   - Save tokenizers and configurations\")\n",
    "print(\"   - Document model architecture and parameters\")\n",
    "\n",
    "print(\"\\n3. REGISTER FOR DEPLOYMENT\")\n",
    "print(\"   - Open Register_Model.ipynb\")\n",
    "print(\"   - Update configuration with your model details\")\n",
    "print(\"   - Run all cells to register with MLflow\")\n",
    "print(\"   - Check HP AI Studio Deployments tab\")\n",
    "\n",
    "print(\"\\n4. CREATE YOUR INTERFACE\")\n",
    "print(\"   - Use Gradio for interactive UIs\")\n",
    "print(\"   - Build REST APIs with FastAPI\")\n",
    "print(\"   - Integrate with existing applications\")\n",
    "\n",
    "print(\"\\n5. DOCUMENT YOUR WORK\")\n",
    "print(\"   - Keep a development journal\")\n",
    "print(\"   - Screenshot important results\")\n",
    "print(\"   - Record process and iterations\")\n",
    "print(\"   - Prepare portfolio presentation\")\n",
    "\n",
    "if not hf_authenticated:\n",
    "    print(\"\\n⚠️ RECOMMENDATION:\")\n",
    "    print(\"   Run Cell 7 again to set up Hugging Face authentication\")\n",
    "    print(\"   This will give you access to more models and datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HELPFUL RESOURCES:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  • HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\")\n",
    "print(\"  • Hugging Face: https://huggingface.co/\")\n",
    "print(\"  • MLflow Documentation: https://mlflow.org/docs/latest/\")\n",
    "print(\"  • PyTorch Tutorials: https://pytorch.org/tutorials/\")\n",
    "print(\"  • Gradio Documentation: https://gradio.app/docs/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REMEMBER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  • Save your work frequently (Ctrl+S)\")\n",
    "print(\"  • Document your process in your project journal\")\n",
    "print(\"  • Test on small datasets before full training\")\n",
    "print(\"  • Ask for help in office hours if needed\")\n",
    "print(\"  • Clear GPU memory: torch.cuda.empty_cache()\")\n",
    "\n",
    "print(\"\\n✓ You're ready to begin your AI project!\")\n",
    "print(\"  Good luck with your creative AI development!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38510d87-3557-4279-88eb-1ad092ffff87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
