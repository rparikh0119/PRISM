{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138bfa62-d37d-461b-b38f-f58b4348d3b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 20) (3674714900.py, line 20)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mHere's how it will flow:\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 20)\n"
     ]
    }
   ],
   "source": [
    "# In your NEW Brain AI notebook:\n",
    "\n",
    "# Load the training data we just created\n",
    "import json\n",
    "\n",
    "with open('../prism_dataset_info.json', 'r') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# Load actual training data\n",
    "with open('./synthetic_data/full_10k/training_data.json', 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} boards for training Brain AI\")\n",
    "\n",
    "# Then build your Brain AI that learns from this data...\n",
    "```\n",
    "\n",
    "## Step 3: FigJam Integration Architecture\n",
    "\n",
    "Here's how it will flow:\n",
    "```\n",
    "CURRENT WORKBOOK (Data Generator)\n",
    "‚îî‚îÄ‚îÄ Generates synthetic training data\n",
    "    ‚îî‚îÄ‚îÄ Saves to: ./synthetic_data/\n",
    "\n",
    "NEXT WORKBOOK (Brain AI - Organizer/Analyzer)  \n",
    "‚îú‚îÄ‚îÄ Loads synthetic data for training\n",
    "‚îú‚îÄ‚îÄ Learns pattern recognition\n",
    "‚îú‚îÄ‚îÄ Tests on synthetic boards\n",
    "‚îî‚îÄ‚îÄ Exports trained model\n",
    "\n",
    "FINAL WORKBOOK (UI + FigJam Integration)\n",
    "‚îú‚îÄ‚îÄ Imports trained Brain AI model\n",
    "‚îú‚îÄ‚îÄ Connects to FigJam API (reads real boards)\n",
    "‚îú‚îÄ‚îÄ Brain AI processes real FigJam data\n",
    "‚îî‚îÄ‚îÄ Outputs organized insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7c1dc-34ed-4007-b45b-f0ee300b8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../prism_dataset_info.json', 'r') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# Load actual training data\n",
    "with open('./synthetic_data/full_10k/training_data.json', 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} boards for training Brain AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7144e-afbf-4d47-8351-54dd239b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data directly (no metadata file needed)\n",
    "import json\n",
    "\n",
    "# Load the synthetic data we generated\n",
    "try:\n",
    "    with open('./synthetic_data/test_100/training_data.json', 'r') as f:\n",
    "        training_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded training data!\")\n",
    "    print(f\"üìä Total boards: {len(training_data)}\")\n",
    "    print(f\"üìù Total notes: {sum(board['total_notes'] for board in training_data):,}\")\n",
    "    \n",
    "    # Show data structure\n",
    "    print(f\"\\nüìã First board structure:\")\n",
    "    print(f\"   Board ID: {training_data[0]['board_id']}\")\n",
    "    print(f\"   Persona: {training_data[0]['persona']}\")\n",
    "    print(f\"   Notes: {training_data[0]['total_notes']}\")\n",
    "    print(f\"\\nüìù Each note contains:\")\n",
    "    for key in training_data[0]['notes'][0].keys():\n",
    "        print(f\"   - {key}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data loaded successfully! Ready to build Brain AI.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Training data not found!\")\n",
    "    print(\"üìç Make sure you're in the same directory as your generator notebook\")\n",
    "    print(\"üí° Or update the path to where your synthetic_data folder is located\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ed2ed-7f39-43af-81da-6c2e7f8acef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check current directory\n",
    "print(f\"üìç Current directory: {os.getcwd()}\")\n",
    "print(f\"\\nüìÅ Files and folders here:\")\n",
    "for item in os.listdir('.'):\n",
    "    print(f\"   - {item}\")\n",
    "\n",
    "# Try to find synthetic_data folder\n",
    "print(f\"\\nüîç Searching for synthetic_data folder...\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    if 'synthetic_data' in dirs:\n",
    "        print(f\"   ‚úÖ Found at: {root}/synthetic_data\")\n",
    "    if 'training_data.json' in files:\n",
    "        print(f\"   ‚úÖ Found training_data.json at: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d487c-b6cb-4c5e-9ffa-35d85bf9ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from DeepLearning folder\n",
    "import json\n",
    "\n",
    "data_path = './DeepLearning/synthetic_data/test_100/training_data.json'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded training data!\")\n",
    "print(f\"üìä Total boards: {len(training_data)}\")\n",
    "print(f\"üìù Total notes: {sum(board['total_notes'] for board in training_data):,}\")\n",
    "\n",
    "# Show data structure\n",
    "print(f\"\\nüìã Sample board:\")\n",
    "print(f\"   Board ID: {training_data[0]['board_id']}\")\n",
    "print(f\"   Persona: {training_data[0]['persona']}\")\n",
    "print(f\"   Notes: {training_data[0]['total_notes']}\")\n",
    "\n",
    "print(f\"\\nüìù Sample note structure:\")\n",
    "sample_note = training_data[0]['notes'][0]\n",
    "for key, value in sample_note.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to build Brain AI with this data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b3751-8d0c-43d4-8b8b-c6819fe3972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from DeepLearning folder\n",
    "import json\n",
    "\n",
    "data_path = './DeepLearning/synthetic_data/full_10k/training_data.json'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded training data!\")\n",
    "print(f\"üìä Total boards: {len(training_data)}\")\n",
    "print(f\"üìù Total notes: {sum(board['total_notes'] for board in training_data):,}\")\n",
    "\n",
    "# Show data structure\n",
    "print(f\"\\nüìã Sample board:\")\n",
    "print(f\"   Board ID: {training_data[0]['board_id']}\")\n",
    "print(f\"   Persona: {training_data[0]['persona']}\")\n",
    "print(f\"   Notes: {training_data[0]['total_notes']}\")\n",
    "\n",
    "print(f\"\\nüìù Sample note structure:\")\n",
    "sample_note = training_data[0]['notes'][0]\n",
    "for key, value in sample_note.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to build Brain AI with this data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ebe930-79bc-4d5d-a9ee-f9da09e86b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PRISM BRAIN AI - Content Analyzer & Organizer\n",
    "# Learns to understand sticky notes regardless of color\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class PRISMBrainAI:\n",
    "    \"\"\"\n",
    "    AI that learns patterns from synthetic data to analyze real FigJam boards\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data):\n",
    "        self.training_data = training_data\n",
    "        self.patterns = {}\n",
    "        print(\"üß† PRISM Brain AI initialized\")\n",
    "        print(f\"üìö Training on {len(training_data)} boards\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Learn patterns from the synthetic training data\"\"\"\n",
    "        print(\"\\nüéì Training Brain AI...\")\n",
    "        \n",
    "        # Collect all notes\n",
    "        all_notes = []\n",
    "        for board in self.training_data:\n",
    "            all_notes.extend(board['notes'])\n",
    "        \n",
    "        print(f\"   Analyzing {len(all_notes):,} sticky notes...\")\n",
    "        \n",
    "        # Learn color-content relationships\n",
    "        self.patterns['color_accuracy'] = self._analyze_color_patterns(all_notes)\n",
    "        \n",
    "        # Learn content type distributions\n",
    "        self.patterns['content_types'] = self._analyze_content_types(all_notes)\n",
    "        \n",
    "        # Learn persona behaviors\n",
    "        self.patterns['personas'] = self._analyze_personas()\n",
    "        \n",
    "        # Learn keyword patterns for each content type\n",
    "        self.patterns['keywords'] = self._learn_keywords(all_notes)\n",
    "        \n",
    "        print(\"   ‚úÖ Training complete!\")\n",
    "        self._print_learned_patterns()\n",
    "    \n",
    "    def _analyze_color_patterns(self, notes):\n",
    "        \"\"\"Learn how often colors match their typical meanings\"\"\"\n",
    "        color_stats = defaultdict(lambda: {'total': 0, 'matches': 0})\n",
    "        \n",
    "        for note in notes:\n",
    "            color = note['color']\n",
    "            color_stats[color]['total'] += 1\n",
    "            if note['color_matches_meaning']:\n",
    "                color_stats[color]['matches'] += 1\n",
    "        \n",
    "        # Calculate accuracy per color\n",
    "        accuracy = {}\n",
    "        for color, stats in color_stats.items():\n",
    "            accuracy[color] = stats['matches'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def _analyze_content_types(self, notes):\n",
    "        \"\"\"Learn distribution of content types\"\"\"\n",
    "        types = [note['true_type'] for note in notes]\n",
    "        return dict(Counter(types))\n",
    "    \n",
    "    def _analyze_personas(self):\n",
    "        \"\"\"Learn persona characteristics\"\"\"\n",
    "        persona_stats = defaultdict(lambda: {'boards': 0, 'atypical': 0})\n",
    "        \n",
    "        for board in self.training_data:\n",
    "            persona = board['persona']\n",
    "            persona_stats[persona]['boards'] += 1\n",
    "            if board['is_atypical']:\n",
    "                persona_stats[persona]['atypical'] += 1\n",
    "        \n",
    "        return dict(persona_stats)\n",
    "    \n",
    "    def _learn_keywords(self, notes):\n",
    "        \"\"\"Learn keywords associated with each content type\"\"\"\n",
    "        keywords = defaultdict(list)\n",
    "        \n",
    "        for note in notes:\n",
    "            content_type = note['true_type']\n",
    "            words = note['content'].lower().split()\n",
    "            keywords[content_type].extend(words)\n",
    "        \n",
    "        # Get most common keywords per type\n",
    "        top_keywords = {}\n",
    "        for content_type, words in keywords.items():\n",
    "            top_keywords[content_type] = [word for word, count in Counter(words).most_common(10)]\n",
    "        \n",
    "        return top_keywords\n",
    "    \n",
    "    def _print_learned_patterns(self):\n",
    "        \"\"\"Display what the AI learned\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"üß† BRAIN AI LEARNED PATTERNS:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(\"\\nüìä Color Reliability (how often color matches meaning):\")\n",
    "        for color, accuracy in sorted(self.patterns['color_accuracy'].items(), \n",
    "                                     key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"   {color:12} ‚Üí {accuracy*100:.1f}% reliable\")\n",
    "        \n",
    "        print(\"\\nüìù Content Type Distribution:\")\n",
    "        for ctype, count in sorted(self.patterns['content_types'].items(), \n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "            pct = (count / sum(self.patterns['content_types'].values())) * 100\n",
    "            print(f\"   {ctype:12} ‚Üí {count:,} notes ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "    \n",
    "    def analyze_note(self, note_content, note_color):\n",
    "        \"\"\"\n",
    "        Analyze a single sticky note (like from a real FigJam board)\n",
    "        Returns: predicted content type and confidence\n",
    "        \"\"\"\n",
    "        # Simple keyword-based prediction for now\n",
    "        content_lower = note_content.lower()\n",
    "        \n",
    "        scores = {}\n",
    "        for content_type, keywords in self.patterns['keywords'].items():\n",
    "            score = sum(1 for keyword in keywords if keyword in content_lower)\n",
    "            scores[content_type] = score\n",
    "        \n",
    "        # Get prediction\n",
    "        if scores:\n",
    "            predicted_type = max(scores, key=scores.get)\n",
    "            confidence = scores[predicted_type] / 10  # Normalize to 0-1\n",
    "        else:\n",
    "            predicted_type = 'neutral'\n",
    "            confidence = 0.5\n",
    "        \n",
    "        # Adjust confidence based on color reliability\n",
    "        color_reliability = self.patterns['color_accuracy'].get(note_color, 0.5)\n",
    "        \n",
    "        return {\n",
    "            'predicted_type': predicted_type,\n",
    "            'confidence': min(confidence, 1.0),\n",
    "            'color_reliability': color_reliability,\n",
    "            'color_might_mislead': color_reliability < 0.6\n",
    "        }\n",
    "    \n",
    "    def analyze_board(self, board_notes):\n",
    "        \"\"\"\n",
    "        Analyze an entire FigJam board\n",
    "        board_notes: list of dicts with 'content' and 'color' keys\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for note in board_notes:\n",
    "            analysis = self.analyze_note(note['content'], note['color'])\n",
    "            results.append({\n",
    "                'original': note,\n",
    "                'analysis': analysis\n",
    "            })\n",
    "        \n",
    "        # Organize by type\n",
    "        organized = defaultdict(list)\n",
    "        for result in results:\n",
    "            note_type = result['analysis']['predicted_type']\n",
    "            organized[note_type].append(result['original'])\n",
    "        \n",
    "        return {\n",
    "            'total_notes': len(board_notes),\n",
    "            'organized_by_type': dict(organized),\n",
    "            'detailed_analysis': results\n",
    "        }\n",
    "\n",
    "# Initialize and train the Brain AI\n",
    "brain = PRISMBrainAI(training_data)\n",
    "brain.train()\n",
    "\n",
    "print(\"\\n‚úÖ Brain AI ready to analyze FigJam boards!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fff098-e5a4-49b9-988c-805f9a22ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# TEST THE BRAIN AI\n",
    "# =====================================================\n",
    "\n",
    "print(\"üß™ TESTING BRAIN AI ON SAMPLE NOTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get a test board (not used in training visualization)\n",
    "test_board = training_data[50]  # Random board\n",
    "\n",
    "print(f\"\\nüìã Testing on board: {test_board['board_id']}\")\n",
    "print(f\"üë§ Persona: {test_board['persona']}\")\n",
    "print(f\"üìù Notes: {test_board['total_notes']}\")\n",
    "print(f\"üé® Is atypical: {test_board['is_atypical']}\")\n",
    "\n",
    "# Test on first 5 notes\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANALYSIS RESULTS:\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for i, note in enumerate(test_board['notes'][:5], 1):\n",
    "    analysis = brain.analyze_note(note['content'], note['color'])\n",
    "    \n",
    "    # Compare prediction to ground truth\n",
    "    correct = \"‚úÖ\" if analysis['predicted_type'] == note['true_type'] else \"‚ùå\"\n",
    "    warning = \"‚ö†Ô∏è COLOR UNRELIABLE\" if analysis['color_might_mislead'] else \"\"\n",
    "    \n",
    "    print(f\"{i}. {correct} [{note['color']:8}] Content: \\\"{note['content'][:50]}...\\\"\")\n",
    "    print(f\"   True type: {note['true_type']:12} | Predicted: {analysis['predicted_type']:12}\")\n",
    "    print(f\"   Confidence: {analysis['confidence']:.2f} | Color reliability: {analysis['color_reliability']:.2f} {warning}\")\n",
    "    print()\n",
    "\n",
    "# Test full board organization\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FULL BOARD ORGANIZATION:\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "board_analysis = brain.analyze_board(test_board['notes'])\n",
    "\n",
    "print(f\"üìä Total notes analyzed: {board_analysis['total_notes']}\")\n",
    "print(f\"\\nüìÅ Organized by type:\")\n",
    "for note_type, notes in board_analysis['organized_by_type'].items():\n",
    "    print(f\"   {note_type:12} ‚Üí {len(notes)} notes\")\n",
    "\n",
    "print(f\"\\n‚úÖ Brain AI successfully organized {board_analysis['total_notes']} notes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bf302-451c-4128-a0d1-469d7423b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SAVE BRAIN AI SUMMARY FOR PORTFOLIO\n",
    "# =====================================================\n",
    "\n",
    "summary = {\n",
    "    'project': 'PRISM - Brain AI Content Analyzer',\n",
    "    'purpose': 'Analyze FigJam sticky notes regardless of color coding',\n",
    "    'training_data': {\n",
    "        'boards': len(training_data),\n",
    "        'total_notes': sum(board['total_notes'] for board in training_data),\n",
    "        'personas': 6,\n",
    "        'content_types': 6\n",
    "    },\n",
    "    'learned_patterns': {\n",
    "        'most_reliable_color': 'PURPLE (76.9%)',\n",
    "        'least_reliable_colors': 'BLUE/YELLOW (48.8%)',\n",
    "        'content_distribution': 'Balanced (16.6-16.7% each type)'\n",
    "    },\n",
    "    'capabilities': [\n",
    "        'Analyzes individual sticky notes',\n",
    "        'Predicts content type from text',\n",
    "        'Detects unreliable color usage',\n",
    "        'Organizes entire boards by type',\n",
    "        'Ready for FigJam API integration'\n",
    "    ],\n",
    "    'next_steps': [\n",
    "        'Upgrade to ML-based classifier (better accuracy)',\n",
    "        'Add FigJam API connection',\n",
    "        'Build UI for real-time analysis',\n",
    "        'Add timeline/contributor tracking'\n",
    "    ],\n",
    "    'current_accuracy': '40% (keyword-based, ready for ML upgrade)'\n",
    "}\n",
    "\n",
    "with open('./prism_brain_ai_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Summary saved to: prism_brain_ai_summary.json\")\n",
    "print(\"\\nüìã WHAT YOU'VE BUILT:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. ‚úÖ Generated 899,195 labeled training examples\")\n",
    "print(\"2. ‚úÖ Built Brain AI that learns color reliability patterns\")\n",
    "print(\"3. ‚úÖ AI can analyze and organize sticky notes by content\")\n",
    "print(\"4. ‚úÖ Detects when colors are misleading\")\n",
    "print(\"5. ‚úÖ Ready to connect to real FigJam boards\")\n",
    "print(\"\\nüéØ NEXT: Connect FigJam API and build UI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dc3e5-4e31-4084-836d-19631360cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FIGJAM API SETUP\n",
    "# =====================================================\n",
    "\n",
    "print(\"üîê FIGJAM API SETUP\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìã To get your FigJam access token:\")\n",
    "print(\"1. Go to: https://www.figma.com/developers/api#access-tokens\")\n",
    "print(\"2. Log in to your Figma account\")\n",
    "print(\"3. Click 'Get personal access token'\")\n",
    "print(\"4. Copy the token\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep your token secret - don't share it!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store your token here (you'll paste it after getting it)\n",
    "FIGJAM_TOKEN = \"\"  # Paste your token between the quotes\n",
    "\n",
    "if not FIGJAM_TOKEN:\n",
    "    print(\"\\n‚è∏Ô∏è  Waiting for you to add your FigJam token above...\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Token configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e6082-a67e-450d-832e-21fa9e43d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONFIGURE FIGJAM API WITH YOUR TOKEN\n",
    "# =====================================================\n",
    "\n",
    "# Paste your token here (keep it secret!)\n",
    "FIGJAM_TOKEN = figd_YP-yLbvxZ0jOVR9C54bCPveiHdkFB3uZD7hKQKDF\n",
    "\n",
    "# Initialize the FigJam connector\n",
    "connector = FigJamConnector(FIGJAM_TOKEN)\n",
    "\n",
    "print(\"‚úÖ FigJam connector configured!\")\n",
    "print(\"\\nüìã Next step: Get a FigJam file key to test with\")\n",
    "print(\"   1. Open any FigJam board\")\n",
    "print(\"   2. Copy the URL (looks like: figma.com/file/XXXXX/...)\")\n",
    "print(\"   3. The XXXXX part is your file key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593d5f9-3dcc-4b56-be54-361aeb43496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# TEST WITH MOCK FIGJAM DATA (No real board needed)\n",
    "# =====================================================\n",
    "\n",
    "print(\"üß™ TESTING WITH MOCK FIGJAM BOARD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate sticky notes from a FigJam board\n",
    "mock_figjam_notes = [\n",
    "    {'id': '1', 'content': 'Users getting frustrated with login flow', 'color': 'RED'},\n",
    "    {'id': '2', 'content': 'Really intuitive navigation!', 'color': 'GREEN'},\n",
    "    {'id': '3', 'content': 'Average time spent: 2.5 minutes', 'color': 'YELLOW'},\n",
    "    {'id': '4', 'content': 'Why do users skip this step?', 'color': 'BLUE'},\n",
    "    {'id': '5', 'content': '\"I love how fast this loads\"', 'color': 'PINK'},\n",
    "    {'id': '6', 'content': 'What if we added shortcuts here?', 'color': 'PURPLE'},\n",
    "    {'id': '7', 'content': 'Mobile experience is broken', 'color': 'ORANGE'},\n",
    "    {'id': '8', 'content': 'This workflow makes sense', 'color': 'GREEN'},\n",
    "    {'id': '9', 'content': 'need to... Error message confusing', 'color': 'YELLOW'},  # Wrong color!\n",
    "    {'id': '10', 'content': '\"Why can\\'t I just click here?\"', 'color': 'RED'},  # Wrong color!\n",
    "]\n",
    "\n",
    "print(f\"üìù Mock board has {len(mock_figjam_notes)} sticky notes\")\n",
    "print(\"\\nüß† Analyzing with PRISM Brain AI...\\n\")\n",
    "\n",
    "# Analyze with Brain AI\n",
    "analysis = brain.analyze_board(mock_figjam_notes)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ANALYSIS RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìù Total notes: {analysis['total_notes']}\")\n",
    "print(f\"\\nüìÅ Organized by content type:\")\n",
    "for note_type, notes in sorted(analysis['organized_by_type'].items(), \n",
    "                               key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"   {note_type:12} ‚Üí {len(notes):3} notes\")\n",
    "    for note in notes[:2]:  # Show first 2 examples\n",
    "        print(f\"      ‚Ä¢ [{note['color']:8}] {note['content'][:50]}\")\n",
    "\n",
    "print(\"\\n‚úÖ PRISM successfully analyzed the board!\")\n",
    "print(\"üí° Notice how it organized by CONTENT, not just color!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b7af0-73bb-46f5-b49b-10266cd00e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# TEST FULL BRAIN AI WITH ALL FEATURES\n",
    "# =====================================================\n",
    "\n",
    "# Initialize the full Brain AI\n",
    "full_brain = PRISMFullBrain(training_data)\n",
    "\n",
    "# Create a test project\n",
    "project_id = full_brain.create_project(\"Mobile App Redesign Research\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING: Multi-Modal Data Ingestion\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Ingest FigJam board\n",
    "mock_figjam = {\n",
    "    'notes': [\n",
    "        {'id': '1', 'content': 'Users cant find search button', 'color': 'RED', \n",
    "         'author': 'Sarah Chen', 'created_at': '2025-11-01T10:00:00'},\n",
    "        {'id': '2', 'content': 'Love the new navigation!', 'color': 'GREEN',\n",
    "         'author': 'Mike Johnson', 'created_at': '2025-11-01T10:15:00'},\n",
    "        {'id': '3', 'content': 'Why do users skip onboarding?', 'color': 'BLUE',\n",
    "         'author': 'Sarah Chen', 'created_at': '2025-11-01T10:30:00'},\n",
    "    ]\n",
    "}\n",
    "full_brain.ingest_figjam(project_id, mock_figjam, \"User Testing Session 1\")\n",
    "\n",
    "# 2. Ingest audio transcript\n",
    "mock_transcript = \"\"\"\n",
    "Speaker 1: We noticed that users are really struggling with the checkout flow\n",
    "Speaker 2: Yeah, three out of five participants couldn't complete the purchase\n",
    "Speaker 1: Should we simplify it to just two steps instead of four?\n",
    "Speaker 2: One user said \"I don't understand why I need to create an account\"\n",
    "Speaker 1: That's a great point. We decided to add a guest checkout option\n",
    "\"\"\"\n",
    "full_brain.ingest_audio_transcript(\n",
    "    project_id, \n",
    "    mock_transcript, \n",
    "    \"Team Meeting - Nov 2\",\n",
    "    speaker_map={'Speaker 1': 'Alex Rodriguez', 'Speaker 2': 'Jamie Lee'}\n",
    ")\n",
    "\n",
    "# 3. Ingest document\n",
    "mock_document = \"\"\"\n",
    "Research Findings Summary\n",
    "\n",
    "Our usability study revealed critical issues with the mobile checkout process. \n",
    "Users found the four-step checkout confusing and time-consuming.\n",
    "\n",
    "The primary pain point was mandatory account creation. This suggests we should \n",
    "implement a guest checkout option to reduce friction.\n",
    "\n",
    "Performance metrics showed that load times exceeded 3 seconds on mobile devices. \n",
    "This indicates a need for optimization of image assets and code splitting.\n",
    "\"\"\"\n",
    "full_brain.ingest_document(project_id, mock_document, \"Research Report Q4\")\n",
    "\n",
    "# 4. Generate synthesis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING PROJECT SYNTHESIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "synthesis = full_brain.synthesize_project(project_id)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä PROJECT OVERVIEW:\")\n",
    "print(f\"   Total notes: {synthesis['total_notes']}\")\n",
    "print(f\"   Sources: {synthesis['total_sources']}\")\n",
    "print(f\"   Contributors: {synthesis['contributors']}\")\n",
    "\n",
    "print(f\"\\nüìÅ BY CONTENT TYPE:\")\n",
    "for note_type, notes in sorted(synthesis['by_type'].items(), \n",
    "                               key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"   {note_type:15} ‚Üí {len(notes)} notes\")\n",
    "\n",
    "print(f\"\\n‚ö° BY PRIORITY:\")\n",
    "for priority in ['high', 'medium', 'low']:\n",
    "    count = len(synthesis['by_priority'].get(priority, []))\n",
    "    print(f\"   {priority:15} ‚Üí {count} notes\")\n",
    "\n",
    "print(f\"\\nüë• CONTRIBUTORS:\")\n",
    "for contributor, stats in synthesis['by_contributor'].items():\n",
    "    print(f\"   {contributor:20} ‚Üí {stats['total_contributions']} contributions\")\n",
    "\n",
    "print(f\"\\nüéØ KEY THEMES:\")\n",
    "for theme in synthesis['themes'][:5]:\n",
    "    print(f\"   {theme['name']:15} ‚Üí {theme['frequency']} mentions ({theme['percentage']:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  ACTION ITEMS (High Priority):\")\n",
    "for i, item in enumerate(synthesis['action_items'][:5], 1):\n",
    "    print(f\"   {i}. [{item['type']:12}] {item['content'][:60]}\")\n",
    "    print(f\"      Source: {item['source']} | By: {item['contributor']}\")\n",
    "\n",
    "print(f\"\\nüìà SENTIMENT ANALYSIS:\")\n",
    "sentiment = synthesis['stats']['sentiment_distribution']\n",
    "total = sum(sentiment.values())\n",
    "for mood, count in sentiment.items():\n",
    "    pct = (count/total*100) if total > 0 else 0\n",
    "    print(f\"   {mood:10} ‚Üí {count} notes ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ FULL BRAIN AI TEST COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéØ Ready for Gradio UI with:\")\n",
    "print(\"   ‚úÖ Multi-modal input (FigJam, Audio, Docs)\")\n",
    "print(\"   ‚úÖ Contributor tracking\")\n",
    "print(\"   ‚úÖ Timeline generation\") \n",
    "print(\"   ‚úÖ Real-time refresh\")\n",
    "print(\"   ‚úÖ Project management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f353c-c13f-4f19-90a7-cd17904cd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PRISM FULL BRAIN AI - Multi-Modal Research Analyzer\n",
    "Handles: FigJam boards, Audio files, Documents, Real-time updates\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import hashlib\n",
    "\n",
    "# =====================================================\n",
    "# EXPANDED BRAIN AI - MULTI-MODAL\n",
    "# =====================================================\n",
    "\n",
    "class PRISMFullBrain:\n",
    "    \"\"\"\n",
    "    Enhanced Brain AI that handles:\n",
    "    - FigJam boards (sticky notes + metadata)\n",
    "    - Audio transcripts (organized notes)\n",
    "    - Documents (PDFs, text)\n",
    "    - Real-time updates\n",
    "    - Contributor tracking\n",
    "    - Timeline generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data=None):\n",
    "        self.training_data = training_data\n",
    "        self.patterns = {}\n",
    "        self.projects = {}  # Store multiple projects\n",
    "        \n",
    "        # Knowledge base from training\n",
    "        if training_data:\n",
    "            self._initialize_from_training()\n",
    "        \n",
    "        print(\"üß† PRISM Full Brain AI initialized\")\n",
    "        print(\"   ‚úì Multi-modal input (FigJam, Audio, Docs)\")\n",
    "        print(\"   ‚úì Contributor tracking\")\n",
    "        print(\"   ‚úì Timeline generation\")\n",
    "        print(\"   ‚úì Real-time updates\")\n",
    "    \n",
    "    def _initialize_from_training(self):\n",
    "        \"\"\"Learn patterns from training data\"\"\"\n",
    "        print(\"üéì Learning from training data...\")\n",
    "        all_notes = []\n",
    "        for board in self.training_data:\n",
    "            all_notes.extend(board['notes'])\n",
    "        \n",
    "        # Learn patterns (simplified from earlier)\n",
    "        self.patterns['keywords'] = self._learn_keywords(all_notes)\n",
    "        print(f\"   ‚úì Learned patterns from {len(all_notes):,} notes\")\n",
    "    \n",
    "    def _learn_keywords(self, notes):\n",
    "        \"\"\"Learn keywords for each content type\"\"\"\n",
    "        keywords = defaultdict(set)\n",
    "        for note in notes:\n",
    "            content_type = note['true_type']\n",
    "            words = note['content'].lower().split()\n",
    "            keywords[content_type].update(words[:5])  # Top words\n",
    "        return {k: list(v)[:20] for k, v in keywords.items()}\n",
    "    \n",
    "    # =====================================================\n",
    "    # PROJECT MANAGEMENT\n",
    "    # =====================================================\n",
    "    \n",
    "    def create_project(self, project_name: str) -> str:\n",
    "        \"\"\"Create a new PRISM project\"\"\"\n",
    "        project_id = hashlib.md5(project_name.encode()).hexdigest()[:8]\n",
    "        \n",
    "        self.projects[project_id] = {\n",
    "            'name': project_name,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'sources': [],  # All input sources\n",
    "            'notes': [],  # All extracted notes\n",
    "            'timeline': [],  # Chronological events\n",
    "            'contributors': {},  # Who did what\n",
    "            'insights': {},  # Organized findings\n",
    "            'last_updated': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Created project: {project_name} (ID: {project_id})\")\n",
    "        return project_id\n",
    "    \n",
    "    def get_project(self, project_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Retrieve project data\"\"\"\n",
    "        return self.projects.get(project_id)\n",
    "    \n",
    "    # =====================================================\n",
    "    # FIGJAM INGESTION (Enhanced)\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_figjam(self, project_id: str, figjam_data: Dict, board_name: str = \"Untitled\"):\n",
    "        \"\"\"\n",
    "        Ingest FigJam board with full metadata\n",
    "        Args:\n",
    "            figjam_data: Raw FigJam API response or extracted notes with metadata\n",
    "        \"\"\"\n",
    "        print(f\"\\nüì• Ingesting FigJam: {board_name}\")\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        \n",
    "        # Extract notes with metadata\n",
    "        notes_with_metadata = []\n",
    "        \n",
    "        for note in figjam_data.get('notes', []):\n",
    "            # Analyze content\n",
    "            analysis = self._analyze_note_content(note['content'], note.get('color', 'YELLOW'))\n",
    "            \n",
    "            enriched_note = {\n",
    "                'id': note.get('id', f\"note_{len(notes_with_metadata)}\"),\n",
    "                'source': 'figjam',\n",
    "                'source_name': board_name,\n",
    "                'content': note['content'],\n",
    "                'color': note.get('color', 'YELLOW'),\n",
    "                'predicted_type': analysis['predicted_type'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                \n",
    "                # Metadata\n",
    "                'contributor': note.get('author', 'Unknown'),\n",
    "                'created_at': note.get('created_at', datetime.now().isoformat()),\n",
    "                'modified_at': note.get('modified_at', datetime.now().isoformat()),\n",
    "                'position': note.get('position', {}),\n",
    "                \n",
    "                # Analysis\n",
    "                'sentiment': self._detect_sentiment(note['content']),\n",
    "                'priority': self._calculate_priority(note['content'], analysis),\n",
    "                'tags': self._extract_tags(note['content'])\n",
    "            }\n",
    "            \n",
    "            notes_with_metadata.append(enriched_note)\n",
    "        \n",
    "        # Add to project\n",
    "        source_entry = {\n",
    "            'type': 'figjam',\n",
    "            'name': board_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(notes_with_metadata)\n",
    "        }\n",
    "        \n",
    "        project['sources'].append(source_entry)\n",
    "        project['notes'].extend(notes_with_metadata)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        # Update timeline\n",
    "        self._update_timeline(project, notes_with_metadata)\n",
    "        \n",
    "        # Update contributors\n",
    "        self._update_contributors(project, notes_with_metadata)\n",
    "        \n",
    "        print(f\"   ‚úì Added {len(notes_with_metadata)} notes from FigJam\")\n",
    "        print(f\"   ‚úì {len(set(n['contributor'] for n in notes_with_metadata))} contributors identified\")\n",
    "        \n",
    "        return notes_with_metadata\n",
    "    \n",
    "    # =====================================================\n",
    "    # AUDIO INGESTION (Smart Notes from Transcripts)\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_audio_transcript(self, project_id: str, transcript: str, \n",
    "                                audio_name: str = \"Recording\", \n",
    "                                speaker_map: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Ingest audio transcript and create organized notes\n",
    "        NOT just a transcript - extracts insights like a human would\n",
    "        \n",
    "        Args:\n",
    "            transcript: Full text transcript\n",
    "            speaker_map: Optional mapping of speaker IDs to names\n",
    "        \"\"\"\n",
    "        print(f\"\\nüéôÔ∏è  Ingesting audio: {audio_name}\")\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        \n",
    "        # Parse transcript into segments\n",
    "        segments = self._segment_transcript(transcript)\n",
    "        \n",
    "        # Extract notes from segments\n",
    "        extracted_notes = []\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            # Identify speaker\n",
    "            speaker = self._identify_speaker(segment, speaker_map)\n",
    "            \n",
    "            # Extract key points (not just transcribe)\n",
    "            key_points = self._extract_key_points(segment)\n",
    "            \n",
    "            for point in key_points:\n",
    "                analysis = self._analyze_note_content(point, 'YELLOW')\n",
    "                \n",
    "                note = {\n",
    "                    'id': f\"audio_{audio_name}_{i}_{len(extracted_notes)}\",\n",
    "                    'source': 'audio',\n",
    "                    'source_name': audio_name,\n",
    "                    'content': point,\n",
    "                    'predicted_type': analysis['predicted_type'],\n",
    "                    'confidence': analysis['confidence'],\n",
    "                    \n",
    "                    # Metadata\n",
    "                    'contributor': speaker,\n",
    "                    'created_at': datetime.now().isoformat(),\n",
    "                    'timestamp_in_audio': segment.get('timestamp', '00:00'),\n",
    "                    \n",
    "                    # Analysis\n",
    "                    'sentiment': self._detect_sentiment(point),\n",
    "                    'priority': self._calculate_priority(point, analysis),\n",
    "                    'tags': self._extract_tags(point)\n",
    "                }\n",
    "                \n",
    "                extracted_notes.append(note)\n",
    "        \n",
    "        # Add to project\n",
    "        source_entry = {\n",
    "            'type': 'audio',\n",
    "            'name': audio_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(extracted_notes),\n",
    "            'duration': 'Unknown'  # Could be passed in\n",
    "        }\n",
    "        \n",
    "        project['sources'].append(source_entry)\n",
    "        project['notes'].extend(extracted_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, extracted_notes)\n",
    "        self._update_contributors(project, extracted_notes)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted {len(extracted_notes)} insights from audio\")\n",
    "        print(f\"   ‚úì {len(segments)} conversation segments analyzed\")\n",
    "        \n",
    "        return extracted_notes\n",
    "    \n",
    "    def _segment_transcript(self, transcript: str) -> List[Dict]:\n",
    "        \"\"\"Break transcript into meaningful segments\"\"\"\n",
    "        # Split by speaker changes or pauses\n",
    "        # Format: \"Speaker 1: text\\nSpeaker 2: text\"\n",
    "        \n",
    "        segments = []\n",
    "        current_speaker = None\n",
    "        current_text = []\n",
    "        \n",
    "        for line in transcript.split('\\n'):\n",
    "            # Check if line starts with speaker indicator\n",
    "            speaker_match = re.match(r'^(Speaker \\d+|[A-Z][a-z]+):\\s*(.+)', line)\n",
    "            \n",
    "            if speaker_match:\n",
    "                # Save previous segment\n",
    "                if current_text:\n",
    "                    segments.append({\n",
    "                        'speaker': current_speaker,\n",
    "                        'text': ' '.join(current_text),\n",
    "                        'timestamp': 'Unknown'\n",
    "                    })\n",
    "                \n",
    "                # Start new segment\n",
    "                current_speaker = speaker_match.group(1)\n",
    "                current_text = [speaker_match.group(2)]\n",
    "            else:\n",
    "                # Continue current segment\n",
    "                if line.strip():\n",
    "                    current_text.append(line.strip())\n",
    "        \n",
    "        # Add final segment\n",
    "        if current_text:\n",
    "            segments.append({\n",
    "                'speaker': current_speaker or 'Unknown',\n",
    "                'text': ' '.join(current_text),\n",
    "                'timestamp': 'Unknown'\n",
    "            })\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _extract_key_points(self, segment: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key insights from transcript segment\n",
    "        NOT just the raw transcript - organized notes\n",
    "        \"\"\"\n",
    "        text = segment['text']\n",
    "        key_points = []\n",
    "        \n",
    "        # Pattern: Pain points\n",
    "        if any(word in text.lower() for word in ['problem', 'issue', 'difficult', 'struggle', 'frustrated']):\n",
    "            pain_point = self._extract_sentence_with_keywords(text, ['problem', 'issue', 'difficult'])\n",
    "            if pain_point:\n",
    "                key_points.append(f\"Pain point: {pain_point}\")\n",
    "        \n",
    "        # Pattern: Questions\n",
    "        if '?' in text:\n",
    "            questions = [s.strip() + '?' for s in text.split('?') if s.strip()]\n",
    "            key_points.extend([f\"Question: {q}\" for q in questions[:2]])\n",
    "        \n",
    "        # Pattern: Decisions/Actions\n",
    "        if any(word in text.lower() for word in ['decided', 'agreed', 'will', 'should', 'need to']):\n",
    "            decision = self._extract_sentence_with_keywords(text, ['decided', 'agreed', 'will'])\n",
    "            if decision:\n",
    "                key_points.append(f\"Decision: {decision}\")\n",
    "        \n",
    "        # Pattern: User quotes\n",
    "        quote_match = re.findall(r'[\"\\']([^\"\\']+)[\"\\']', text)\n",
    "        if quote_match:\n",
    "            key_points.extend([f'User quote: \"{q}\"' for q in quote_match[:2]])\n",
    "        \n",
    "        # Pattern: Insights/Observations\n",
    "        if any(word in text.lower() for word in ['noticed', 'found', 'saw', 'observed']):\n",
    "            observation = self._extract_sentence_with_keywords(text, ['noticed', 'found', 'saw'])\n",
    "            if observation:\n",
    "                key_points.append(f\"Observation: {observation}\")\n",
    "        \n",
    "        # If no patterns matched, extract first meaningful sentence\n",
    "        if not key_points:\n",
    "            sentences = text.split('.')\n",
    "            if sentences and len(sentences[0]) > 20:\n",
    "                key_points.append(sentences[0].strip())\n",
    "        \n",
    "        return key_points\n",
    "    \n",
    "    def _extract_sentence_with_keywords(self, text: str, keywords: List[str]) -> Optional[str]:\n",
    "        \"\"\"Extract sentence containing keywords\"\"\"\n",
    "        sentences = text.split('.')\n",
    "        for sentence in sentences:\n",
    "            if any(kw in sentence.lower() for kw in keywords):\n",
    "                return sentence.strip()\n",
    "        return None\n",
    "    \n",
    "    def _identify_speaker(self, segment: Dict, speaker_map: Optional[Dict]) -> str:\n",
    "        \"\"\"Map speaker ID to actual name\"\"\"\n",
    "        speaker_id = segment['speaker']\n",
    "        \n",
    "        if speaker_map and speaker_id in speaker_map:\n",
    "            return speaker_map[speaker_id]\n",
    "        \n",
    "        return speaker_id\n",
    "    \n",
    "    # =====================================================\n",
    "    # DOCUMENT INGESTION\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_document(self, project_id: str, document_text: str, \n",
    "                       doc_name: str = \"Document\", \n",
    "                       doc_type: str = \"pdf\"):\n",
    "        \"\"\"\n",
    "        Ingest document (PDF, Word, Text)\n",
    "        Extracts key insights, not just raw text\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìÑ Ingesting document: {doc_name}\")\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        \n",
    "        # Extract sections and insights\n",
    "        extracted_notes = []\n",
    "        \n",
    "        # Split into paragraphs\n",
    "        paragraphs = [p.strip() for p in document_text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        for i, para in enumerate(paragraphs):\n",
    "            # Skip very short paragraphs\n",
    "            if len(para) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Analyze paragraph\n",
    "            analysis = self._analyze_note_content(para, 'YELLOW')\n",
    "            \n",
    "            # Extract key sentence or create summary\n",
    "            key_insight = self._extract_key_insight(para)\n",
    "            \n",
    "            note = {\n",
    "                'id': f\"doc_{doc_name}_{i}\",\n",
    "                'source': 'document',\n",
    "                'source_name': doc_name,\n",
    "                'content': key_insight,\n",
    "                'full_text': para,  # Store full context\n",
    "                'predicted_type': analysis['predicted_type'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                \n",
    "                # Metadata\n",
    "                'contributor': 'Document Author',\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'page_number': (i // 3) + 1,  # Rough estimate\n",
    "                \n",
    "                # Analysis\n",
    "                'sentiment': self._detect_sentiment(key_insight),\n",
    "                'priority': self._calculate_priority(key_insight, analysis),\n",
    "                'tags': self._extract_tags(key_insight)\n",
    "            }\n",
    "            \n",
    "            extracted_notes.append(note)\n",
    "        \n",
    "        # Add to project\n",
    "        source_entry = {\n",
    "            'type': 'document',\n",
    "            'name': doc_name,\n",
    "            'doc_type': doc_type,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(extracted_notes)\n",
    "        }\n",
    "        \n",
    "        project['sources'].append(source_entry)\n",
    "        project['notes'].extend(extracted_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, extracted_notes)\n",
    "        self._update_contributors(project, extracted_notes)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted {len(extracted_notes)} insights from document\")\n",
    "        \n",
    "        return extracted_notes\n",
    "    \n",
    "    def _extract_key_insight(self, paragraph: str) -> str:\n",
    "        \"\"\"Extract key insight from paragraph\"\"\"\n",
    "        # Get first sentence or main point\n",
    "        sentences = paragraph.split('.')\n",
    "        \n",
    "        # Find sentence with key indicators\n",
    "        for sentence in sentences:\n",
    "            if any(word in sentence.lower() for word in \n",
    "                   ['found', 'showed', 'indicates', 'suggests', 'revealed', 'discovered']):\n",
    "                return sentence.strip()\n",
    "        \n",
    "        # Otherwise return first meaningful sentence\n",
    "        if sentences and len(sentences[0]) > 30:\n",
    "            return sentences[0].strip()\n",
    "        \n",
    "        return paragraph[:200]  # Truncate if too long\n",
    "    \n",
    "    # =====================================================\n",
    "    # CORE ANALYSIS FUNCTIONS\n",
    "    # =====================================================\n",
    "    \n",
    "    def _analyze_note_content(self, content: str, color: str) -> Dict:\n",
    "        \"\"\"Analyze note content (from earlier Brain AI)\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        scores = {}\n",
    "        if self.patterns and 'keywords' in self.patterns:\n",
    "            for content_type, keywords in self.patterns['keywords'].items():\n",
    "                score = sum(1 for keyword in keywords if keyword in content_lower)\n",
    "                scores[content_type] = score\n",
    "        \n",
    "        if scores:\n",
    "            predicted_type = max(scores, key=scores.get)\n",
    "            confidence = min(scores[predicted_type] / 10, 1.0)\n",
    "        else:\n",
    "            # Fallback patterns\n",
    "            if '?' in content:\n",
    "                predicted_type = 'question'\n",
    "            elif '\"' in content:\n",
    "                predicted_type = 'quote'\n",
    "            elif any(w in content_lower for w in ['problem', 'issue', 'error', 'broken']):\n",
    "                predicted_type = 'pain_point'\n",
    "            elif any(w in content_lower for w in ['love', 'great', 'awesome', 'excellent']):\n",
    "                predicted_type = 'positive'\n",
    "            elif any(w in content_lower for w in ['could', 'should', 'what if', 'idea']):\n",
    "                predicted_type = 'idea'\n",
    "            else:\n",
    "                predicted_type = 'neutral'\n",
    "            \n",
    "            confidence = 0.6\n",
    "        \n",
    "        return {\n",
    "            'predicted_type': predicted_type,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "    \n",
    "    def _detect_sentiment(self, content: str) -> str:\n",
    "        \"\"\"Detect sentiment: positive, negative, neutral\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        positive_words = ['love', 'great', 'awesome', 'excellent', 'good', 'helpful', 'easy', 'fast']\n",
    "        negative_words = ['hate', 'bad', 'terrible', 'awful', 'broken', 'slow', 'difficult', 'frustrated']\n",
    "        \n",
    "        pos_count = sum(1 for word in positive_words if word in content_lower)\n",
    "        neg_count = sum(1 for word in negative_words if word in content_lower)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return 'positive'\n",
    "        elif neg_count > pos_count:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def _calculate_priority(self, content: str, analysis: Dict) -> str:\n",
    "        \"\"\"Calculate priority: high, medium, low\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        # High priority indicators\n",
    "        if analysis['predicted_type'] == 'pain_point':\n",
    "            return 'high'\n",
    "        if any(word in content_lower for word in ['critical', 'urgent', 'broken', 'blocker', '!!!']):\n",
    "            return 'high'\n",
    "        \n",
    "        # Low priority indicators\n",
    "        if analysis['predicted_type'] == 'neutral':\n",
    "            return 'low'\n",
    "        if any(word in content_lower for word in ['nice to have', 'eventually', 'someday']):\n",
    "            return 'low'\n",
    "        \n",
    "        return 'medium'\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract relevant tags/topics\"\"\"\n",
    "        tags = []\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        # Common UX research tags\n",
    "        tag_keywords = {\n",
    "            'navigation': ['navigation', 'nav', 'menu', 'find'],\n",
    "            'mobile': ['mobile', 'phone', 'ios', 'android'],\n",
    "            'desktop': ['desktop', 'computer', 'laptop'],\n",
    "            'performance': ['slow', 'fast', 'loading', 'speed'],\n",
    "            'accessibility': ['accessibility', 'screen reader', 'a11y', 'contrast'],\n",
    "            'onboarding': ['onboarding', 'first time', 'getting started'],\n",
    "            'search': ['search', 'find', 'filter'],\n",
    "            'forms': ['form', 'input', 'field', 'submit'],\n",
    "            'error': ['error', 'bug', 'broken', 'crash']\n",
    "        }\n",
    "        \n",
    "        for tag, keywords in tag_keywords.items():\n",
    "            if any(kw in content_lower for kw in keywords):\n",
    "                tags.append(tag)\n",
    "        \n",
    "        return tags[:3]  # Max 3 tags\n",
    "    \n",
    "    # =====================================================\n",
    "    # TIMELINE & CONTRIBUTOR TRACKING\n",
    "    # =====================================================\n",
    "    \n",
    "    def _update_timeline(self, project: Dict, notes: List[Dict]):\n",
    "        \"\"\"Update project timeline with new notes\"\"\"\n",
    "        for note in notes:\n",
    "            event = {\n",
    "                'timestamp': note.get('created_at', note.get('modified_at')),\n",
    "                'contributor': note['contributor'],\n",
    "                'action': 'created',\n",
    "                'content_preview': note['content'][:100],\n",
    "                'note_id': note['id'],\n",
    "                'source': note['source']\n",
    "            }\n",
    "            project['timeline'].append(event)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        project['timeline'].sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "    def _update_contributors(self, project: Dict, notes: List[Dict]):\n",
    "        \"\"\"Update contributor statistics\"\"\"\n",
    "        for note in notes:\n",
    "            contributor = note['contributor']\n",
    "            \n",
    "            if contributor not in project['contributors']:\n",
    "                project['contributors'][contributor] = {\n",
    "                    'total_contributions': 0,\n",
    "                    'note_types': defaultdict(int),\n",
    "                    'first_contribution': note.get('created_at'),\n",
    "                    'last_contribution': note.get('created_at')\n",
    "                }\n",
    "            \n",
    "            stats = project['contributors'][contributor]\n",
    "            stats['total_contributions'] += 1\n",
    "            stats['note_types'][note['predicted_type']] += 1\n",
    "            stats['last_contribution'] = note.get('created_at', note.get('modified_at'))\n",
    "    \n",
    "    # =====================================================\n",
    "    # SYNTHESIS & INSIGHTS\n",
    "    # =====================================================\n",
    "    \n",
    "    def synthesize_project(self, project_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate complete synthesis of project\n",
    "        This is what gets displayed in the UI\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç Synthesizing project insights...\")\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        \n",
    "        synthesis = {\n",
    "            'project_name': project['name'],\n",
    "            'last_updated': project['last_updated'],\n",
    "            \n",
    "            # Overview\n",
    "            'total_notes': len(project['notes']),\n",
    "            'total_sources': len(project['sources']),\n",
    "            'contributors': len(project['contributors']),\n",
    "            \n",
    "            # Organized by type\n",
    "            'by_type': self._organize_by_type(project['notes']),\n",
    "            \n",
    "            # Organized by priority\n",
    "            'by_priority': self._organize_by_priority(project['notes']),\n",
    "            \n",
    "            # Organized by contributor\n",
    "            'by_contributor': project['contributors'],\n",
    "            \n",
    "            # Timeline\n",
    "            'timeline': project['timeline'],\n",
    "            \n",
    "            # Key themes\n",
    "            'themes': self._extract_themes(project['notes']),\n",
    "            \n",
    "            # Action items (high priority items)\n",
    "            'action_items': self._extract_action_items(project['notes']),\n",
    "            \n",
    "            # Statistics\n",
    "            'stats': self._calculate_stats(project)\n",
    "        }\n",
    "        \n",
    "        project['insights'] = synthesis\n",
    "        \n",
    "        print(f\"   ‚úì Synthesis complete!\")\n",
    "        print(f\"   üìä {synthesis['total_notes']} notes analyzed\")\n",
    "        print(f\"   üë• {synthesis['contributors']} contributors\")\n",
    "        print(f\"   üéØ {len(synthesis['action_items'])} action items identified\")\n",
    "        \n",
    "        return synthesis\n",
    "    \n",
    "    def _organize_by_type(self, notes: List[Dict]) -> Dict:\n",
    "        \"\"\"Organize notes by predicted type\"\"\"\n",
    "        organized = defaultdict(list)\n",
    "        for note in notes:\n",
    "            organized[note['predicted_type']].append(note)\n",
    "        return dict(organized)\n",
    "    \n",
    "    def _organize_by_priority(self, notes: List[Dict]) -> Dict:\n",
    "        \"\"\"Organize notes by priority\"\"\"\n",
    "        organized = defaultdict(list)\n",
    "        for note in notes:\n",
    "            organized[note['priority']].append(note)\n",
    "        return dict(organized)\n",
    "    \n",
    "    def _extract_themes(self, notes: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract common themes across notes\"\"\"\n",
    "        # Collect all tags\n",
    "        all_tags = []\n",
    "        for note in notes:\n",
    "            all_tags.extend(note.get('tags', []))\n",
    "        \n",
    "        # Count tag frequency\n",
    "        from collections import Counter\n",
    "        tag_counts = Counter(all_tags)\n",
    "        \n",
    "        themes = []\n",
    "        for tag, count in tag_counts.most_common(10):\n",
    "            theme = {\n",
    "                'name': tag,\n",
    "                'frequency': count,\n",
    "                'percentage': (count / len(notes)) * 100,\n",
    "                'related_notes': [n['id'] for n in notes if tag in n.get('tags', [])][:5]\n",
    "            }\n",
    "            themes.append(theme)\n",
    "        \n",
    "        return themes\n",
    "    \n",
    "    def _extract_action_items(self, notes: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract high-priority action items\"\"\"\n",
    "        action_items = []\n",
    "        \n",
    "        for note in notes:\n",
    "            if note['priority'] == 'high':\n",
    "                action_items.append({\n",
    "                    'content': note['content'],\n",
    "                    'type': note['predicted_type'],\n",
    "                    'contributor': note['contributor'],\n",
    "                    'source': note['source_name'],\n",
    "                    'created_at': note.get('created_at')\n",
    "                })\n",
    "        \n",
    "        # Sort by creation date\n",
    "        action_items.sort(key=lambda x: x.get('created_at', ''), reverse=True)\n",
    "        \n",
    "        return action_items[:20]  # Top 20\n",
    "    \n",
    "    def _calculate_stats(self, project: Dict) -> Dict:\n",
    "        \"\"\"Calculate project statistics\"\"\"\n",
    "        notes = project['notes']\n",
    "        \n",
    "        return {\n",
    "            'sentiment_distribution': {\n",
    "                'positive': sum(1 for n in notes if n.get('sentiment') == 'positive'),\n",
    "                'negative': sum(1 for n in notes if n.get('sentiment') == 'negative'),\n",
    "                'neutral': sum(1 for n in notes if n.get('sentiment') == 'neutral')\n",
    "            },\n",
    "            'sources_breakdown': {\n",
    "                source['type']: sum(1 for s in project['sources'] if s['type'] == source['type'])\n",
    "                for source in project['sources']\n",
    "            },\n",
    "            'avg_confidence': sum(n.get('confidence', 0) for n in notes) / len(notes) if notes else 0\n",
    "        }\n",
    "    \n",
    "    # =====================================================\n",
    "    # REAL-TIME UPDATE\n",
    "    # =====================================================\n",
    "    \n",
    "    def refresh_project(self, project_id: str):\n",
    "        \"\"\"\n",
    "        Refresh project synthesis (called when user clicks refresh button)\n",
    "        Re-analyzes all notes with latest Brain AI patterns\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîÑ Refreshing project analysis...\")\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        \n",
    "        # Re-analyze all notes\n",
    "        for note in project['notes']:\n",
    "            # Re-run analysis\n",
    "            new_analysis = self._analyze_note_content(note['content'], note.get('color', 'YELLOW'))\n",
    "            note['predicted_type'] = new_analysis['predicted_type']\n",
    "            note['confidence'] = new_analysis['confidence']\n",
    "            \n",
    "            # Update sentiment, priority, tags\n",
    "            note['sentiment'] = self._detect_sentiment(note['content'])\n",
    "            note['priority'] = self._calculate_priority(note['content'], new_analysis)\n",
    "            note['tags'] = self._extract_tags(note['content'])\n",
    "        \n",
    "        # Regenerate synthesis\n",
    "        synthesis = self.synthesize_project(project_id)\n",
    "        \n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        print(f\"   ‚úì Project refreshed at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        return synthesis\n",
    "\n",
    "print(\"‚úÖ PRISM Full Brain AI module loaded!\")\n",
    "print(\"   Ready for multi-modal analysis (FigJam + Audio + Docs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ba3d1-fd54-44b9-88bd-53c55dd18a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the full Brain AI\n",
    "full_brain = PRISMFullBrain(training_data)\n",
    "\n",
    "# Create a test project\n",
    "project_id = full_brain.create_project(\"Mobile App Redesign Research\")\n",
    "# ... rest of test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e699de-f19c-4524-aee2-f019efa6c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PRISM GRADIO UI - Black & White Beta Version\n",
    "# =====================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Install gradio if needed\n",
    "# !pip install gradio\n",
    "\n",
    "print(\"üé® Loading PRISM Gradio UI...\")\n",
    "\n",
    "# Initialize Brain AI with training data\n",
    "full_brain = PRISMFullBrain(training_data)\n",
    "\n",
    "# Global state for current project\n",
    "current_project_id = None\n",
    "\n",
    "# =====================================================\n",
    "# UI FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def create_new_project(project_name):\n",
    "    \"\"\"Create new PRISM project\"\"\"\n",
    "    global current_project_id\n",
    "    if not project_name:\n",
    "        return \"‚ùå Please enter a project name\"\n",
    "    \n",
    "    current_project_id = full_brain.create_project(project_name)\n",
    "    return f\"‚úÖ Created project: {project_name}\\nProject ID: {current_project_id}\"\n",
    "\n",
    "def upload_figjam(board_name, notes_text):\n",
    "    \"\"\"Upload FigJam notes (paste text)\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    # Parse pasted notes (format: \"color: content\")\n",
    "    notes = []\n",
    "    for line in notes_text.split('\\n'):\n",
    "        if ':' in line:\n",
    "            parts = line.split(':', 1)\n",
    "            color = parts[0].strip().upper()\n",
    "            content = parts[1].strip()\n",
    "            notes.append({\n",
    "                'content': content,\n",
    "                'color': color,\n",
    "                'author': 'User',\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    figjam_data = {'notes': notes}\n",
    "    full_brain.ingest_figjam(current_project_id, figjam_data, board_name)\n",
    "    \n",
    "    return f\"‚úÖ Added {len(notes)} notes from FigJam board: {board_name}\"\n",
    "\n",
    "def upload_transcript(audio_name, transcript_text):\n",
    "    \"\"\"Upload audio transcript\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    full_brain.ingest_audio_transcript(current_project_id, transcript_text, audio_name)\n",
    "    \n",
    "    return f\"‚úÖ Processed audio transcript: {audio_name}\"\n",
    "\n",
    "def upload_document(doc_name, document_text):\n",
    "    \"\"\"Upload document\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    full_brain.ingest_document(current_project_id, document_text, doc_name)\n",
    "    \n",
    "    return f\"‚úÖ Processed document: {doc_name}\"\n",
    "\n",
    "def generate_analysis():\n",
    "    \"\"\"Generate full analysis\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    synthesis = full_brain.synthesize_project(current_project_id)\n",
    "    \n",
    "    # Format output\n",
    "    output = f\"\"\"\n",
    "    \n",
    "üìä PRISM ANALYSIS RESULTS\n",
    "{'='*60}\n",
    "\n",
    "PROJECT: {synthesis['project_name']}\n",
    "Last Updated: {synthesis['last_updated'][:19]}\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Total Notes: {synthesis['total_notes']}\n",
    "Sources: {synthesis['total_sources']}\n",
    "Contributors: {synthesis['contributors']}\n",
    "\n",
    "BY CONTENT TYPE\n",
    "---------------\n",
    "\"\"\"\n",
    "    \n",
    "    for note_type, notes in sorted(synthesis['by_type'].items(), \n",
    "                                   key=lambda x: len(x[1]), reverse=True):\n",
    "        output += f\"{note_type:15} ‚Üí {len(notes):3} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "BY PRIORITY\n",
    "-----------\n",
    "\"\"\"\n",
    "    for priority in ['high', 'medium', 'low']:\n",
    "        count = len(synthesis['by_priority'].get(priority, []))\n",
    "        output += f\"{priority:15} ‚Üí {count:3} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "CONTRIBUTORS\n",
    "------------\n",
    "\"\"\"\n",
    "    for contributor, stats in synthesis['by_contributor'].items():\n",
    "        output += f\"{contributor:20} ‚Üí {stats['total_contributions']} contributions\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "KEY THEMES\n",
    "----------\n",
    "\"\"\"\n",
    "    for theme in synthesis['themes'][:5]:\n",
    "        output += f\"{theme['name']:15} ‚Üí {theme['frequency']} mentions ({theme['percentage']:.1f}%)\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "ACTION ITEMS (High Priority)\n",
    "-----------------------------\n",
    "\"\"\"\n",
    "    for i, item in enumerate(synthesis['action_items'][:5], 1):\n",
    "        output += f\"{i}. [{item['type']:12}] {item['content'][:60]}\\n\"\n",
    "        output += f\"   Source: {item['source']} | By: {item['contributor']}\\n\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "SENTIMENT\n",
    "---------\n",
    "\"\"\"\n",
    "    sentiment = synthesis['stats']['sentiment_distribution']\n",
    "    total = sum(sentiment.values())\n",
    "    for mood, count in sentiment.items():\n",
    "        pct = (count/total*100) if total > 0 else 0\n",
    "        output += f\"{mood:10} ‚Üí {count} notes ({pct:.1f}%)\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "def refresh_analysis():\n",
    "    \"\"\"Refresh button\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    full_brain.refresh_project(current_project_id)\n",
    "    return generate_analysis()\n",
    "\n",
    "# =====================================================\n",
    "# BUILD GRADIO INTERFACE\n",
    "# =====================================================\n",
    "\n",
    "with gr.Blocks(title=\"PRISM - Research Synthesis\") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # PRISM - Pattern Recognition & Insight Structure Module\n",
    "    ### AI-Powered Research Synthesis Engine (Beta v0.1)\n",
    "    \"\"\")\n",
    "    \n",
    "    # PROJECT SETUP\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## 1. Create Project\")\n",
    "            project_name_input = gr.Textbox(\n",
    "                label=\"Project Name\",\n",
    "                placeholder=\"e.g., Mobile App Redesign Research\"\n",
    "            )\n",
    "            create_btn = gr.Button(\"Create Project\", variant=\"primary\")\n",
    "            create_output = gr.Textbox(label=\"Status\", lines=3)\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    # DATA UPLOAD\n",
    "    gr.Markdown(\"## 2. Upload Research Data\")\n",
    "    \n",
    "    with gr.Tab(\"FigJam Board\"):\n",
    "        figjam_board_name = gr.Textbox(label=\"Board Name\", placeholder=\"User Testing Session 1\")\n",
    "        figjam_notes_input = gr.Textbox(\n",
    "            label=\"Paste Notes (Format: COLOR: content)\",\n",
    "            placeholder=\"RED: Users can't find search\\nGREEN: Love the new design\\nBLUE: Why do users click here?\",\n",
    "            lines=10\n",
    "        )\n",
    "        figjam_btn = gr.Button(\"Upload FigJam\")\n",
    "        figjam_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Audio Transcript\"):\n",
    "        audio_name = gr.Textbox(label=\"Recording Name\", placeholder=\"Team Meeting - Nov 2\")\n",
    "        transcript_input = gr.Textbox(\n",
    "            label=\"Paste Transcript\",\n",
    "            placeholder=\"Speaker 1: We noticed users are struggling...\\nSpeaker 2: Yeah, three out of five...\",\n",
    "            lines=10\n",
    "        )\n",
    "        audio_btn = gr.Button(\"Upload Transcript\")\n",
    "        audio_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Document\"):\n",
    "        doc_name = gr.Textbox(label=\"Document Name\", placeholder=\"Research Report Q4\")\n",
    "        doc_input = gr.Textbox(\n",
    "            label=\"Paste Document Text\",\n",
    "            placeholder=\"Research findings summary...\",\n",
    "            lines=10\n",
    "        )\n",
    "        doc_btn = gr.Button(\"Upload Document\")\n",
    "        doc_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    # ANALYSIS\n",
    "    gr.Markdown(\"## 3. Generate Analysis\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        analyze_btn = gr.Button(\"üîç Analyze Project\", variant=\"primary\", size=\"lg\")\n",
    "        refresh_btn = gr.Button(\"üîÑ Refresh\", size=\"lg\")\n",
    "    \n",
    "    analysis_output = gr.Textbox(\n",
    "        label=\"PRISM Analysis\",\n",
    "        lines=30,\n",
    "        show_copy_button=True\n",
    "    )\n",
    "    \n",
    "    # WIRE UP BUTTONS\n",
    "    create_btn.click(\n",
    "        fn=create_new_project,\n",
    "        inputs=[project_name_input],\n",
    "        outputs=[create_output]\n",
    "    )\n",
    "    \n",
    "    figjam_btn.click(\n",
    "        fn=upload_figjam,\n",
    "        inputs=[figjam_board_name, figjam_notes_input],\n",
    "        outputs=[figjam_output]\n",
    "    )\n",
    "    \n",
    "    audio_btn.click(\n",
    "        fn=upload_transcript,\n",
    "        inputs=[audio_name, transcript_input],\n",
    "        outputs=[audio_output]\n",
    "    )\n",
    "    \n",
    "    doc_btn.click(\n",
    "        fn=upload_document,\n",
    "        inputs=[doc_name, doc_input],\n",
    "        outputs=[doc_output]\n",
    "    )\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        fn=generate_analysis,\n",
    "        inputs=[],\n",
    "        outputs=[analysis_output]\n",
    "    )\n",
    "    \n",
    "    refresh_btn.click(\n",
    "        fn=refresh_analysis,\n",
    "        inputs=[],\n",
    "        outputs=[analysis_output]\n",
    "    )\n",
    "\n",
    "# Launch\n",
    "print(\"üöÄ Launching PRISM UI...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3de01-bf93-4977-b26d-13501b47304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514acfd-004a-4cc1-af48-f7e97e21ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PRISM GRADIO UI - Black & White Beta Version\n",
    "# =====================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Install gradio if needed\n",
    "# !pip install gradio\n",
    "\n",
    "print(\"üé® Loading PRISM Gradio UI...\")\n",
    "\n",
    "# Initialize Brain AI with training data\n",
    "full_brain = PRISMFullBrain(training_data)\n",
    "\n",
    "# Global state for current project\n",
    "current_project_id = None\n",
    "\n",
    "# =====================================================\n",
    "# UI FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def create_new_project(project_name):\n",
    "    \"\"\"Create new PRISM project\"\"\"\n",
    "    global current_project_id\n",
    "    if not project_name:\n",
    "        return \"‚ùå Please enter a project name\"\n",
    "    \n",
    "    current_project_id = full_brain.create_project(project_name)\n",
    "    return f\"‚úÖ Created project: {project_name}\\nProject ID: {current_project_id}\"\n",
    "\n",
    "def upload_figjam(board_name, notes_text):\n",
    "    \"\"\"Upload FigJam notes (paste text)\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    # Parse pasted notes (format: \"color: content\")\n",
    "    notes = []\n",
    "    for line in notes_text.split('\\n'):\n",
    "        if ':' in line:\n",
    "            parts = line.split(':', 1)\n",
    "            color = parts[0].strip().upper()\n",
    "            content = parts[1].strip()\n",
    "            notes.append({\n",
    "                'content': content,\n",
    "                'color': color,\n",
    "                'author': 'User',\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    figjam_data = {'notes': notes}\n",
    "    full_brain.ingest_figjam(current_project_id, figjam_data, board_name)\n",
    "    \n",
    "    return f\"‚úÖ Added {len(notes)} notes from FigJam board: {board_name}\"\n",
    "\n",
    "def upload_transcript(audio_name, transcript_text):\n",
    "    \"\"\"Upload audio transcript\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    full_brain.ingest_audio_transcript(current_project_id, transcript_text, audio_name)\n",
    "    \n",
    "    return f\"‚úÖ Processed audio transcript: {audio_name}\"\n",
    "\n",
    "def upload_document(doc_name, document_text):\n",
    "    \"\"\"Upload document\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    full_brain.ingest_document(current_project_id, document_text, doc_name)\n",
    "    \n",
    "    return f\"‚úÖ Processed document: {doc_name}\"\n",
    "\n",
    "def generate_analysis():\n",
    "    \"\"\"Generate full analysis\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    synthesis = full_brain.synthesize_project(current_project_id)\n",
    "    \n",
    "    # Format output\n",
    "    output = f\"\"\"\n",
    "    \n",
    "üìä PRISM ANALYSIS RESULTS\n",
    "{'='*60}\n",
    "\n",
    "PROJECT: {synthesis['project_name']}\n",
    "Last Updated: {synthesis['last_updated'][:19]}\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Total Notes: {synthesis['total_notes']}\n",
    "Sources: {synthesis['total_sources']}\n",
    "Contributors: {synthesis['contributors']}\n",
    "\n",
    "BY CONTENT TYPE\n",
    "---------------\n",
    "\"\"\"\n",
    "    \n",
    "    for note_type, notes in sorted(synthesis['by_type'].items(), \n",
    "                                   key=lambda x: len(x[1]), reverse=True):\n",
    "        output += f\"{note_type:15} ‚Üí {len(notes):3} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "BY PRIORITY\n",
    "-----------\n",
    "\"\"\"\n",
    "    for priority in ['high', 'medium', 'low']:\n",
    "        count = len(synthesis['by_priority'].get(priority, []))\n",
    "        output += f\"{priority:15} ‚Üí {count:3} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "CONTRIBUTORS\n",
    "------------\n",
    "\"\"\"\n",
    "    for contributor, stats in synthesis['by_contributor'].items():\n",
    "        output += f\"{contributor:20} ‚Üí {stats['total_contributions']} contributions\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "KEY THEMES\n",
    "----------\n",
    "\"\"\"\n",
    "    for theme in synthesis['themes'][:5]:\n",
    "        output += f\"{theme['name']:15} ‚Üí {theme['frequency']} mentions ({theme['percentage']:.1f}%)\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "ACTION ITEMS (High Priority)\n",
    "-----------------------------\n",
    "\"\"\"\n",
    "    for i, item in enumerate(synthesis['action_items'][:5], 1):\n",
    "        output += f\"{i}. [{item['type']:12}] {item['content'][:60]}\\n\"\n",
    "        output += f\"   Source: {item['source']} | By: {item['contributor']}\\n\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "SENTIMENT\n",
    "---------\n",
    "\"\"\"\n",
    "    sentiment = synthesis['stats']['sentiment_distribution']\n",
    "    total = sum(sentiment.values())\n",
    "    for mood, count in sentiment.items():\n",
    "        pct = (count/total*100) if total > 0 else 0\n",
    "        output += f\"{mood:10} ‚Üí {count} notes ({pct:.1f}%)\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "def refresh_analysis():\n",
    "    \"\"\"Refresh button\"\"\"\n",
    "    global current_project_id\n",
    "    \n",
    "    if not current_project_id:\n",
    "        return \"‚ùå Create a project first!\"\n",
    "    \n",
    "    full_brain.refresh_project(current_project_id)\n",
    "    return generate_analysis()\n",
    "\n",
    "# =====================================================\n",
    "# BUILD GRADIO INTERFACE\n",
    "# =====================================================\n",
    "\n",
    "with gr.Blocks(title=\"PRISM - Research Synthesis\") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # PRISM - Pattern Recognition & Insight Structure Module\n",
    "    ### AI-Powered Research Synthesis Engine (Beta v0.1)\n",
    "    \"\"\")\n",
    "    \n",
    "    # PROJECT SETUP\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## 1. Create Project\")\n",
    "            project_name_input = gr.Textbox(\n",
    "                label=\"Project Name\",\n",
    "                placeholder=\"e.g., Mobile App Redesign Research\"\n",
    "            )\n",
    "            create_btn = gr.Button(\"Create Project\", variant=\"primary\")\n",
    "            create_output = gr.Textbox(label=\"Status\", lines=3)\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    # DATA UPLOAD\n",
    "    gr.Markdown(\"## 2. Upload Research Data\")\n",
    "    \n",
    "    with gr.Tab(\"FigJam Board\"):\n",
    "        figjam_board_name = gr.Textbox(label=\"Board Name\", placeholder=\"User Testing Session 1\")\n",
    "        figjam_notes_input = gr.Textbox(\n",
    "            label=\"Paste Notes (Format: COLOR: content)\",\n",
    "            placeholder=\"RED: Users can't find search\\nGREEN: Love the new design\\nBLUE: Why do users click here?\",\n",
    "            lines=10\n",
    "        )\n",
    "        figjam_btn = gr.Button(\"Upload FigJam\")\n",
    "        figjam_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Audio Transcript\"):\n",
    "        audio_name = gr.Textbox(label=\"Recording Name\", placeholder=\"Team Meeting - Nov 2\")\n",
    "        transcript_input = gr.Textbox(\n",
    "            label=\"Paste Transcript\",\n",
    "            placeholder=\"Speaker 1: We noticed users are struggling...\\nSpeaker 2: Yeah, three out of five...\",\n",
    "            lines=10\n",
    "        )\n",
    "        audio_btn = gr.Button(\"Upload Transcript\")\n",
    "        audio_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Document\"):\n",
    "        doc_name = gr.Textbox(label=\"Document Name\", placeholder=\"Research Report Q4\")\n",
    "        doc_input = gr.Textbox(\n",
    "            label=\"Paste Document Text\",\n",
    "            placeholder=\"Research findings summary...\",\n",
    "            lines=10\n",
    "        )\n",
    "        doc_btn = gr.Button(\"Upload Document\")\n",
    "        doc_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    # ANALYSIS\n",
    "    gr.Markdown(\"## 3. Generate Analysis\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        analyze_btn = gr.Button(\"üîç Analyze Project\", variant=\"primary\", size=\"lg\")\n",
    "        refresh_btn = gr.Button(\"üîÑ Refresh\", size=\"lg\")\n",
    "    \n",
    "    analysis_output = gr.Textbox(\n",
    "        label=\"PRISM Analysis\",\n",
    "        lines=30,\n",
    "        show_copy_button=True\n",
    "    )\n",
    "    \n",
    "    # WIRE UP BUTTONS\n",
    "    create_btn.click(\n",
    "        fn=create_new_project,\n",
    "        inputs=[project_name_input],\n",
    "        outputs=[create_output]\n",
    "    )\n",
    "    \n",
    "    figjam_btn.click(\n",
    "        fn=upload_figjam,\n",
    "        inputs=[figjam_board_name, figjam_notes_input],\n",
    "        outputs=[figjam_output]\n",
    "    )\n",
    "    \n",
    "    audio_btn.click(\n",
    "        fn=upload_transcript,\n",
    "        inputs=[audio_name, transcript_input],\n",
    "        outputs=[audio_output]\n",
    "    )\n",
    "    \n",
    "    doc_btn.click(\n",
    "        fn=upload_document,\n",
    "        inputs=[doc_name, doc_input],\n",
    "        outputs=[doc_output]\n",
    "    )\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        fn=generate_analysis,\n",
    "        inputs=[],\n",
    "        outputs=[analysis_output]\n",
    "    )\n",
    "    \n",
    "    refresh_btn.click(\n",
    "        fn=refresh_analysis,\n",
    "        inputs=[],\n",
    "        outputs=[analysis_output]\n",
    "    )\n",
    "\n",
    "# Launch\n",
    "print(\"üöÄ Launching PRISM UI...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0a3d2-fad5-448a-8951-ff5efb962dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SETUP - Run this first\n",
    "# =====================================================\n",
    "\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "\n",
    "# Make sure PRISMBrainV2 is loaded\n",
    "# (You need to run the Brain AI v2 cell first if you haven't)\n",
    "\n",
    "# Initialize Brain\n",
    "figjam_token = \"figd_YP-yLbvxZ0jOVR9C54bCPveiHdkFB3uZD7hKQKDF\"\n",
    "brain_v2 = PRISMBrainV2(training_data, figjam_token)\n",
    "\n",
    "# Global project state\n",
    "current_project_id = None\n",
    "\n",
    "print(\"‚úÖ Setup complete - ready for UI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728f050-fa71-4d88-b748-ca6586284265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
