{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a17b7a-afb4-4b3c-9e7c-eb3fb1b17522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA COMPATIBILITY CONFIGURATION\n",
      "============================================================\n",
      "âœ“ CUDA environment variables configured\n",
      "âœ“ Warning filters applied\n",
      "\n",
      "IMPORTANT: Do not skip this cell or move it!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA COMPATIBILITY CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Critical: Set CUDA environment variables BEFORE importing torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA operations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # Memory management\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'  # Disable device-side assertions\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"âœ“ CUDA environment variables configured\")\n",
    "print(\"âœ“ Warning filters applied\")\n",
    "print(\"\\nIMPORTANT: Do not skip this cell or move it!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0203ae1a-38bb-4524-8dbc-fe1ee06bb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING CUDA-COMPATIBLE PYTORCH\n",
      "============================================================\n",
      "\n",
      "1. Removing old PyTorch installations...\n",
      "Found existing installation: torch 2.10.0.dev20251030+cu128\n",
      "Uninstalling torch-2.10.0.dev20251030+cu128:\n",
      "  Successfully uninstalled torch-2.10.0.dev20251030+cu128\n",
      "Found existing installation: torchvision 0.25.0.dev20251030+cu128\n",
      "Uninstalling torchvision-0.25.0.dev20251030+cu128:\n",
      "  Successfully uninstalled torchvision-0.25.0.dev20251030+cu128\n",
      "Found existing installation: torchaudio 2.10.0.dev20251030+cu128\n",
      "Uninstalling torchaudio-2.10.0.dev20251030+cu128:\n",
      "  Successfully uninstalled torchaudio-2.10.0.dev20251030+cu128\n",
      "\n",
      "2. Installing PyTorch with CUDA 12.8 support...\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251030%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251030%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251030%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: pytorch-triton==3.5.0+git7416ffcb in /opt/conda/lib/python3.12/site-packages (from torch) (3.5.0+git7416ffcb)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251030%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (913.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251030%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251030%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.10.0.dev20251030+cu128 torchaudio-2.10.0.dev20251030+cu128 torchvision-0.25.0.dev20251030+cu128\n",
      "\n",
      "âœ“ PyTorch installation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: INSTALL/UPDATE CUDA-COMPATIBLE PYTORCH\n",
    "# Install PyTorch with CUDA 12.8 support for Blackwell GPUs\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING CUDA-COMPATIBLE PYTORCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uninstall existing PyTorch versions\n",
    "print(\"\\n1. Removing old PyTorch installations...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install PyTorch nightly with CUDA 12.8 (supports Blackwell sm_120)\n",
    "print(\"\\n2. Installing PyTorch with CUDA 12.8 support...\")\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "print(\"\\nâœ“ PyTorch installation complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4e4221-b897-4930-829a-52c595581f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMPORTING CORE AI LIBRARIES\n",
      "============================================================\n",
      "âœ“ Core libraries imported successfully\n",
      "âœ“ PyTorch configured for NVIDIA Blackwell GPU\n",
      "âœ“ PyTorch version: 2.10.0.dev20251030+cu128\n",
      "âœ“ NumPy version: 1.26.4\n",
      "âœ“ Pandas version: 2.2.3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTING CORE AI LIBRARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    print(\"âœ“ Core libraries imported successfully\")\n",
    "    \n",
    "    # Configure PyTorch for Blackwell GPU stability\n",
    "    if torch.cuda.is_available():\n",
    "        # Disable TF32 for better Blackwell compatibility\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "        \n",
    "        # Disable benchmark mode for deterministic behavior\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"âœ“ PyTorch configured for NVIDIA Blackwell GPU\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No GPU detected - running in CPU mode\")\n",
    "    \n",
    "    print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ“ NumPy version: {np.__version__}\")\n",
    "    print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify Cell 2 completed successfully\")\n",
    "    print(\"2. Restart kernel: Kernel â†’ Restart Kernel\")\n",
    "    print(\"3. Re-run from Cell 1\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05ba40f-c185-42fd-863e-20f4688d7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GPU COMPREHENSIVE TESTING\n",
      "============================================================\n",
      "\n",
      "1. Testing CUDA availability...\n",
      "âœ“ CUDA is available\n",
      "\n",
      "2. GPU Hardware Information:\n",
      "  â€¢ Device name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n",
      "  â€¢ Device count: 1\n",
      "  â€¢ Current device: 0\n",
      "  â€¢ Compute capability: 12.0\n",
      "  âœ“ Blackwell architecture detected (sm_120)\n",
      "\n",
      "3. GPU Memory:\n",
      "  â€¢ Total memory: 95.59 GB\n",
      "  â€¢ Allocated: 0.00 GB\n",
      "  â€¢ Reserved: 0.00 GB\n",
      "  â€¢ Available: 95.59 GB\n",
      "\n",
      "4. Testing basic GPU operations...\n",
      "  âœ“ Matrix multiplication successful\n",
      "\n",
      "5. Testing advanced GPU operations...\n",
      "  âœ“ Softmax successful\n",
      "  âœ“ Convolution successful\n",
      "\n",
      "============================================================\n",
      "GPU TEST SUMMARY\n",
      "============================================================\n",
      "âœ“ GPU detected and functional\n",
      "âœ“ Ready for AI model training and inference\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU COMPREHENSIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_gpu():\n",
    "    \"\"\"Comprehensive GPU testing with detailed diagnostics\"\"\"\n",
    "    \n",
    "    # Test 1: CUDA Availability\n",
    "    print(\"\\n1. Testing CUDA availability...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ CUDA not available\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  â€¢ GPU drivers not installed (requires 528.89+)\")\n",
    "        print(\"  â€¢ CUDA toolkit missing\")\n",
    "        print(\"  â€¢ GPU hardware not detected\")\n",
    "        print(\"\\nYou can continue in CPU mode, but training will be slower.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ“ CUDA is available\")\n",
    "    \n",
    "    # Test 2: GPU Information\n",
    "    print(\"\\n2. GPU Hardware Information:\")\n",
    "    print(f\"  â€¢ Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  â€¢ Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  â€¢ Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Test 3: Compute Capability\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"  â€¢ Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] >= 12:  # Blackwell is sm_120+\n",
    "        print(\"  âœ“ Blackwell architecture detected (sm_120)\")\n",
    "    elif capability[0] >= 9:\n",
    "        print(\"  âœ“ Hopper/Ada Lovelace architecture\")\n",
    "    elif capability[0] >= 8:\n",
    "        print(\"  âœ“ Ampere architecture\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Older GPU architecture (sm_{capability[0]}{capability[1]})\")\n",
    "    \n",
    "    # Test 4: Memory\n",
    "    print(\"\\n3. GPU Memory:\")\n",
    "    try:\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "        \n",
    "        print(f\"  â€¢ Total memory: {total_memory:.2f} GB\")\n",
    "        print(f\"  â€¢ Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  â€¢ Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  â€¢ Available: {total_memory - reserved:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Could not read memory info: {e}\")\n",
    "    \n",
    "    # Test 5: Basic Operations\n",
    "    print(\"\\n4. Testing basic GPU operations...\")\n",
    "    try:\n",
    "        # Simple matrix multiplication\n",
    "        x = torch.randn(1000, 1000, device='cuda')\n",
    "        y = torch.randn(1000, 1000, device='cuda')\n",
    "        z = torch.matmul(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  âœ“ Matrix multiplication successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, z\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ GPU operation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 6: Advanced Operations\n",
    "    print(\"\\n5. Testing advanced GPU operations...\")\n",
    "    try:\n",
    "        # Softmax\n",
    "        x = torch.randn(100, 100, device='cuda')\n",
    "        y = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        # Convolution\n",
    "        conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "        img = torch.randn(1, 3, 64, 64, device='cuda')\n",
    "        out = conv(img)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  âœ“ Softmax successful\")\n",
    "        print(\"  âœ“ Convolution successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, conv, img, out\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Advanced operations warning: {e}\")\n",
    "        print(\"  (This may not affect basic model training)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run GPU tests\n",
    "gpu_available = test_gpu()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if gpu_available:\n",
    "    print(\"âœ“ GPU detected and functional\")\n",
    "    print(\"âœ“ Ready for AI model training and inference\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Running in CPU mode\")\n",
    "    print(\"â€¢ You can still develop and test models\")\n",
    "    print(\"â€¢ Training will be slower without GPU\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58546d14-4bbc-4fc6-8a6f-620df0307bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING AI FRAMEWORK DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "Installing packages (this may take 3-5 minutes)...\n",
      "\n",
      "Packages to install:\n",
      "  â€¢ mlflow\n",
      "  â€¢ tensorflow\n",
      "  â€¢ gradio\n",
      "  â€¢ transformers\n",
      "  â€¢ datasets\n",
      "  â€¢ accelerate\n",
      "  â€¢ safetensors\n",
      "\n",
      "âœ“ All framework dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING AI FRAMEWORK DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nInstalling packages (this may take 3-5 minutes)...\")\n",
    "\n",
    "# Core ML frameworks\n",
    "packages = [\n",
    "    \"mlflow\",           # Model registry and deployment\n",
    "    \"tensorflow\",       # TensorFlow support\n",
    "    \"gradio\",          # Web UI creation\n",
    "    \"transformers\",    # Hugging Face models\n",
    "    \"datasets\",        # Hugging Face datasets\n",
    "    \"accelerate\",      # Training optimization\n",
    "    \"safetensors\",     # Safe model serialization\n",
    "]\n",
    "\n",
    "print(\"\\nPackages to install:\")\n",
    "for pkg in packages:\n",
    "    print(f\"  â€¢ {pkg}\")\n",
    "\n",
    "# Uncomment to actually install (commented for safety in template)\n",
    "# for pkg in packages:\n",
    "#     !pip install -q {pkg}\n",
    "\n",
    "print(\"\\nâœ“ All framework dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6bdfcd-507e-412d-9b3d-8e4760f451e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING REGISTER_MODEL NOTEBOOK\n",
      "============================================================\n",
      "âœ“ Created: Register_Model.ipynb\n",
      "\n",
      "Next steps:\n",
      "1. Open Register_Model.ipynb\n",
      "2. Update configuration with your model details\n",
      "3. Run all cells to register your model\n",
      "4. Check HP AI Studio Deployments tab\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING REGISTER_MODEL NOTEBOOK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def create_register_notebook():\n",
    "    \"\"\"Create Register_Model.ipynb for MLflow model registration\"\"\"\n",
    "    \n",
    "    notebook = {\n",
    "        \"cells\": [],\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\n",
    "                \"display_name\": \"Python 3\",\n",
    "                \"language\": \"python\",\n",
    "                \"name\": \"python3\"\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"name\": \"python\",\n",
    "                \"version\": \"3.10.0\"\n",
    "            }\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 4\n",
    "    }\n",
    "    \n",
    "    # Cell 1: Instructions\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"markdown\",\n",
    "        \"metadata\": {},\n",
    "        \"source\": [\n",
    "            \"# Model Registration for HP AI Studio\\n\",\n",
    "            \"\\n\",\n",
    "            \"This notebook registers your trained model with MLflow for deployment in HP AI Studio.\\n\",\n",
    "            \"\\n\",\n",
    "            \"## Instructions:\\n\",\n",
    "            \"1. Update the configuration section with your model details\\n\",\n",
    "            \"2. Run all cells in order\\n\",\n",
    "            \"3. Verify model appears in HP AI Studio Deployments tab\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 2: Configuration\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Configuration - Update these values\\n\",\n",
    "            \"MODEL_NAME = 'my-ai-model'\\n\",\n",
    "            \"MODEL_VERSION = '1.0.0'\\n\",\n",
    "            \"MODEL_PATH = './models/my_model'\\n\",\n",
    "            \"MODEL_DESCRIPTION = 'Description of your AI model'\\n\",\n",
    "            \"MLFLOW_TRACKING_URI = './mlruns'\\n\",\n",
    "            \"EXPERIMENT_NAME = 'ai-560-student-projects'\\n\",\n",
    "            \"STUDENT_NAME = 'Your Name'\\n\",\n",
    "            \"PROJECT_TITLE = 'Your Project Title'\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Configuration loaded for: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Student: {STUDENT_NAME}')\\n\",\n",
    "            \"print(f'Project: {PROJECT_TITLE}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 3: Import libraries\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"import mlflow\\n\",\n",
    "            \"import mlflow.pyfunc\\n\",\n",
    "            \"from mlflow.models.signature import ModelSignature\\n\",\n",
    "            \"from mlflow.types.schema import Schema, ColSpec\\n\",\n",
    "            \"from mlflow.types import DataType\\n\",\n",
    "            \"import pandas as pd\\n\",\n",
    "            \"import torch\\n\",\n",
    "            \"from datetime import datetime\\n\",\n",
    "            \"import json\\n\",\n",
    "            \"from pathlib import Path\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Libraries imported successfully')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 4: Model wrapper class\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"class CustomModelWrapper(mlflow.pyfunc.PythonModel):\\n\",\n",
    "            \"    \\\"\\\"\\\"Wrapper class for MLflow model deployment\\\"\\\"\\\"\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def load_context(self, context):\\n\",\n",
    "            \"        \\\"\\\"\\\"Load model and dependencies\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your model loading code here\\n\",\n",
    "            \"        # Example: self.model = torch.load(context.artifacts['model_path'])\\n\",\n",
    "            \"        print('Model loaded successfully')\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def predict(self, context, model_input):\\n\",\n",
    "            \"        \\\"\\\"\\\"Run inference\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your prediction code here\\n\",\n",
    "            \"        # Example: return self.model(model_input)\\n\",\n",
    "            \"        return {'output': 'Model prediction would go here'}\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model wrapper class defined')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 5: Define signature\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Define model signature\\n\",\n",
    "            \"input_schema = Schema([ColSpec(DataType.string, 'input')])\\n\",\n",
    "            \"output_schema = Schema([ColSpec(DataType.string, 'output')])\\n\",\n",
    "            \"signature = ModelSignature(inputs=input_schema, outputs=output_schema)\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Create example input\\n\",\n",
    "            \"input_example = pd.DataFrame({'input': ['example input data']})\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model signature defined')\\n\",\n",
    "            \"print(f'Input schema: {input_schema}')\\n\",\n",
    "            \"print(f'Output schema: {output_schema}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 6: Register model\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Set MLflow tracking\\n\",\n",
    "            \"mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\\n\",\n",
    "            \"mlflow.set_experiment(EXPERIMENT_NAME)\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Registering model: {MODEL_NAME}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Start MLflow run\\n\",\n",
    "            \"with mlflow.start_run(run_name=f\\\"{MODEL_NAME}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\\\") as run:\\n\",\n",
    "            \"    # Log parameters\\n\",\n",
    "            \"    mlflow.log_param('model_version', MODEL_VERSION)\\n\",\n",
    "            \"    mlflow.log_param('student_name', STUDENT_NAME)\\n\",\n",
    "            \"    mlflow.log_param('project_title', PROJECT_TITLE)\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    # Log model\\n\",\n",
    "            \"    mlflow.pyfunc.log_model(\\n\",\n",
    "            \"        artifact_path='model',\\n\",\n",
    "            \"        python_model=CustomModelWrapper(),\\n\",\n",
    "            \"        signature=signature,\\n\",\n",
    "            \"        input_example=input_example,\\n\",\n",
    "            \"        registered_model_name=MODEL_NAME\\n\",\n",
    "            \"    )\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    print(f'âœ“ Model registered: {MODEL_NAME}')\\n\",\n",
    "            \"    print(f'âœ“ Run ID: {run.info.run_id}')\\n\",\n",
    "            \"    print(f'âœ“ Check HP AI Studio Deployments tab')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 7: Verification\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Verify registration\\n\",\n",
    "            \"client = mlflow.tracking.MlflowClient()\\n\",\n",
    "            \"model_versions = client.search_model_versions(f\\\"name='{MODEL_NAME}'\\\")\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Model: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Versions registered: {len(model_versions)}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"for mv in model_versions:\\n\",\n",
    "            \"    print(f\\\"\\\\nVersion: {mv.version}\\\")\\n\",\n",
    "            \"    print(f\\\"Stage: {mv.current_stage}\\\")\\n\",\n",
    "            \"    print(f\\\"Status: {mv.status}\\\")\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save notebook\n",
    "    notebook_path = Path(\"Register_Model.ipynb\")\n",
    "    with open(notebook_path, 'w') as f:\n",
    "        json.dump(notebook, f, indent=2)\n",
    "    \n",
    "    return notebook_path\n",
    "\n",
    "# Create the notebook\n",
    "try:\n",
    "    notebook_path = create_register_notebook()\n",
    "    print(f\"âœ“ Created: {notebook_path}\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Open Register_Model.ipynb\")\n",
    "    print(\"2. Update configuration with your model details\")\n",
    "    print(\"3. Run all cells to register your model\")\n",
    "    print(\"4. Check HP AI Studio Deployments tab\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating notebook: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2a60c9-4ea5-4721-aa2b-aa450163f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HUGGING FACE AUTHENTICATION\n",
      "============================================================\n",
      "\n",
      "Why authenticate with Hugging Face?\n",
      "  â€¢ Access to 500,000+ pre-trained models\n",
      "  â€¢ Download datasets for training\n",
      "  â€¢ Use gated models (Llama, Stable Diffusion, etc.)\n",
      "  â€¢ Share your trained models (optional)\n",
      "\n",
      "âœ“ Already logged in as: Riya119\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Continue with this account? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using existing authentication\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HUGGING FACE AUTHENTICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def authenticate_huggingface():\n",
    "    \"\"\"Interactive Hugging Face authentication\"\"\"\n",
    "    \n",
    "    print(\"\\nWhy authenticate with Hugging Face?\")\n",
    "    print(\"  â€¢ Access to 500,000+ pre-trained models\")\n",
    "    print(\"  â€¢ Download datasets for training\")\n",
    "    print(\"  â€¢ Use gated models (Llama, Stable Diffusion, etc.)\")\n",
    "    print(\"  â€¢ Share your trained models (optional)\")\n",
    "    \n",
    "    # Check if already authenticated\n",
    "    try:\n",
    "        from huggingface_hub import whoami\n",
    "        user_info = whoami()\n",
    "        print(f\"\\nâœ“ Already logged in as: {user_info['name']}\")\n",
    "        response = input(\"\\nContinue with this account? (y/n): \").lower()\n",
    "        if response == 'y':\n",
    "            print(\"âœ“ Using existing authentication\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"\\nâ€¢ No existing Hugging Face login found\")\n",
    "    \n",
    "    # Get authentication token\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"HOW TO GET YOUR HUGGING FACE TOKEN:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"1. Go to: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Click 'Create new token'\")\n",
    "    print(\"3. Name it: 'HP-AI-Studio-Student'\")\n",
    "    print(\"4. Select: 'Read' access (or 'Write' if you'll publish models)\")\n",
    "    print(\"5. Click 'Create token'\")\n",
    "    print(\"6. Copy the token (it looks like: hf_xxxxxxxxxxxxxxxxxxxxx)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    choice = input(\"\\nDo you want to authenticate now? (y/n): \").lower()\n",
    "    \n",
    "    if choice == 'y':\n",
    "        try:\n",
    "            # Import login function\n",
    "            from huggingface_hub import login\n",
    "            \n",
    "            # Get token from user\n",
    "            token = input(\"\\nPaste your Hugging Face token here: \").strip()\n",
    "            \n",
    "            # Validate token format\n",
    "            if not token.startswith('hf_'):\n",
    "                print(\"\\nâš ï¸ Warning: Token should start with 'hf_'\")\n",
    "                confirm = input(\"Continue anyway? (y/n): \").lower()\n",
    "                if confirm != 'y':\n",
    "                    print(\"Authentication cancelled\")\n",
    "                    return False\n",
    "            \n",
    "            # Attempt login\n",
    "            print(\"\\nAuthenticating...\")\n",
    "            login(token=token, add_to_git_credential=True)\n",
    "            \n",
    "            # Verify authentication\n",
    "            from huggingface_hub import whoami\n",
    "            user_info = whoami()\n",
    "            \n",
    "            print(f\"\\nâœ“ Successfully authenticated as: {user_info['name']}\")\n",
    "            print(\"âœ“ You can now access Hugging Face models and datasets\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Authentication failed: {e}\")\n",
    "            print(\"\\nTroubleshooting:\")\n",
    "            print(\"  1. Verify token is correct\")\n",
    "            print(\"  2. Check token has required permissions\")\n",
    "            print(\"  3. Try creating a new token\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"\\nâ„¹ï¸ Skipping authentication\")\n",
    "        print(\"You can authenticate later by running:\")\n",
    "        print(\"  from huggingface_hub import login\")\n",
    "        print(\"  login()\")\n",
    "        return False\n",
    "\n",
    "# Run authentication\n",
    "hf_authenticated = authenticate_huggingface()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf5151d-fe33-4593-8c22-c6d321d31965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ‰ SETUP COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Your HP AI Studio environment is configured and ready.\n",
      "All core dependencies are installed and tested.\n",
      "\n",
      "âœ“ GPU: Detected and functional\n",
      "âœ“ Hugging Face: Authenticated\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS FOR YOUR AI PROJECT:\n",
      "============================================================\n",
      "\n",
      "1. DEVELOP YOUR MODEL\n",
      "   - Load datasets using Hugging Face datasets library\n",
      "   - Fine-tune models or train from scratch\n",
      "   - Test and evaluate your model performance\n",
      "\n",
      "2. SAVE YOUR MODEL\n",
      "   - Use torch.save() for PyTorch models\n",
      "   - Save tokenizers and configurations\n",
      "   - Document model architecture and parameters\n",
      "\n",
      "3. REGISTER FOR DEPLOYMENT\n",
      "   - Open Register_Model.ipynb\n",
      "   - Update configuration with your model details\n",
      "   - Run all cells to register with MLflow\n",
      "   - Check HP AI Studio Deployments tab\n",
      "\n",
      "4. CREATE YOUR INTERFACE\n",
      "   - Use Gradio for interactive UIs\n",
      "   - Build REST APIs with FastAPI\n",
      "   - Integrate with existing applications\n",
      "\n",
      "5. DOCUMENT YOUR WORK\n",
      "   - Keep a development journal\n",
      "   - Screenshot important results\n",
      "   - Record process and iterations\n",
      "   - Prepare portfolio presentation\n",
      "\n",
      "============================================================\n",
      "HELPFUL RESOURCES:\n",
      "============================================================\n",
      "  â€¢ HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\n",
      "  â€¢ Hugging Face: https://huggingface.co/\n",
      "  â€¢ MLflow Documentation: https://mlflow.org/docs/latest/\n",
      "  â€¢ PyTorch Tutorials: https://pytorch.org/tutorials/\n",
      "  â€¢ Gradio Documentation: https://gradio.app/docs/\n",
      "\n",
      "============================================================\n",
      "REMEMBER:\n",
      "============================================================\n",
      "  â€¢ Save your work frequently (Ctrl+S)\n",
      "  â€¢ Document your process in your project journal\n",
      "  â€¢ Test on small datasets before full training\n",
      "  â€¢ Ask for help in office hours if needed\n",
      "  â€¢ Clear GPU memory: torch.cuda.empty_cache()\n",
      "\n",
      "âœ“ You're ready to begin your AI project!\n",
      "  Good luck with your creative AI development!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYour HP AI Studio environment is configured and ready.\")\n",
    "print(\"All core dependencies are installed and tested.\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(\"\\nâœ“ GPU: Detected and functional\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ GPU: Not detected (using CPU mode)\")\n",
    "\n",
    "if hf_authenticated:\n",
    "    print(\"âœ“ Hugging Face: Authenticated\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Hugging Face: Not authenticated (optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS FOR YOUR AI PROJECT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. DEVELOP YOUR MODEL\")\n",
    "print(\"   - Load datasets using Hugging Face datasets library\")\n",
    "print(\"   - Fine-tune models or train from scratch\")\n",
    "print(\"   - Test and evaluate your model performance\")\n",
    "\n",
    "print(\"\\n2. SAVE YOUR MODEL\")\n",
    "print(\"   - Use torch.save() for PyTorch models\")\n",
    "print(\"   - Save tokenizers and configurations\")\n",
    "print(\"   - Document model architecture and parameters\")\n",
    "\n",
    "print(\"\\n3. REGISTER FOR DEPLOYMENT\")\n",
    "print(\"   - Open Register_Model.ipynb\")\n",
    "print(\"   - Update configuration with your model details\")\n",
    "print(\"   - Run all cells to register with MLflow\")\n",
    "print(\"   - Check HP AI Studio Deployments tab\")\n",
    "\n",
    "print(\"\\n4. CREATE YOUR INTERFACE\")\n",
    "print(\"   - Use Gradio for interactive UIs\")\n",
    "print(\"   - Build REST APIs with FastAPI\")\n",
    "print(\"   - Integrate with existing applications\")\n",
    "\n",
    "print(\"\\n5. DOCUMENT YOUR WORK\")\n",
    "print(\"   - Keep a development journal\")\n",
    "print(\"   - Screenshot important results\")\n",
    "print(\"   - Record process and iterations\")\n",
    "print(\"   - Prepare portfolio presentation\")\n",
    "\n",
    "if not hf_authenticated:\n",
    "    print(\"\\nâš ï¸ RECOMMENDATION:\")\n",
    "    print(\"   Run Cell 7 again to set up Hugging Face authentication\")\n",
    "    print(\"   This will give you access to more models and datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HELPFUL RESOURCES:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  â€¢ HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\")\n",
    "print(\"  â€¢ Hugging Face: https://huggingface.co/\")\n",
    "print(\"  â€¢ MLflow Documentation: https://mlflow.org/docs/latest/\")\n",
    "print(\"  â€¢ PyTorch Tutorials: https://pytorch.org/tutorials/\")\n",
    "print(\"  â€¢ Gradio Documentation: https://gradio.app/docs/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REMEMBER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  â€¢ Save your work frequently (Ctrl+S)\")\n",
    "print(\"  â€¢ Document your process in your project journal\")\n",
    "print(\"  â€¢ Test on small datasets before full training\")\n",
    "print(\"  â€¢ Ask for help in office hours if needed\")\n",
    "print(\"  â€¢ Clear GPU memory: torch.cuda.empty_cache()\")\n",
    "\n",
    "print(\"\\nâœ“ You're ready to begin your AI project!\")\n",
    "print(\"  Good luck with your creative AI development!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38510d87-3557-4279-88eb-1ad092ffff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /opt/conda/lib/python3.12/site-packages (37.12.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (3.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.12/site-packages (from faker) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.10.0.dev20251030+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: pytorch-triton==3.5.0+git7416ffcb in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.0+git7416ffcb)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install faker networkx matplotlib scikit-learn sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc94fb84-de70-4283-be33-618f1d7e6d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created folders:\n",
      "  â€¢ synthetic_data/ (for generated boards)\n",
      "  â€¢ models/ (for saving trained models)\n",
      "  â€¢ outputs/ (for results)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create folders for your project\n",
    "os.makedirs('synthetic_data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Created folders:\")\n",
    "print(\"  â€¢ synthetic_data/ (for generated boards)\")\n",
    "print(\"  â€¢ models/ (for saving trained models)\")\n",
    "print(\"  â€¢ outputs/ (for results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3787a94-7c5e-49bd-9936-b130e0b67225",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2382462651.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgenerator_code = import json\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# This creates the generator file\n",
    "generator_code = import json\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "\n",
    "class StickyNoteType(Enum):\n",
    "    \"\"\"Standardized sticky note categories\"\"\"\n",
    "    PAIN_POINT = \"pain_point\"\n",
    "    OPPORTUNITY = \"opportunity\"\n",
    "    OBSERVATION = \"observation\"\n",
    "    QUOTE = \"quote\"\n",
    "    IDEA = \"idea\"\n",
    "    QUESTION = \"question\"\n",
    "    TASK = \"task\"\n",
    "    DECISION = \"decision\"\n",
    "\n",
    "class ResearchScenario(Enum):\n",
    "    \"\"\"Different research contexts to generate\"\"\"\n",
    "    USABILITY_TEST = \"usability_testing\"\n",
    "    USER_INTERVIEW = \"user_interview\"\n",
    "    JOURNEY_MAP = \"journey_mapping\"\n",
    "    COMPETITIVE_ANALYSIS = \"competitive_analysis\"\n",
    "    AFFINITY_MAP = \"affinity_mapping\"\n",
    "    WORKSHOP = \"design_workshop\"\n",
    "    SURVEY_SYNTHESIS = \"survey_synthesis\"\n",
    "\n",
    "class FigJamColorScheme:\n",
    "    \"\"\"Standard FigJam color meanings\"\"\"\n",
    "    COLORS = {\n",
    "        \"red\": {\"hex\": \"#FF6B6B\", \"typical_use\": [\"pain_point\", \"critical\", \"blocker\"]},\n",
    "        \"yellow\": {\"hex\": \"#FFD93D\", \"typical_use\": [\"observation\", \"neutral\", \"note\"]},\n",
    "        \"green\": {\"hex\": \"#6BCF7F\", \"typical_use\": [\"opportunity\", \"positive\", \"success\"]},\n",
    "        \"blue\": {\"hex\": \"#4A90E2\", \"typical_use\": [\"question\", \"information\", \"process\"]},\n",
    "        \"purple\": {\"hex\": \"#9B59B6\", \"typical_use\": [\"idea\", \"innovation\", \"concept\"]},\n",
    "        \"orange\": {\"hex\": \"#FF8C42\", \"typical_use\": [\"warning\", \"attention\", \"important\"]},\n",
    "        \"pink\": {\"hex\": \"#FF69B4\", \"typical_use\": [\"quote\", \"user_voice\", \"feedback\"]},\n",
    "        \"gray\": {\"hex\": \"#95A5A6\", \"typical_use\": [\"context\", \"background\", \"archived\"]}\n",
    "    }\n",
    "\n",
    "class SyntheticFigJamGenerator:\n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        \"\"\"Initialize the generator with optional seed for reproducibility\"\"\"\n",
    "        if seed:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.faker = Faker()\n",
    "        if seed:\n",
    "            Faker.seed(seed)\n",
    "        \n",
    "        # Research-specific vocabulary\n",
    "        self.ux_vocabulary = {\n",
    "            \"pain_points\": [\n",
    "                \"confusing navigation\", \"slow loading times\", \"unclear messaging\",\n",
    "                \"missing features\", \"broken flow\", \"inconsistent design\",\n",
    "                \"poor mobile experience\", \"accessibility issues\", \"data loss\",\n",
    "                \"complex onboarding\", \"hidden functionality\", \"error handling\"\n",
    "            ],\n",
    "            \"opportunities\": [\n",
    "                \"streamline process\", \"add shortcuts\", \"improve clarity\",\n",
    "                \"personalization\", \"automation potential\", \"better feedback\",\n",
    "                \"progressive disclosure\", \"gamification\", \"social features\",\n",
    "                \"AI assistance\", \"predictive features\", \"contextual help\"\n",
    "            ],\n",
    "            \"user_emotions\": [\n",
    "                \"frustrated\", \"confused\", \"delighted\", \"anxious\", \"confident\",\n",
    "                \"overwhelmed\", \"satisfied\", \"disappointed\", \"curious\", \"engaged\"\n",
    "            ],\n",
    "            \"actions\": [\n",
    "                \"clicked\", \"scrolled\", \"searched\", \"filtered\", \"uploaded\",\n",
    "                \"shared\", \"saved\", \"deleted\", \"exported\", \"configured\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def generate_sticky_note(self, \n",
    "                            note_type: StickyNoteType,\n",
    "                            scenario: ResearchScenario,\n",
    "                            severity: float = 0.5) -> Dict:\n",
    "        \"\"\"Generate a single sticky note with realistic UX research content\"\"\"\n",
    "        \n",
    "        # Select appropriate color based on note type\n",
    "        color = self._select_color(note_type, severity)\n",
    "        \n",
    "        # Generate contextual content\n",
    "        content = self._generate_content(note_type, scenario, severity)\n",
    "        \n",
    "        # Add metadata\n",
    "        note = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"type\": note_type.value,\n",
    "            \"content\": content,\n",
    "            \"color\": color,\n",
    "            \"position\": {\"x\": 0, \"y\": 0},  # Will be set by layout algorithm\n",
    "            \"severity\": severity,\n",
    "            \"confidence\": random.uniform(0.6, 1.0),\n",
    "            \"word_count\": len(content.split()),\n",
    "            \"has_question\": \"?\" in content,\n",
    "            \"has_quote\": '\"' in content,\n",
    "            \"timestamp\": self.faker.date_time_between(\"-30d\", \"now\").isoformat(),\n",
    "            \"tags\": self._extract_tags(content)\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _select_color(self, note_type: StickyNoteType, severity: float) -> str:\n",
    "        \"\"\"Select appropriate color based on type and severity\"\"\"\n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            return \"red\" if severity > 0.7 else \"orange\"\n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            return \"green\"\n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            return \"pink\"\n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return \"blue\"\n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return \"purple\"\n",
    "        else:\n",
    "            return \"yellow\"\n",
    "    \n",
    "    def _generate_content(self, \n",
    "                         note_type: StickyNoteType, \n",
    "                         scenario: ResearchScenario,\n",
    "                         severity: float) -> str:\n",
    "        \"\"\"Generate realistic content based on type and scenario\"\"\"\n",
    "        \n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            pain = random.choice(self.ux_vocabulary[\"pain_points\"])\n",
    "            emotion = random.choice(self.ux_vocabulary[\"user_emotions\"][:5])  # Negative emotions\n",
    "            if severity > 0.7:\n",
    "                return f\"CRITICAL: Users {emotion} - {pain}\"\n",
    "            return f\"User reported: {pain}\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            opp = random.choice(self.ux_vocabulary[\"opportunities\"])\n",
    "            return f\"Opportunity: {opp} - could improve user satisfaction\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            action = random.choice(self.ux_vocabulary[\"actions\"])\n",
    "            return f'\"I {action} but couldn\\'t find what I needed\" - P{random.randint(1,12)}'\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return f\"Why do users {random.choice(self.ux_vocabulary['actions'])} here?\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return f\"Idea: {random.choice(self.ux_vocabulary['opportunities'])}\"\n",
    "            \n",
    "        else:\n",
    "            return f\"Observed: User {random.choice(self.ux_vocabulary['actions'])} {random.randint(2,8)} times\"\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract relevant tags from content\"\"\"\n",
    "        tags = []\n",
    "        \n",
    "        # Check for keywords\n",
    "        for keyword in [\"critical\", \"blocker\", \"opportunity\", \"idea\"]:\n",
    "            if keyword.lower() in content.lower():\n",
    "                tags.append(keyword)\n",
    "        \n",
    "        # Check for user references\n",
    "        if \"P\" in content and any(char.isdigit() for char in content):\n",
    "            tags.append(\"user_quote\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def generate_board(self,\n",
    "                      scenario: ResearchScenario,\n",
    "                      num_notes: int = 50,\n",
    "                      clustering_strength: float = 0.7) -> Dict:\n",
    "        \"\"\"Generate a complete FigJam-like board with clustered notes\"\"\"\n",
    "        \n",
    "        # Determine note distribution based on scenario\n",
    "        distribution = self._get_note_distribution(scenario)\n",
    "        \n",
    "        # Generate notes\n",
    "        notes = []\n",
    "        for note_type, count in distribution.items():\n",
    "            actual_count = int(num_notes * count)\n",
    "            for _ in range(actual_count):\n",
    "                severity = np.random.beta(2, 5)  # Skewed towards lower severity\n",
    "                note = self.generate_sticky_note(note_type, scenario, severity)\n",
    "                notes.append(note)\n",
    "        \n",
    "        # Create spatial clusters\n",
    "        clusters = self._create_clusters(notes, clustering_strength)\n",
    "        \n",
    "        # Apply layout algorithm\n",
    "        self._apply_layout(notes, clusters)\n",
    "        \n",
    "        # Generate connections\n",
    "        connections = self._generate_connections(notes, clusters)\n",
    "        \n",
    "        # Create board metadata\n",
    "        board = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"scenario\": scenario.value,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"note_count\": len(notes),\n",
    "            \"notes\": notes,\n",
    "            \"clusters\": clusters,\n",
    "            \"connections\": connections,\n",
    "            \"metadata\": {\n",
    "                \"clustering_strength\": clustering_strength,\n",
    "                \"dominant_sentiment\": self._calculate_sentiment(notes),\n",
    "                \"key_themes\": self._extract_themes(notes),\n",
    "                \"severity_distribution\": self._severity_distribution(notes)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _get_note_distribution(self, scenario: ResearchScenario) -> Dict[StickyNoteType, float]:\n",
    "        \"\"\"Define note type distribution based on research scenario\"\"\"\n",
    "        distributions = {\n",
    "            ResearchScenario.USABILITY_TEST: {\n",
    "                StickyNoteType.PAIN_POINT: 0.35,\n",
    "                StickyNoteType.OBSERVATION: 0.25,\n",
    "                StickyNoteType.QUOTE: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.15,\n",
    "                StickyNoteType.QUESTION: 0.05\n",
    "            },\n",
    "            ResearchScenario.USER_INTERVIEW: {\n",
    "                StickyNoteType.QUOTE: 0.40,\n",
    "                StickyNoteType.OBSERVATION: 0.20,\n",
    "                StickyNoteType.PAIN_POINT: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.10,\n",
    "                StickyNoteType.IDEA: 0.10\n",
    "            },\n",
    "            ResearchScenario.JOURNEY_MAP: {\n",
    "                StickyNoteType.OBSERVATION: 0.30,\n",
    "                StickyNoteType.PAIN_POINT: 0.25,\n",
    "                StickyNoteType.OPPORTUNITY: 0.20,\n",
    "                StickyNoteType.QUOTE: 0.15,\n",
    "                StickyNoteType.TASK: 0.10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return distributions.get(scenario, {\n",
    "            StickyNoteType.OBSERVATION: 0.30,\n",
    "            StickyNoteType.PAIN_POINT: 0.25,\n",
    "            StickyNoteType.OPPORTUNITY: 0.20,\n",
    "            StickyNoteType.QUOTE: 0.15,\n",
    "            StickyNoteType.IDEA: 0.10\n",
    "        })\n",
    "    \n",
    "    def _create_clusters(self, notes: List[Dict], strength: float) -> List[Dict]:\n",
    "        \"\"\"Group related notes into clusters\"\"\"\n",
    "        # Simplified clustering - group by note type and severity\n",
    "        clusters = []\n",
    "        \n",
    "        # Group notes by type\n",
    "        type_groups = {}\n",
    "        for note in notes:\n",
    "            note_type = note[\"type\"]\n",
    "            if note_type not in type_groups:\n",
    "                type_groups[note_type] = []\n",
    "            type_groups[note_type].append(note[\"id\"])\n",
    "        \n",
    "        # Create cluster definitions\n",
    "        for cluster_type, note_ids in type_groups.items():\n",
    "            clusters.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": cluster_type,\n",
    "                \"note_ids\": note_ids,\n",
    "                \"center\": {\"x\": random.randint(100, 900), \"y\": random.randint(100, 700)},\n",
    "                \"radius\": 150 + len(note_ids) * 10\n",
    "            })\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _apply_layout(self, notes: List[Dict], clusters: List[Dict]):\n",
    "        \"\"\"Apply spatial layout to notes based on clusters\"\"\"\n",
    "        for cluster in clusters:\n",
    "            center_x = cluster[\"center\"][\"x\"]\n",
    "            center_y = cluster[\"center\"][\"y\"]\n",
    "            radius = cluster[\"radius\"]\n",
    "            \n",
    "            cluster_notes = [n for n in notes if n[\"id\"] in cluster[\"note_ids\"]]\n",
    "            \n",
    "            # Arrange notes in cluster\n",
    "            for i, note in enumerate(cluster_notes):\n",
    "                angle = (2 * np.pi * i) / len(cluster_notes)\n",
    "                r = radius * (0.5 + 0.5 * random.random())  # Vary radius\n",
    "                \n",
    "                note[\"position\"][\"x\"] = int(center_x + r * np.cos(angle))\n",
    "                note[\"position\"][\"y\"] = int(center_y + r * np.sin(angle))\n",
    "    \n",
    "    def _generate_connections(self, notes: List[Dict], clusters: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate meaningful connections between related notes\"\"\"\n",
    "        connections = []\n",
    "        \n",
    "        # Connect high-severity pain points to opportunities\n",
    "        pain_points = [n for n in notes if n[\"type\"] == \"pain_point\" and n[\"severity\"] > 0.6]\n",
    "        opportunities = [n for n in notes if n[\"type\"] == \"opportunity\"]\n",
    "        \n",
    "        for pain in pain_points[:5]:  # Limit connections\n",
    "            if opportunities:\n",
    "                opp = random.choice(opportunities)\n",
    "                connections.append({\n",
    "                    \"from\": pain[\"id\"],\n",
    "                    \"to\": opp[\"id\"],\n",
    "                    \"type\": \"addresses\",\n",
    "                    \"strength\": pain[\"severity\"]\n",
    "                })\n",
    "        \n",
    "        return connections\n",
    "    \n",
    "    def _calculate_sentiment(self, notes: List[Dict]) -> str:\n",
    "        \"\"\"Calculate overall board sentiment\"\"\"\n",
    "        pain_count = sum(1 for n in notes if n[\"type\"] == \"pain_point\")\n",
    "        opp_count = sum(1 for n in notes if n[\"type\"] == \"opportunity\")\n",
    "        \n",
    "        if pain_count > opp_count * 1.5:\n",
    "            return \"critical\"\n",
    "        elif opp_count > pain_count:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    \n",
    "    def _extract_themes(self, notes: List[Dict]) -> List[str]:\n",
    "        \"\"\"Extract key themes from all notes\"\"\"\n",
    "        themes = set()\n",
    "        for note in notes:\n",
    "            themes.update(note.get(\"tags\", []))\n",
    "        return list(themes)[:5]  # Top 5 themes\n",
    "    \n",
    "    def _severity_distribution(self, notes: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate severity distribution\"\"\"\n",
    "        dist = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "        for note in notes:\n",
    "            sev = note.get(\"severity\", 0.5)\n",
    "            if sev < 0.33:\n",
    "                dist[\"low\"] += 1\n",
    "            elif sev < 0.67:\n",
    "                dist[\"medium\"] += 1\n",
    "            else:\n",
    "                dist[\"high\"] += 1\n",
    "        return dist\n",
    "    \n",
    "    def export_board(self, board: Dict, format: str = \"json\") -> str:\n",
    "        \"\"\"Export board to specified format\"\"\"\n",
    "        if format == \"json\":\n",
    "            return json.dumps(board, indent=2)\n",
    "        elif format == \"csv\":\n",
    "            # Simplified CSV export of notes only\n",
    "            import csv\n",
    "            import io\n",
    "            output = io.StringIO()\n",
    "            writer = csv.DictWriter(output, \n",
    "                fieldnames=[\"id\", \"type\", \"content\", \"color\", \"severity\"])\n",
    "            writer.writeheader()\n",
    "            for note in board[\"notes\"]:\n",
    "                writer.writerow({\n",
    "                    \"id\": note[\"id\"],\n",
    "                    \"type\": note[\"type\"],\n",
    "                    \"content\": note[\"content\"],\n",
    "                    \"color\": note[\"color\"],\n",
    "                    \"severity\": note[\"severity\"]\n",
    "                })\n",
    "            return output.getvalue()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create generator instance\n",
    "    generator = SyntheticFigJamGenerator(seed=42)\n",
    "    \n",
    "    # Generate a board for usability testing scenario\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=75,\n",
    "        clustering_strength=0.8\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Generated board with {board['note_count']} notes\")\n",
    "    print(f\"Scenario: {board['scenario']}\")\n",
    "    print(f\"Sentiment: {board['metadata']['dominant_sentiment']}\")\n",
    "    print(f\"Themes: {board['metadata']['key_themes']}\")\n",
    "    print(f\"Severity distribution: {board['metadata']['severity_distribution']}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_output = generator.export_board(board, format=\"json\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"synthetic_figjam_board.json\", \"w\") as f:\n",
    "        f.write(json_output)\n",
    "    \n",
    "    print(\"\\nBoard exported to synthetic_figjam_board.json\")\n",
    "\n",
    "with open('synthetic_figjam_generator.py', 'w') as f:\n",
    "    f.write(generator_code)\n",
    "\n",
    "print(\"âœ“ Generator code saved as synthetic_figjam_generator.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3f583-275f-460b-8e9d-9d07787927d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "\n",
    "class StickyNoteType(Enum):\n",
    "    \"\"\"Standardized sticky note categories\"\"\"\n",
    "    PAIN_POINT = \"pain_point\"\n",
    "    OPPORTUNITY = \"opportunity\"\n",
    "    OBSERVATION = \"observation\"\n",
    "    QUOTE = \"quote\"\n",
    "    IDEA = \"idea\"\n",
    "    QUESTION = \"question\"\n",
    "    TASK = \"task\"\n",
    "    DECISION = \"decision\"\n",
    "\n",
    "class ResearchScenario(Enum):\n",
    "    \"\"\"Different research contexts to generate\"\"\"\n",
    "    USABILITY_TEST = \"usability_testing\"\n",
    "    USER_INTERVIEW = \"user_interview\"\n",
    "    JOURNEY_MAP = \"journey_mapping\"\n",
    "    COMPETITIVE_ANALYSIS = \"competitive_analysis\"\n",
    "    AFFINITY_MAP = \"affinity_mapping\"\n",
    "    WORKSHOP = \"design_workshop\"\n",
    "    SURVEY_SYNTHESIS = \"survey_synthesis\"\n",
    "\n",
    "class FigJamColorScheme:\n",
    "    \"\"\"Standard FigJam color meanings\"\"\"\n",
    "    COLORS = {\n",
    "        \"red\": {\"hex\": \"#FF6B6B\", \"typical_use\": [\"pain_point\", \"critical\", \"blocker\"]},\n",
    "        \"yellow\": {\"hex\": \"#FFD93D\", \"typical_use\": [\"observation\", \"neutral\", \"note\"]},\n",
    "        \"green\": {\"hex\": \"#6BCF7F\", \"typical_use\": [\"opportunity\", \"positive\", \"success\"]},\n",
    "        \"blue\": {\"hex\": \"#4A90E2\", \"typical_use\": [\"question\", \"information\", \"process\"]},\n",
    "        \"purple\": {\"hex\": \"#9B59B6\", \"typical_use\": [\"idea\", \"innovation\", \"concept\"]},\n",
    "        \"orange\": {\"hex\": \"#FF8C42\", \"typical_use\": [\"warning\", \"attention\", \"important\"]},\n",
    "        \"pink\": {\"hex\": \"#FF69B4\", \"typical_use\": [\"quote\", \"user_voice\", \"feedback\"]},\n",
    "        \"gray\": {\"hex\": \"#95A5A6\", \"typical_use\": [\"context\", \"background\", \"archived\"]}\n",
    "    }\n",
    "\n",
    "class SyntheticFigJamGenerator:\n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        \"\"\"Initialize the generator with optional seed for reproducibility\"\"\"\n",
    "        if seed:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.faker = Faker()\n",
    "        if seed:\n",
    "            Faker.seed(seed)\n",
    "        \n",
    "        # Research-specific vocabulary\n",
    "        self.ux_vocabulary = {\n",
    "            \"pain_points\": [\n",
    "                \"confusing navigation\", \"slow loading times\", \"unclear messaging\",\n",
    "                \"missing features\", \"broken flow\", \"inconsistent design\",\n",
    "                \"poor mobile experience\", \"accessibility issues\", \"data loss\",\n",
    "                \"complex onboarding\", \"hidden functionality\", \"error handling\"\n",
    "            ],\n",
    "            \"opportunities\": [\n",
    "                \"streamline process\", \"add shortcuts\", \"improve clarity\",\n",
    "                \"personalization\", \"automation potential\", \"better feedback\",\n",
    "                \"progressive disclosure\", \"gamification\", \"social features\",\n",
    "                \"AI assistance\", \"predictive features\", \"contextual help\"\n",
    "            ],\n",
    "            \"user_emotions\": [\n",
    "                \"frustrated\", \"confused\", \"delighted\", \"anxious\", \"confident\",\n",
    "                \"overwhelmed\", \"satisfied\", \"disappointed\", \"curious\", \"engaged\"\n",
    "            ],\n",
    "            \"actions\": [\n",
    "                \"clicked\", \"scrolled\", \"searched\", \"filtered\", \"uploaded\",\n",
    "                \"shared\", \"saved\", \"deleted\", \"exported\", \"configured\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def generate_sticky_note(self, \n",
    "                            note_type: StickyNoteType,\n",
    "                            scenario: ResearchScenario,\n",
    "                            severity: float = 0.5) -> Dict:\n",
    "        \"\"\"Generate a single sticky note with realistic UX research content\"\"\"\n",
    "        \n",
    "        # Select appropriate color based on note type\n",
    "        color = self._select_color(note_type, severity)\n",
    "        \n",
    "        # Generate contextual content\n",
    "        content = self._generate_content(note_type, scenario, severity)\n",
    "        \n",
    "        # Add metadata\n",
    "        note = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"type\": note_type.value,\n",
    "            \"content\": content,\n",
    "            \"color\": color,\n",
    "            \"position\": {\"x\": 0, \"y\": 0},  # Will be set by layout algorithm\n",
    "            \"severity\": severity,\n",
    "            \"confidence\": random.uniform(0.6, 1.0),\n",
    "            \"word_count\": len(content.split()),\n",
    "            \"has_question\": \"?\" in content,\n",
    "            \"has_quote\": '\"' in content,\n",
    "            \"timestamp\": self.faker.date_time_between(\"-30d\", \"now\").isoformat(),\n",
    "            \"tags\": self._extract_tags(content)\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _select_color(self, note_type: StickyNoteType, severity: float) -> str:\n",
    "        \"\"\"Select appropriate color based on type and severity\"\"\"\n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            return \"red\" if severity > 0.7 else \"orange\"\n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            return \"green\"\n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            return \"pink\"\n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return \"blue\"\n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return \"purple\"\n",
    "        else:\n",
    "            return \"yellow\"\n",
    "    \n",
    "    def _generate_content(self, \n",
    "                         note_type: StickyNoteType, \n",
    "                         scenario: ResearchScenario,\n",
    "                         severity: float) -> str:\n",
    "        \"\"\"Generate realistic content based on type and scenario\"\"\"\n",
    "        \n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            pain = random.choice(self.ux_vocabulary[\"pain_points\"])\n",
    "            emotion = random.choice(self.ux_vocabulary[\"user_emotions\"][:5])  # Negative emotions\n",
    "            if severity > 0.7:\n",
    "                return f\"CRITICAL: Users {emotion} - {pain}\"\n",
    "            return f\"User reported: {pain}\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            opp = random.choice(self.ux_vocabulary[\"opportunities\"])\n",
    "            return f\"Opportunity: {opp} - could improve user satisfaction\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            action = random.choice(self.ux_vocabulary[\"actions\"])\n",
    "            return f'\"I {action} but couldn\\'t find what I needed\" - P{random.randint(1,12)}'\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return f\"Why do users {random.choice(self.ux_vocabulary['actions'])} here?\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return f\"Idea: {random.choice(self.ux_vocabulary['opportunities'])}\"\n",
    "            \n",
    "        else:\n",
    "            return f\"Observed: User {random.choice(self.ux_vocabulary['actions'])} {random.randint(2,8)} times\"\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract relevant tags from content\"\"\"\n",
    "        tags = []\n",
    "        \n",
    "        # Check for keywords\n",
    "        for keyword in [\"critical\", \"blocker\", \"opportunity\", \"idea\"]:\n",
    "            if keyword.lower() in content.lower():\n",
    "                tags.append(keyword)\n",
    "        \n",
    "        # Check for user references\n",
    "        if \"P\" in content and any(char.isdigit() for char in content):\n",
    "            tags.append(\"user_quote\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def generate_board(self,\n",
    "                      scenario: ResearchScenario,\n",
    "                      num_notes: int = 50,\n",
    "                      clustering_strength: float = 0.7) -> Dict:\n",
    "        \"\"\"Generate a complete FigJam-like board with clustered notes\"\"\"\n",
    "        \n",
    "        # Determine note distribution based on scenario\n",
    "        distribution = self._get_note_distribution(scenario)\n",
    "        \n",
    "        # Generate notes\n",
    "        notes = []\n",
    "        for note_type, count in distribution.items():\n",
    "            actual_count = int(num_notes * count)\n",
    "            for _ in range(actual_count):\n",
    "                severity = np.random.beta(2, 5)  # Skewed towards lower severity\n",
    "                note = self.generate_sticky_note(note_type, scenario, severity)\n",
    "                notes.append(note)\n",
    "        \n",
    "        # Create spatial clusters\n",
    "        clusters = self._create_clusters(notes, clustering_strength)\n",
    "        \n",
    "        # Apply layout algorithm\n",
    "        self._apply_layout(notes, clusters)\n",
    "        \n",
    "        # Generate connections\n",
    "        connections = self._generate_connections(notes, clusters)\n",
    "        \n",
    "        # Create board metadata\n",
    "        board = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"scenario\": scenario.value,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"note_count\": len(notes),\n",
    "            \"notes\": notes,\n",
    "            \"clusters\": clusters,\n",
    "            \"connections\": connections,\n",
    "            \"metadata\": {\n",
    "                \"clustering_strength\": clustering_strength,\n",
    "                \"dominant_sentiment\": self._calculate_sentiment(notes),\n",
    "                \"key_themes\": self._extract_themes(notes),\n",
    "                \"severity_distribution\": self._severity_distribution(notes)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _get_note_distribution(self, scenario: ResearchScenario) -> Dict[StickyNoteType, float]:\n",
    "        \"\"\"Define note type distribution based on research scenario\"\"\"\n",
    "        distributions = {\n",
    "            ResearchScenario.USABILITY_TEST: {\n",
    "                StickyNoteType.PAIN_POINT: 0.35,\n",
    "                StickyNoteType.OBSERVATION: 0.25,\n",
    "                StickyNoteType.QUOTE: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.15,\n",
    "                StickyNoteType.QUESTION: 0.05\n",
    "            },\n",
    "            ResearchScenario.USER_INTERVIEW: {\n",
    "                StickyNoteType.QUOTE: 0.40,\n",
    "                StickyNoteType.OBSERVATION: 0.20,\n",
    "                StickyNoteType.PAIN_POINT: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.10,\n",
    "                StickyNoteType.IDEA: 0.10\n",
    "            },\n",
    "            ResearchScenario.JOURNEY_MAP: {\n",
    "                StickyNoteType.OBSERVATION: 0.30,\n",
    "                StickyNoteType.PAIN_POINT: 0.25,\n",
    "                StickyNoteType.OPPORTUNITY: 0.20,\n",
    "                StickyNoteType.QUOTE: 0.15,\n",
    "                StickyNoteType.TASK: 0.10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return distributions.get(scenario, {\n",
    "            StickyNoteType.OBSERVATION: 0.30,\n",
    "            StickyNoteType.PAIN_POINT: 0.25,\n",
    "            StickyNoteType.OPPORTUNITY: 0.20,\n",
    "            StickyNoteType.QUOTE: 0.15,\n",
    "            StickyNoteType.IDEA: 0.10\n",
    "        })\n",
    "    \n",
    "    def _create_clusters(self, notes: List[Dict], strength: float) -> List[Dict]:\n",
    "        \"\"\"Group related notes into clusters\"\"\"\n",
    "        # Simplified clustering - group by note type and severity\n",
    "        clusters = []\n",
    "        \n",
    "        # Group notes by type\n",
    "        type_groups = {}\n",
    "        for note in notes:\n",
    "            note_type = note[\"type\"]\n",
    "            if note_type not in type_groups:\n",
    "                type_groups[note_type] = []\n",
    "            type_groups[note_type].append(note[\"id\"])\n",
    "        \n",
    "        # Create cluster definitions\n",
    "        for cluster_type, note_ids in type_groups.items():\n",
    "            clusters.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": cluster_type,\n",
    "                \"note_ids\": note_ids,\n",
    "                \"center\": {\"x\": random.randint(100, 900), \"y\": random.randint(100, 700)},\n",
    "                \"radius\": 150 + len(note_ids) * 10\n",
    "            })\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _apply_layout(self, notes: List[Dict], clusters: List[Dict]):\n",
    "        \"\"\"Apply spatial layout to notes based on clusters\"\"\"\n",
    "        for cluster in clusters:\n",
    "            center_x = cluster[\"center\"][\"x\"]\n",
    "            center_y = cluster[\"center\"][\"y\"]\n",
    "            radius = cluster[\"radius\"]\n",
    "            \n",
    "            cluster_notes = [n for n in notes if n[\"id\"] in cluster[\"note_ids\"]]\n",
    "            \n",
    "            # Arrange notes in cluster\n",
    "            for i, note in enumerate(cluster_notes):\n",
    "                angle = (2 * np.pi * i) / len(cluster_notes)\n",
    "                r = radius * (0.5 + 0.5 * random.random())  # Vary radius\n",
    "                \n",
    "                note[\"position\"][\"x\"] = int(center_x + r * np.cos(angle))\n",
    "                note[\"position\"][\"y\"] = int(center_y + r * np.sin(angle))\n",
    "    \n",
    "    def _generate_connections(self, notes: List[Dict], clusters: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate meaningful connections between related notes\"\"\"\n",
    "        connections = []\n",
    "        \n",
    "        # Connect high-severity pain points to opportunities\n",
    "        pain_points = [n for n in notes if n[\"type\"] == \"pain_point\" and n[\"severity\"] > 0.6]\n",
    "        opportunities = [n for n in notes if n[\"type\"] == \"opportunity\"]\n",
    "        \n",
    "        for pain in pain_points[:5]:  # Limit connections\n",
    "            if opportunities:\n",
    "                opp = random.choice(opportunities)\n",
    "                connections.append({\n",
    "                    \"from\": pain[\"id\"],\n",
    "                    \"to\": opp[\"id\"],\n",
    "                    \"type\": \"addresses\",\n",
    "                    \"strength\": pain[\"severity\"]\n",
    "                })\n",
    "        \n",
    "        return connections\n",
    "    \n",
    "    def _calculate_sentiment(self, notes: List[Dict]) -> str:\n",
    "        \"\"\"Calculate overall board sentiment\"\"\"\n",
    "        pain_count = sum(1 for n in notes if n[\"type\"] == \"pain_point\")\n",
    "        opp_count = sum(1 for n in notes if n[\"type\"] == \"opportunity\")\n",
    "        \n",
    "        if pain_count > opp_count * 1.5:\n",
    "            return \"critical\"\n",
    "        elif opp_count > pain_count:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    \n",
    "    def _extract_themes(self, notes: List[Dict]) -> List[str]:\n",
    "        \"\"\"Extract key themes from all notes\"\"\"\n",
    "        themes = set()\n",
    "        for note in notes:\n",
    "            themes.update(note.get(\"tags\", []))\n",
    "        return list(themes)[:5]  # Top 5 themes\n",
    "    \n",
    "    def _severity_distribution(self, notes: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate severity distribution\"\"\"\n",
    "        dist = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "        for note in notes:\n",
    "            sev = note.get(\"severity\", 0.5)\n",
    "            if sev < 0.33:\n",
    "                dist[\"low\"] += 1\n",
    "            elif sev < 0.67:\n",
    "                dist[\"medium\"] += 1\n",
    "            else:\n",
    "                dist[\"high\"] += 1\n",
    "        return dist\n",
    "    \n",
    "    def export_board(self, board: Dict, format: str = \"json\") -> str:\n",
    "        \"\"\"Export board to specified format\"\"\"\n",
    "        if format == \"json\":\n",
    "            return json.dumps(board, indent=2)\n",
    "        elif format == \"csv\":\n",
    "            # Simplified CSV export of notes only\n",
    "            import csv\n",
    "            import io\n",
    "            output = io.StringIO()\n",
    "            writer = csv.DictWriter(output, \n",
    "                fieldnames=[\"id\", \"type\", \"content\", \"color\", \"severity\"])\n",
    "            writer.writeheader()\n",
    "            for note in board[\"notes\"]:\n",
    "                writer.writerow({\n",
    "                    \"id\": note[\"id\"],\n",
    "                    \"type\": note[\"type\"],\n",
    "                    \"content\": note[\"content\"],\n",
    "                    \"color\": note[\"color\"],\n",
    "                    \"severity\": note[\"severity\"]\n",
    "                })\n",
    "            return output.getvalue()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create generator instance\n",
    "    generator = SyntheticFigJamGenerator(seed=42)\n",
    "    \n",
    "    # Generate a board for usability testing scenario\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=75,\n",
    "        clustering_strength=0.8\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Generated board with {board['note_count']} notes\")\n",
    "    print(f\"Scenario: {board['scenario']}\")\n",
    "    print(f\"Sentiment: {board['metadata']['dominant_sentiment']}\")\n",
    "    print(f\"Themes: {board['metadata']['key_themes']}\")\n",
    "    print(f\"Severity distribution: {board['metadata']['severity_distribution']}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_output = generator.export_board(board, format=\"json\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"synthetic_figjam_board.json\", \"w\") as f:\n",
    "        f.write(json_output)\n",
    "    \n",
    "    print(\"\\nBoard exported to synthetic_figjam_board.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031fc4c-8f44-4aaf-a06a-725b9bee58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that we can use the generator\n",
    "generator = SyntheticFigJamGenerator(seed=42)\n",
    "\n",
    "# Generate ONE test board\n",
    "test_board = generator.generate_board(\n",
    "    scenario=ResearchScenario.USABILITY_TEST,\n",
    "    num_notes=30,  # Just 30 notes for testing\n",
    "    clustering_strength=0.7\n",
    ")\n",
    "\n",
    "# Check if it worked\n",
    "print(f\"âœ“ Success! Generated {test_board['note_count']} sticky notes\")\n",
    "print(f\"Board type: {test_board['scenario']}\")\n",
    "print(f\"Sentiment: {test_board['metadata']['dominant_sentiment']}\")\n",
    "print(f\"\\nSample notes:\")\n",
    "for note in test_board['notes'][:3]:\n",
    "    print(f\"  - {note['color']} note: {note['content']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8ccdb-f498-4c76-b5c2-93a912c0292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure our folders exist\n",
    "os.makedirs('synthetic_data', exist_ok=True)\n",
    "\n",
    "# Generate 10 different boards\n",
    "boards = []\n",
    "scenarios = [ResearchScenario.USABILITY_TEST, ResearchScenario.USER_INTERVIEW, ResearchScenario.JOURNEY_MAP]\n",
    "\n",
    "print(\"Generating synthetic training data...\")\n",
    "for i in range(10):\n",
    "    # Vary the parameters for diversity\n",
    "    scenario = random.choice(scenarios)\n",
    "    num_notes = random.randint(30, 80)\n",
    "    \n",
    "    board = generator.generate_board(\n",
    "        scenario=scenario,\n",
    "        num_notes=num_notes,\n",
    "        clustering_strength=random.uniform(0.5, 0.9)\n",
    "    )\n",
    "    \n",
    "    boards.append(board)\n",
    "    \n",
    "    # Save each board\n",
    "    filename = f'synthetic_data/board_{i:03d}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(board, f, indent=2)\n",
    "    \n",
    "    print(f\"  Generated board {i+1}/10: {scenario.value} with {board['note_count']} notes\")\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(boards)} training boards\")\n",
    "print(f\"âœ“ Total notes: {sum(b['note_count'] for b in boards)}\")\n",
    "print(f\"âœ“ Saved in synthetic_data/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39114dcd-e72e-4228-9939-2f2b848f3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65c507-cea7-4f34-bd81-339688966037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Loading AI model for pattern recognition...\")\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Test it on your synthetic data\n",
    "all_notes = []\n",
    "for board in boards:\n",
    "    for note in board['notes']:\n",
    "        all_notes.append(note['content'])\n",
    "\n",
    "print(f\"\\nEncoding {len(all_notes)} notes...\")\n",
    "embeddings = encoder.encode(all_notes[:50])  # Just encode first 50 for speed\n",
    "\n",
    "print(f\"âœ“ Successfully created embeddings!\")\n",
    "print(f\"âœ“ Each note is now a {len(embeddings[0])}-dimensional vector\")\n",
    "print(f\"âœ“ Ready to train PRISM pattern recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344f98b-842e-4844-b7f1-978f896b8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "class PRISMCore:\n",
    "    def __init__(self):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.patterns = {}\n",
    "        \n",
    "    def analyze_board(self, board):\n",
    "        \"\"\"Analyze a single board for patterns\"\"\"\n",
    "        results = {\n",
    "            'pain_points': [],\n",
    "            'opportunities': [],\n",
    "            'themes': [],\n",
    "            'severity_score': 0\n",
    "        }\n",
    "        \n",
    "        # Find critical pain points\n",
    "        for note in board['notes']:\n",
    "            if note['type'] == 'pain_point' and note['severity'] > 0.7:\n",
    "                results['pain_points'].append({\n",
    "                    'content': note['content'],\n",
    "                    'severity': note['severity']\n",
    "                })\n",
    "            elif note['type'] == 'opportunity':\n",
    "                results['opportunities'].append(note['content'])\n",
    "        \n",
    "        # Calculate overall severity\n",
    "        severities = [n['severity'] for n in board['notes'] if n['type'] == 'pain_point']\n",
    "        results['severity_score'] = np.mean(severities) if severities else 0\n",
    "        \n",
    "        # Sort pain points by severity\n",
    "        results['pain_points'].sort(key=lambda x: x['severity'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def find_patterns_across_boards(self, boards):\n",
    "        \"\"\"Find patterns across multiple boards\"\"\"\n",
    "        all_pain_points = []\n",
    "        all_opportunities = []\n",
    "        \n",
    "        for board in boards:\n",
    "            analysis = self.analyze_board(board)\n",
    "            all_pain_points.extend([p['content'] for p in analysis['pain_points']])\n",
    "            all_opportunities.extend(analysis['opportunities'])\n",
    "        \n",
    "        print(f\"Found {len(all_pain_points)} total pain points\")\n",
    "        print(f\"Found {len(all_opportunities)} total opportunities\")\n",
    "        \n",
    "        # Encode and cluster pain points to find common themes\n",
    "        if all_pain_points:\n",
    "            embeddings = self.encoder.encode(all_pain_points[:20])  # Limit for speed\n",
    "            clustering = DBSCAN(eps=0.5, min_samples=2)\n",
    "            clusters = clustering.fit_predict(embeddings)\n",
    "            \n",
    "            n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "            print(f\"Identified {n_clusters} pain point patterns\")\n",
    "        \n",
    "        return {\n",
    "            'total_pain_points': len(all_pain_points),\n",
    "            'total_opportunities': len(all_opportunities),\n",
    "            'common_patterns': n_clusters if all_pain_points else 0\n",
    "        }\n",
    "\n",
    "# Create and test PRISM\n",
    "prism = PRISMCore()\n",
    "\n",
    "# Analyze your first board\n",
    "first_board_analysis = prism.analyze_board(boards[0])\n",
    "print(\"\\nðŸ“Š Analysis of first board:\")\n",
    "print(f\"  Critical pain points: {len(first_board_analysis['pain_points'])}\")\n",
    "print(f\"  Opportunities found: {len(first_board_analysis['opportunities'])}\")\n",
    "print(f\"  Severity score: {first_board_analysis['severity_score']:.2f}\")\n",
    "\n",
    "# Find patterns across all boards\n",
    "print(\"\\nðŸ” Finding patterns across all boards...\")\n",
    "patterns = prism.find_patterns_across_boards(boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18416240-797f-42d4-acdd-cef3c1af6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(board_analysis):\n",
    "    \"\"\"Convert analysis into actionable insights\"\"\"\n",
    "    insights = []\n",
    "    \n",
    "    # Generate insights based on pain points\n",
    "    if board_analysis['pain_points']:\n",
    "        top_pain = board_analysis['pain_points'][0]\n",
    "        insights.append(f\"CRITICAL: Address '{top_pain['content']}' (severity: {top_pain['severity']:.2f})\")\n",
    "    \n",
    "    # Generate insights based on opportunities\n",
    "    if board_analysis['opportunities']:\n",
    "        insights.append(f\"OPPORTUNITY: Consider implementing {board_analysis['opportunities'][0]}\")\n",
    "    \n",
    "    # Generate severity-based recommendations\n",
    "    if board_analysis['severity_score'] > 0.7:\n",
    "        insights.append(\"âš ï¸ High severity issues detected - immediate action recommended\")\n",
    "    elif board_analysis['severity_score'] > 0.4:\n",
    "        insights.append(\"ðŸ“Š Moderate issues present - plan improvements for next sprint\")\n",
    "    else:\n",
    "        insights.append(\"âœ… Low severity - focus on enhancements\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate insights for your boards\n",
    "for i, board in enumerate(boards[:3]):  # Just first 3 boards\n",
    "    analysis = prism.analyze_board(board)\n",
    "    insights = generate_insights(analysis)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Board {i+1} Insights:\")\n",
    "    for insight in insights:\n",
    "        print(f\"  â€¢ {insight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdd586-ac17-4263-b2a2-ff18236559e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def analyze_new_board(num_notes, scenario_type, clustering):\n",
    "    \"\"\"Generate and analyze a new board\"\"\"\n",
    "    \n",
    "    # Map scenario type\n",
    "    scenario_map = {\n",
    "        \"Usability Test\": ResearchScenario.USABILITY_TEST,\n",
    "        \"User Interview\": ResearchScenario.USER_INTERVIEW,\n",
    "        \"Journey Map\": ResearchScenario.JOURNEY_MAP\n",
    "    }\n",
    "    \n",
    "    # Generate new board\n",
    "    new_board = generator.generate_board(\n",
    "        scenario=scenario_map[scenario_type],\n",
    "        num_notes=int(num_notes),\n",
    "        clustering_strength=clustering\n",
    "    )\n",
    "    \n",
    "    # Analyze it\n",
    "    analysis = prism.analyze_board(new_board)\n",
    "    insights = generate_insights(analysis)\n",
    "    \n",
    "    # Format output\n",
    "    output = f\"\"\"\n",
    "    ### ðŸ“Š Board Analysis\n",
    "    - **Notes Generated:** {new_board['note_count']}\n",
    "    - **Scenario:** {scenario_type}\n",
    "    - **Severity Score:** {analysis['severity_score']:.2f}\n",
    "    - **Critical Pain Points:** {len(analysis['pain_points'])}\n",
    "    - **Opportunities:** {len(analysis['opportunities'])}\n",
    "    \n",
    "    ### ðŸ’¡ Key Insights\n",
    "    \"\"\"\n",
    "    for insight in insights:\n",
    "        output += f\"- {insight}\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create interface\n",
    "demo = gr.Interface(\n",
    "    fn=analyze_new_board,\n",
    "    inputs=[\n",
    "        gr.Slider(10, 100, 50, label=\"Number of Notes\"),\n",
    "        gr.Dropdown([\"Usability Test\", \"User Interview\", \"Journey Map\"], \n",
    "                   value=\"Usability Test\", label=\"Research Type\"),\n",
    "        gr.Slider(0.3, 1.0, 0.7, label=\"Clustering Strength\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"ðŸ”· PRISM Pattern Recognition Demo\",\n",
    "    description=\"Generate synthetic UX research data and analyze patterns\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8968be-6170-40d5-b73a-13a29c508387",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e9f24-8d7b-4129-9b35-77eb71b2c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def analyze_new_board(num_notes, scenario_type, clustering):\n",
    "    \"\"\"Generate and analyze a new board\"\"\"\n",
    "    \n",
    "    # Map scenario type\n",
    "    scenario_map = {\n",
    "        \"Usability Test\": ResearchScenario.USABILITY_TEST,\n",
    "        \"User Interview\": ResearchScenario.USER_INTERVIEW,\n",
    "        \"Journey Map\": ResearchScenario.JOURNEY_MAP\n",
    "    }\n",
    "    \n",
    "    # Generate new board\n",
    "    generator = SyntheticFigJamGenerator(seed=None)  # No seed for variety\n",
    "    new_board = generator.generate_board(\n",
    "        scenario=scenario_map[scenario_type],\n",
    "        num_notes=int(num_notes),\n",
    "        clustering_strength=clustering\n",
    "    )\n",
    "    \n",
    "    # Analyze it\n",
    "    analysis = prism.analyze_board(new_board)\n",
    "    insights = generate_insights(analysis)\n",
    "    \n",
    "    # Format output\n",
    "    output = f\"\"\"\n",
    "### ðŸ“Š Board Analysis\n",
    "- **Notes Generated:** {new_board['note_count']}\n",
    "- **Scenario:** {scenario_type}\n",
    "- **Severity Score:** {analysis['severity_score']:.2f}\n",
    "- **Critical Pain Points:** {len(analysis['pain_points'])}\n",
    "- **Opportunities:** {len(analysis['opportunities'])}\n",
    "\n",
    "### ðŸ’¡ Key Insights\n",
    "\"\"\"\n",
    "    for insight in insights:\n",
    "        output += f\"- {insight}\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create interface\n",
    "demo = gr.Interface(\n",
    "    fn=analyze_new_board,\n",
    "    inputs=[\n",
    "        gr.Slider(10, 100, 50, label=\"Number of Notes\"),\n",
    "        gr.Dropdown([\"Usability Test\", \"User Interview\", \"Journey Map\"], \n",
    "                   value=\"Usability Test\", label=\"Research Type\"),\n",
    "        gr.Slider(0.3, 1.0, 0.7, label=\"Clustering Strength\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"ðŸ”· PRISM Pattern Recognition Demo\",\n",
    "    description=\"Generate synthetic UX research data and analyze patterns\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "print(\"Launching PRISM interface...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8d091-d347-427f-aff5-d05e93c0e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efcb8a-0c43-401b-adde-a039971c55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "\n",
    "class StickyNoteType(Enum):\n",
    "    \"\"\"Standardized sticky note categories\"\"\"\n",
    "    PAIN_POINT = \"pain_point\"\n",
    "    OPPORTUNITY = \"opportunity\"\n",
    "    OBSERVATION = \"observation\"\n",
    "    QUOTE = \"quote\"\n",
    "    IDEA = \"idea\"\n",
    "    QUESTION = \"question\"\n",
    "    TASK = \"task\"\n",
    "    DECISION = \"decision\"\n",
    "\n",
    "class ResearchScenario(Enum):\n",
    "    \"\"\"Different research contexts to generate\"\"\"\n",
    "    USABILITY_TEST = \"usability_testing\"\n",
    "    USER_INTERVIEW = \"user_interview\"\n",
    "    JOURNEY_MAP = \"journey_mapping\"\n",
    "    COMPETITIVE_ANALYSIS = \"competitive_analysis\"\n",
    "    AFFINITY_MAP = \"affinity_mapping\"\n",
    "    WORKSHOP = \"design_workshop\"\n",
    "    SURVEY_SYNTHESIS = \"survey_synthesis\"\n",
    "\n",
    "class FigJamColorScheme:\n",
    "    \"\"\"Standard FigJam color meanings\"\"\"\n",
    "    COLORS = {\n",
    "        \"red\": {\"hex\": \"#FF6B6B\", \"typical_use\": [\"pain_point\", \"critical\", \"blocker\"]},\n",
    "        \"yellow\": {\"hex\": \"#FFD93D\", \"typical_use\": [\"observation\", \"neutral\", \"note\"]},\n",
    "        \"green\": {\"hex\": \"#6BCF7F\", \"typical_use\": [\"opportunity\", \"positive\", \"success\"]},\n",
    "        \"blue\": {\"hex\": \"#4A90E2\", \"typical_use\": [\"question\", \"information\", \"process\"]},\n",
    "        \"purple\": {\"hex\": \"#9B59B6\", \"typical_use\": [\"idea\", \"innovation\", \"concept\"]},\n",
    "        \"orange\": {\"hex\": \"#FF8C42\", \"typical_use\": [\"warning\", \"attention\", \"important\"]},\n",
    "        \"pink\": {\"hex\": \"#FF69B4\", \"typical_use\": [\"quote\", \"user_voice\", \"feedback\"]},\n",
    "        \"gray\": {\"hex\": \"#95A5A6\", \"typical_use\": [\"context\", \"background\", \"archived\"]}\n",
    "    }\n",
    "\n",
    "class SyntheticFigJamGenerator:\n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        \"\"\"Initialize the generator with optional seed for reproducibility\"\"\"\n",
    "        if seed:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.faker = Faker()\n",
    "        if seed:\n",
    "            Faker.seed(seed)\n",
    "        \n",
    "        # Research-specific vocabulary\n",
    "        self.ux_vocabulary = {\n",
    "            \"pain_points\": [\n",
    "                \"confusing navigation\", \"slow loading times\", \"unclear messaging\",\n",
    "                \"missing features\", \"broken flow\", \"inconsistent design\",\n",
    "                \"poor mobile experience\", \"accessibility issues\", \"data loss\",\n",
    "                \"complex onboarding\", \"hidden functionality\", \"error handling\"\n",
    "            ],\n",
    "            \"opportunities\": [\n",
    "                \"streamline process\", \"add shortcuts\", \"improve clarity\",\n",
    "                \"personalization\", \"automation potential\", \"better feedback\",\n",
    "                \"progressive disclosure\", \"gamification\", \"social features\",\n",
    "                \"AI assistance\", \"predictive features\", \"contextual help\"\n",
    "            ],\n",
    "            \"user_emotions\": [\n",
    "                \"frustrated\", \"confused\", \"delighted\", \"anxious\", \"confident\",\n",
    "                \"overwhelmed\", \"satisfied\", \"disappointed\", \"curious\", \"engaged\"\n",
    "            ],\n",
    "            \"actions\": [\n",
    "                \"clicked\", \"scrolled\", \"searched\", \"filtered\", \"uploaded\",\n",
    "                \"shared\", \"saved\", \"deleted\", \"exported\", \"configured\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def generate_sticky_note(self, \n",
    "                            note_type: StickyNoteType,\n",
    "                            scenario: ResearchScenario,\n",
    "                            severity: float = 0.5) -> Dict:\n",
    "        \"\"\"Generate a single sticky note with realistic UX research content\"\"\"\n",
    "        \n",
    "        # Select appropriate color based on note type\n",
    "        color = self._select_color(note_type, severity)\n",
    "        \n",
    "        # Generate contextual content\n",
    "        content = self._generate_content(note_type, scenario, severity)\n",
    "        \n",
    "        # Add metadata\n",
    "        note = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"type\": note_type.value,\n",
    "            \"content\": content,\n",
    "            \"color\": color,\n",
    "            \"position\": {\"x\": 0, \"y\": 0},  # Will be set by layout algorithm\n",
    "            \"severity\": severity,\n",
    "            \"confidence\": random.uniform(0.6, 1.0),\n",
    "            \"word_count\": len(content.split()),\n",
    "            \"has_question\": \"?\" in content,\n",
    "            \"has_quote\": '\"' in content,\n",
    "            \"timestamp\": self.faker.date_time_between(\"-30d\", \"now\").isoformat(),\n",
    "            \"tags\": self._extract_tags(content)\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _select_color(self, note_type: StickyNoteType, severity: float) -> str:\n",
    "        \"\"\"Select appropriate color based on type and severity\"\"\"\n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            return \"red\" if severity > 0.7 else \"orange\"\n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            return \"green\"\n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            return \"pink\"\n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return \"blue\"\n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return \"purple\"\n",
    "        else:\n",
    "            return \"yellow\"\n",
    "    \n",
    "    def _generate_content(self, \n",
    "                         note_type: StickyNoteType, \n",
    "                         scenario: ResearchScenario,\n",
    "                         severity: float) -> str:\n",
    "        \"\"\"Generate realistic content based on type and scenario\"\"\"\n",
    "        \n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            pain = random.choice(self.ux_vocabulary[\"pain_points\"])\n",
    "            emotion = random.choice(self.ux_vocabulary[\"user_emotions\"][:5])  # Negative emotions\n",
    "            if severity > 0.7:\n",
    "                return f\"CRITICAL: Users {emotion} - {pain}\"\n",
    "            return f\"User reported: {pain}\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            opp = random.choice(self.ux_vocabulary[\"opportunities\"])\n",
    "            return f\"Opportunity: {opp} - could improve user satisfaction\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            action = random.choice(self.ux_vocabulary[\"actions\"])\n",
    "            return f'\"I {action} but couldn\\'t find what I needed\" - P{random.randint(1,12)}'\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return f\"Why do users {random.choice(self.ux_vocabulary['actions'])} here?\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return f\"Idea: {random.choice(self.ux_vocabulary['opportunities'])}\"\n",
    "            \n",
    "        else:\n",
    "            return f\"Observed: User {random.choice(self.ux_vocabulary['actions'])} {random.randint(2,8)} times\"\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract relevant tags from content\"\"\"\n",
    "        tags = []\n",
    "        \n",
    "        # Check for keywords\n",
    "        for keyword in [\"critical\", \"blocker\", \"opportunity\", \"idea\"]:\n",
    "            if keyword.lower() in content.lower():\n",
    "                tags.append(keyword)\n",
    "        \n",
    "        # Check for user references\n",
    "        if \"P\" in content and any(char.isdigit() for char in content):\n",
    "            tags.append(\"user_quote\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def generate_board(self,\n",
    "                      scenario: ResearchScenario,\n",
    "                      num_notes: int = 50,\n",
    "                      clustering_strength: float = 0.7) -> Dict:\n",
    "        \"\"\"Generate a complete FigJam-like board with clustered notes\"\"\"\n",
    "        \n",
    "        # Determine note distribution based on scenario\n",
    "        distribution = self._get_note_distribution(scenario)\n",
    "        \n",
    "        # Generate notes\n",
    "        notes = []\n",
    "        for note_type, count in distribution.items():\n",
    "            actual_count = int(num_notes * count)\n",
    "            for _ in range(actual_count):\n",
    "                severity = np.random.beta(2, 5)  # Skewed towards lower severity\n",
    "                note = self.generate_sticky_note(note_type, scenario, severity)\n",
    "                notes.append(note)\n",
    "        \n",
    "        # Create spatial clusters\n",
    "        clusters = self._create_clusters(notes, clustering_strength)\n",
    "        \n",
    "        # Apply layout algorithm\n",
    "        self._apply_layout(notes, clusters)\n",
    "        \n",
    "        # Generate connections\n",
    "        connections = self._generate_connections(notes, clusters)\n",
    "        \n",
    "        # Create board metadata\n",
    "        board = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"scenario\": scenario.value,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"note_count\": len(notes),\n",
    "            \"notes\": notes,\n",
    "            \"clusters\": clusters,\n",
    "            \"connections\": connections,\n",
    "            \"metadata\": {\n",
    "                \"clustering_strength\": clustering_strength,\n",
    "                \"dominant_sentiment\": self._calculate_sentiment(notes),\n",
    "                \"key_themes\": self._extract_themes(notes),\n",
    "                \"severity_distribution\": self._severity_distribution(notes)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _get_note_distribution(self, scenario: ResearchScenario) -> Dict[StickyNoteType, float]:\n",
    "        \"\"\"Define note type distribution based on research scenario\"\"\"\n",
    "        distributions = {\n",
    "            ResearchScenario.USABILITY_TEST: {\n",
    "                StickyNoteType.PAIN_POINT: 0.35,\n",
    "                StickyNoteType.OBSERVATION: 0.25,\n",
    "                StickyNoteType.QUOTE: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.15,\n",
    "                StickyNoteType.QUESTION: 0.05\n",
    "            },\n",
    "            ResearchScenario.USER_INTERVIEW: {\n",
    "                StickyNoteType.QUOTE: 0.40,\n",
    "                StickyNoteType.OBSERVATION: 0.20,\n",
    "                StickyNoteType.PAIN_POINT: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.10,\n",
    "                StickyNoteType.IDEA: 0.10\n",
    "            },\n",
    "            ResearchScenario.JOURNEY_MAP: {\n",
    "                StickyNoteType.OBSERVATION: 0.30,\n",
    "                StickyNoteType.PAIN_POINT: 0.25,\n",
    "                StickyNoteType.OPPORTUNITY: 0.20,\n",
    "                StickyNoteType.QUOTE: 0.15,\n",
    "                StickyNoteType.TASK: 0.10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return distributions.get(scenario, {\n",
    "            StickyNoteType.OBSERVATION: 0.30,\n",
    "            StickyNoteType.PAIN_POINT: 0.25,\n",
    "            StickyNoteType.OPPORTUNITY: 0.20,\n",
    "            StickyNoteType.QUOTE: 0.15,\n",
    "            StickyNoteType.IDEA: 0.10\n",
    "        })\n",
    "    \n",
    "    def _create_clusters(self, notes: List[Dict], strength: float) -> List[Dict]:\n",
    "        \"\"\"Group related notes into clusters\"\"\"\n",
    "        # Simplified clustering - group by note type and severity\n",
    "        clusters = []\n",
    "        \n",
    "        # Group notes by type\n",
    "        type_groups = {}\n",
    "        for note in notes:\n",
    "            note_type = note[\"type\"]\n",
    "            if note_type not in type_groups:\n",
    "                type_groups[note_type] = []\n",
    "            type_groups[note_type].append(note[\"id\"])\n",
    "        \n",
    "        # Create cluster definitions\n",
    "        for cluster_type, note_ids in type_groups.items():\n",
    "            clusters.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": cluster_type,\n",
    "                \"note_ids\": note_ids,\n",
    "                \"center\": {\"x\": random.randint(100, 900), \"y\": random.randint(100, 700)},\n",
    "                \"radius\": 150 + len(note_ids) * 10\n",
    "            })\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _apply_layout(self, notes: List[Dict], clusters: List[Dict]):\n",
    "        \"\"\"Apply spatial layout to notes based on clusters\"\"\"\n",
    "        for cluster in clusters:\n",
    "            center_x = cluster[\"center\"][\"x\"]\n",
    "            center_y = cluster[\"center\"][\"y\"]\n",
    "            radius = cluster[\"radius\"]\n",
    "            \n",
    "            cluster_notes = [n for n in notes if n[\"id\"] in cluster[\"note_ids\"]]\n",
    "            \n",
    "            # Arrange notes in cluster\n",
    "            for i, note in enumerate(cluster_notes):\n",
    "                angle = (2 * np.pi * i) / len(cluster_notes)\n",
    "                r = radius * (0.5 + 0.5 * random.random())  # Vary radius\n",
    "                \n",
    "                note[\"position\"][\"x\"] = int(center_x + r * np.cos(angle))\n",
    "                note[\"position\"][\"y\"] = int(center_y + r * np.sin(angle))\n",
    "    \n",
    "    def _generate_connections(self, notes: List[Dict], clusters: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate meaningful connections between related notes\"\"\"\n",
    "        connections = []\n",
    "        \n",
    "        # Connect high-severity pain points to opportunities\n",
    "        pain_points = [n for n in notes if n[\"type\"] == \"pain_point\" and n[\"severity\"] > 0.6]\n",
    "        opportunities = [n for n in notes if n[\"type\"] == \"opportunity\"]\n",
    "        \n",
    "        for pain in pain_points[:5]:  # Limit connections\n",
    "            if opportunities:\n",
    "                opp = random.choice(opportunities)\n",
    "                connections.append({\n",
    "                    \"from\": pain[\"id\"],\n",
    "                    \"to\": opp[\"id\"],\n",
    "                    \"type\": \"addresses\",\n",
    "                    \"strength\": pain[\"severity\"]\n",
    "                })\n",
    "        \n",
    "        return connections\n",
    "    \n",
    "    def _calculate_sentiment(self, notes: List[Dict]) -> str:\n",
    "        \"\"\"Calculate overall board sentiment\"\"\"\n",
    "        pain_count = sum(1 for n in notes if n[\"type\"] == \"pain_point\")\n",
    "        opp_count = sum(1 for n in notes if n[\"type\"] == \"opportunity\")\n",
    "        \n",
    "        if pain_count > opp_count * 1.5:\n",
    "            return \"critical\"\n",
    "        elif opp_count > pain_count:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    \n",
    "    def _extract_themes(self, notes: List[Dict]) -> List[str]:\n",
    "        \"\"\"Extract key themes from all notes\"\"\"\n",
    "        themes = set()\n",
    "        for note in notes:\n",
    "            themes.update(note.get(\"tags\", []))\n",
    "        return list(themes)[:5]  # Top 5 themes\n",
    "    \n",
    "    def _severity_distribution(self, notes: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate severity distribution\"\"\"\n",
    "        dist = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "        for note in notes:\n",
    "            sev = note.get(\"severity\", 0.5)\n",
    "            if sev < 0.33:\n",
    "                dist[\"low\"] += 1\n",
    "            elif sev < 0.67:\n",
    "                dist[\"medium\"] += 1\n",
    "            else:\n",
    "                dist[\"high\"] += 1\n",
    "        return dist\n",
    "    \n",
    "    def export_board(self, board: Dict, format: str = \"json\") -> str:\n",
    "        \"\"\"Export board to specified format\"\"\"\n",
    "        if format == \"json\":\n",
    "            return json.dumps(board, indent=2)\n",
    "        elif format == \"csv\":\n",
    "            # Simplified CSV export of notes only\n",
    "            import csv\n",
    "            import io\n",
    "            output = io.StringIO()\n",
    "            writer = csv.DictWriter(output, \n",
    "                fieldnames=[\"id\", \"type\", \"content\", \"color\", \"severity\"])\n",
    "            writer.writeheader()\n",
    "            for note in board[\"notes\"]:\n",
    "                writer.writerow({\n",
    "                    \"id\": note[\"id\"],\n",
    "                    \"type\": note[\"type\"],\n",
    "                    \"content\": note[\"content\"],\n",
    "                    \"color\": note[\"color\"],\n",
    "                    \"severity\": note[\"severity\"]\n",
    "                })\n",
    "            return output.getvalue()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create generator instance\n",
    "    generator = SyntheticFigJamGenerator(seed=42)\n",
    "    \n",
    "    # Generate a board for usability testing scenario\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=75,\n",
    "        clustering_strength=0.8\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Generated board with {board['note_count']} notes\")\n",
    "    print(f\"Scenario: {board['scenario']}\")\n",
    "    print(f\"Sentiment: {board['metadata']['dominant_sentiment']}\")\n",
    "    print(f\"Themes: {board['metadata']['key_themes']}\")\n",
    "    print(f\"Severity distribution: {board['metadata']['severity_distribution']}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_output = generator.export_board(board, format=\"json\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"synthetic_figjam_board.json\", \"w\") as f:\n",
    "        f.write(json_output)\n",
    "    \n",
    "    print(\"\\nBoard exported to synthetic_figjam_board.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176c1ce-a599-4a84-a951-f65ec2ca07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "class PRISMCore:\n",
    "    def __init__(self):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def analyze_board(self, board):\n",
    "        results = {\n",
    "            'pain_points': [],\n",
    "            'opportunities': [],\n",
    "            'severity_score': 0\n",
    "        }\n",
    "        \n",
    "        for note in board['notes']:\n",
    "            if note['type'] == 'pain_point' and note['severity'] > 0.7:\n",
    "                results['pain_points'].append({\n",
    "                    'content': note['content'],\n",
    "                    'severity': note['severity']\n",
    "                })\n",
    "            elif note['type'] == 'opportunity':\n",
    "                results['opportunities'].append(note['content'])\n",
    "        \n",
    "        severities = [n['severity'] for n in board['notes'] if n['type'] == 'pain_point']\n",
    "        results['severity_score'] = np.mean(severities) if severities else 0\n",
    "        results['pain_points'].sort(key=lambda x: x['severity'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "\n",
    "prism = PRISMCore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ea4527-4bb7-4f00-892f-221972737256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0ffe6-93f8-436e-b895-1e50e1dada43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01078e7-397f-4409-960b-b544adf13e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2caf0e5-3812-41c2-a006-dc8bc9b1cc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292c8c4-d862-475a-ac12-b1f354f9be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some boards for testing\n",
    "generator = SyntheticFigJamGenerator(seed=42)\n",
    "boards = []\n",
    "for i in range(5):\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=50,\n",
    "        clustering_strength=0.7\n",
    "    )\n",
    "    boards.append(board)\n",
    "print(f\"Generated {len(boards)} boards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf9f60c-af33-4313-a553-9c22140c854e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SyntheticFigJamGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick test to make sure generator is loaded\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m generator = \u001b[43mSyntheticFigJamGenerator\u001b[49m(seed=\u001b[32m42\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Generator loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SyntheticFigJamGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "# Quick test to make sure generator is loaded\n",
    "generator = SyntheticFigJamGenerator(seed=42)\n",
    "print(\"âœ“ Generator loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28707606-4d76-4d3b-80ca-9a93f169106f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (637858192.py, line 394)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 394\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m```\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "\n",
    "class StickyNoteType(Enum):\n",
    "    \"\"\"Standardized sticky note categories\"\"\"\n",
    "    PAIN_POINT = \"pain_point\"\n",
    "    OPPORTUNITY = \"opportunity\"\n",
    "    OBSERVATION = \"observation\"\n",
    "    QUOTE = \"quote\"\n",
    "    IDEA = \"idea\"\n",
    "    QUESTION = \"question\"\n",
    "    TASK = \"task\"\n",
    "    DECISION = \"decision\"\n",
    "\n",
    "class ResearchScenario(Enum):\n",
    "    \"\"\"Different research contexts to generate\"\"\"\n",
    "    USABILITY_TEST = \"usability_testing\"\n",
    "    USER_INTERVIEW = \"user_interview\"\n",
    "    JOURNEY_MAP = \"journey_mapping\"\n",
    "    COMPETITIVE_ANALYSIS = \"competitive_analysis\"\n",
    "    AFFINITY_MAP = \"affinity_mapping\"\n",
    "    WORKSHOP = \"design_workshop\"\n",
    "    SURVEY_SYNTHESIS = \"survey_synthesis\"\n",
    "\n",
    "class FigJamColorScheme:\n",
    "    \"\"\"Standard FigJam color meanings\"\"\"\n",
    "    COLORS = {\n",
    "        \"red\": {\"hex\": \"#FF6B6B\", \"typical_use\": [\"pain_point\", \"critical\", \"blocker\"]},\n",
    "        \"yellow\": {\"hex\": \"#FFD93D\", \"typical_use\": [\"observation\", \"neutral\", \"note\"]},\n",
    "        \"green\": {\"hex\": \"#6BCF7F\", \"typical_use\": [\"opportunity\", \"positive\", \"success\"]},\n",
    "        \"blue\": {\"hex\": \"#4A90E2\", \"typical_use\": [\"question\", \"information\", \"process\"]},\n",
    "        \"purple\": {\"hex\": \"#9B59B6\", \"typical_use\": [\"idea\", \"innovation\", \"concept\"]},\n",
    "        \"orange\": {\"hex\": \"#FF8C42\", \"typical_use\": [\"warning\", \"attention\", \"important\"]},\n",
    "        \"pink\": {\"hex\": \"#FF69B4\", \"typical_use\": [\"quote\", \"user_voice\", \"feedback\"]},\n",
    "        \"gray\": {\"hex\": \"#95A5A6\", \"typical_use\": [\"context\", \"background\", \"archived\"]}\n",
    "    }\n",
    "\n",
    "class SyntheticFigJamGenerator:\n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        \"\"\"Initialize the generator with optional seed for reproducibility\"\"\"\n",
    "        if seed:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.faker = Faker()\n",
    "        if seed:\n",
    "            Faker.seed(seed)\n",
    "        \n",
    "        # Research-specific vocabulary\n",
    "        self.ux_vocabulary = {\n",
    "            \"pain_points\": [\n",
    "                \"confusing navigation\", \"slow loading times\", \"unclear messaging\",\n",
    "                \"missing features\", \"broken flow\", \"inconsistent design\",\n",
    "                \"poor mobile experience\", \"accessibility issues\", \"data loss\",\n",
    "                \"complex onboarding\", \"hidden functionality\", \"error handling\"\n",
    "            ],\n",
    "            \"opportunities\": [\n",
    "                \"streamline process\", \"add shortcuts\", \"improve clarity\",\n",
    "                \"personalization\", \"automation potential\", \"better feedback\",\n",
    "                \"progressive disclosure\", \"gamification\", \"social features\",\n",
    "                \"AI assistance\", \"predictive features\", \"contextual help\"\n",
    "            ],\n",
    "            \"user_emotions\": [\n",
    "                \"frustrated\", \"confused\", \"delighted\", \"anxious\", \"confident\",\n",
    "                \"overwhelmed\", \"satisfied\", \"disappointed\", \"curious\", \"engaged\"\n",
    "            ],\n",
    "            \"actions\": [\n",
    "                \"clicked\", \"scrolled\", \"searched\", \"filtered\", \"uploaded\",\n",
    "                \"shared\", \"saved\", \"deleted\", \"exported\", \"configured\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def generate_sticky_note(self, \n",
    "                            note_type: StickyNoteType,\n",
    "                            scenario: ResearchScenario,\n",
    "                            severity: float = 0.5) -> Dict:\n",
    "        \"\"\"Generate a single sticky note with realistic UX research content\"\"\"\n",
    "        \n",
    "        # Select appropriate color based on note type\n",
    "        color = self._select_color(note_type, severity)\n",
    "        \n",
    "        # Generate contextual content\n",
    "        content = self._generate_content(note_type, scenario, severity)\n",
    "        \n",
    "        # Add metadata\n",
    "        note = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"type\": note_type.value,\n",
    "            \"content\": content,\n",
    "            \"color\": color,\n",
    "            \"position\": {\"x\": 0, \"y\": 0},  # Will be set by layout algorithm\n",
    "            \"severity\": severity,\n",
    "            \"confidence\": random.uniform(0.6, 1.0),\n",
    "            \"word_count\": len(content.split()),\n",
    "            \"has_question\": \"?\" in content,\n",
    "            \"has_quote\": '\"' in content,\n",
    "            \"timestamp\": self.faker.date_time_between(\"-30d\", \"now\").isoformat(),\n",
    "            \"tags\": self._extract_tags(content)\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _select_color(self, note_type: StickyNoteType, severity: float) -> str:\n",
    "        \"\"\"Select appropriate color based on type and severity\"\"\"\n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            return \"red\" if severity > 0.7 else \"orange\"\n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            return \"green\"\n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            return \"pink\"\n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return \"blue\"\n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return \"purple\"\n",
    "        else:\n",
    "            return \"yellow\"\n",
    "    \n",
    "    def _generate_content(self, \n",
    "                         note_type: StickyNoteType, \n",
    "                         scenario: ResearchScenario,\n",
    "                         severity: float) -> str:\n",
    "        \"\"\"Generate realistic content based on type and scenario\"\"\"\n",
    "        \n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            pain = random.choice(self.ux_vocabulary[\"pain_points\"])\n",
    "            emotion = random.choice(self.ux_vocabulary[\"user_emotions\"][:5])  # Negative emotions\n",
    "            if severity > 0.7:\n",
    "                return f\"CRITICAL: Users {emotion} - {pain}\"\n",
    "            return f\"User reported: {pain}\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            opp = random.choice(self.ux_vocabulary[\"opportunities\"])\n",
    "            return f\"Opportunity: {opp} - could improve user satisfaction\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            action = random.choice(self.ux_vocabulary[\"actions\"])\n",
    "            return f'\"I {action} but couldn\\'t find what I needed\" - P{random.randint(1,12)}'\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return f\"Why do users {random.choice(self.ux_vocabulary['actions'])} here?\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return f\"Idea: {random.choice(self.ux_vocabulary['opportunities'])}\"\n",
    "            \n",
    "        else:\n",
    "            return f\"Observed: User {random.choice(self.ux_vocabulary['actions'])} {random.randint(2,8)} times\"\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract relevant tags from content\"\"\"\n",
    "        tags = []\n",
    "        \n",
    "        # Check for keywords\n",
    "        for keyword in [\"critical\", \"blocker\", \"opportunity\", \"idea\"]:\n",
    "            if keyword.lower() in content.lower():\n",
    "                tags.append(keyword)\n",
    "        \n",
    "        # Check for user references\n",
    "        if \"P\" in content and any(char.isdigit() for char in content):\n",
    "            tags.append(\"user_quote\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def generate_board(self,\n",
    "                      scenario: ResearchScenario,\n",
    "                      num_notes: int = 50,\n",
    "                      clustering_strength: float = 0.7) -> Dict:\n",
    "        \"\"\"Generate a complete FigJam-like board with clustered notes\"\"\"\n",
    "        \n",
    "        # Determine note distribution based on scenario\n",
    "        distribution = self._get_note_distribution(scenario)\n",
    "        \n",
    "        # Generate notes\n",
    "        notes = []\n",
    "        for note_type, count in distribution.items():\n",
    "            actual_count = int(num_notes * count)\n",
    "            for _ in range(actual_count):\n",
    "                severity = np.random.beta(2, 5)  # Skewed towards lower severity\n",
    "                note = self.generate_sticky_note(note_type, scenario, severity)\n",
    "                notes.append(note)\n",
    "        \n",
    "        # Create spatial clusters\n",
    "        clusters = self._create_clusters(notes, clustering_strength)\n",
    "        \n",
    "        # Apply layout algorithm\n",
    "        self._apply_layout(notes, clusters)\n",
    "        \n",
    "        # Generate connections\n",
    "        connections = self._generate_connections(notes, clusters)\n",
    "        \n",
    "        # Create board metadata\n",
    "        board = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"scenario\": scenario.value,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"note_count\": len(notes),\n",
    "            \"notes\": notes,\n",
    "            \"clusters\": clusters,\n",
    "            \"connections\": connections,\n",
    "            \"metadata\": {\n",
    "                \"clustering_strength\": clustering_strength,\n",
    "                \"dominant_sentiment\": self._calculate_sentiment(notes),\n",
    "                \"key_themes\": self._extract_themes(notes),\n",
    "                \"severity_distribution\": self._severity_distribution(notes)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _get_note_distribution(self, scenario: ResearchScenario) -> Dict[StickyNoteType, float]:\n",
    "        \"\"\"Define note type distribution based on research scenario\"\"\"\n",
    "        distributions = {\n",
    "            ResearchScenario.USABILITY_TEST: {\n",
    "                StickyNoteType.PAIN_POINT: 0.35,\n",
    "                StickyNoteType.OBSERVATION: 0.25,\n",
    "                StickyNoteType.QUOTE: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.15,\n",
    "                StickyNoteType.QUESTION: 0.05\n",
    "            },\n",
    "            ResearchScenario.USER_INTERVIEW: {\n",
    "                StickyNoteType.QUOTE: 0.40,\n",
    "                StickyNoteType.OBSERVATION: 0.20,\n",
    "                StickyNoteType.PAIN_POINT: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.10,\n",
    "                StickyNoteType.IDEA: 0.10\n",
    "            },\n",
    "            ResearchScenario.JOURNEY_MAP: {\n",
    "                StickyNoteType.OBSERVATION: 0.30,\n",
    "                StickyNoteType.PAIN_POINT: 0.25,\n",
    "                StickyNoteType.OPPORTUNITY: 0.20,\n",
    "                StickyNoteType.QUOTE: 0.15,\n",
    "                StickyNoteType.TASK: 0.10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return distributions.get(scenario, {\n",
    "            StickyNoteType.OBSERVATION: 0.30,\n",
    "            StickyNoteType.PAIN_POINT: 0.25,\n",
    "            StickyNoteType.OPPORTUNITY: 0.20,\n",
    "            StickyNoteType.QUOTE: 0.15,\n",
    "            StickyNoteType.IDEA: 0.10\n",
    "        })\n",
    "    \n",
    "    def _create_clusters(self, notes: List[Dict], strength: float) -> List[Dict]:\n",
    "        \"\"\"Group related notes into clusters\"\"\"\n",
    "        # Simplified clustering - group by note type and severity\n",
    "        clusters = []\n",
    "        \n",
    "        # Group notes by type\n",
    "        type_groups = {}\n",
    "        for note in notes:\n",
    "            note_type = note[\"type\"]\n",
    "            if note_type not in type_groups:\n",
    "                type_groups[note_type] = []\n",
    "            type_groups[note_type].append(note[\"id\"])\n",
    "        \n",
    "        # Create cluster definitions\n",
    "        for cluster_type, note_ids in type_groups.items():\n",
    "            clusters.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": cluster_type,\n",
    "                \"note_ids\": note_ids,\n",
    "                \"center\": {\"x\": random.randint(100, 900), \"y\": random.randint(100, 700)},\n",
    "                \"radius\": 150 + len(note_ids) * 10\n",
    "            })\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _apply_layout(self, notes: List[Dict], clusters: List[Dict]):\n",
    "        \"\"\"Apply spatial layout to notes based on clusters\"\"\"\n",
    "        for cluster in clusters:\n",
    "            center_x = cluster[\"center\"][\"x\"]\n",
    "            center_y = cluster[\"center\"][\"y\"]\n",
    "            radius = cluster[\"radius\"]\n",
    "            \n",
    "            cluster_notes = [n for n in notes if n[\"id\"] in cluster[\"note_ids\"]]\n",
    "            \n",
    "            # Arrange notes in cluster\n",
    "            for i, note in enumerate(cluster_notes):\n",
    "                angle = (2 * np.pi * i) / len(cluster_notes)\n",
    "                r = radius * (0.5 + 0.5 * random.random())  # Vary radius\n",
    "                \n",
    "                note[\"position\"][\"x\"] = int(center_x + r * np.cos(angle))\n",
    "                note[\"position\"][\"y\"] = int(center_y + r * np.sin(angle))\n",
    "    \n",
    "    def _generate_connections(self, notes: List[Dict], clusters: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate meaningful connections between related notes\"\"\"\n",
    "        connections = []\n",
    "        \n",
    "        # Connect high-severity pain points to opportunities\n",
    "        pain_points = [n for n in notes if n[\"type\"] == \"pain_point\" and n[\"severity\"] > 0.6]\n",
    "        opportunities = [n for n in notes if n[\"type\"] == \"opportunity\"]\n",
    "        \n",
    "        for pain in pain_points[:5]:  # Limit connections\n",
    "            if opportunities:\n",
    "                opp = random.choice(opportunities)\n",
    "                connections.append({\n",
    "                    \"from\": pain[\"id\"],\n",
    "                    \"to\": opp[\"id\"],\n",
    "                    \"type\": \"addresses\",\n",
    "                    \"strength\": pain[\"severity\"]\n",
    "                })\n",
    "        \n",
    "        return connections\n",
    "    \n",
    "    def _calculate_sentiment(self, notes: List[Dict]) -> str:\n",
    "        \"\"\"Calculate overall board sentiment\"\"\"\n",
    "        pain_count = sum(1 for n in notes if n[\"type\"] == \"pain_point\")\n",
    "        opp_count = sum(1 for n in notes if n[\"type\"] == \"opportunity\")\n",
    "        \n",
    "        if pain_count > opp_count * 1.5:\n",
    "            return \"critical\"\n",
    "        elif opp_count > pain_count:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    \n",
    "    def _extract_themes(self, notes: List[Dict]) -> List[str]:\n",
    "        \"\"\"Extract key themes from all notes\"\"\"\n",
    "        themes = set()\n",
    "        for note in notes:\n",
    "            themes.update(note.get(\"tags\", []))\n",
    "        return list(themes)[:5]  # Top 5 themes\n",
    "    \n",
    "    def _severity_distribution(self, notes: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate severity distribution\"\"\"\n",
    "        dist = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "        for note in notes:\n",
    "            sev = note.get(\"severity\", 0.5)\n",
    "            if sev < 0.33:\n",
    "                dist[\"low\"] += 1\n",
    "            elif sev < 0.67:\n",
    "                dist[\"medium\"] += 1\n",
    "            else:\n",
    "                dist[\"high\"] += 1\n",
    "        return dist\n",
    "    \n",
    "    def export_board(self, board: Dict, format: str = \"json\") -> str:\n",
    "        \"\"\"Export board to specified format\"\"\"\n",
    "        if format == \"json\":\n",
    "            return json.dumps(board, indent=2)\n",
    "        elif format == \"csv\":\n",
    "            # Simplified CSV export of notes only\n",
    "            import csv\n",
    "            import io\n",
    "            output = io.StringIO()\n",
    "            writer = csv.DictWriter(output, \n",
    "                fieldnames=[\"id\", \"type\", \"content\", \"color\", \"severity\"])\n",
    "            writer.writeheader()\n",
    "            for note in board[\"notes\"]:\n",
    "                writer.writerow({\n",
    "                    \"id\": note[\"id\"],\n",
    "                    \"type\": note[\"type\"],\n",
    "                    \"content\": note[\"content\"],\n",
    "                    \"color\": note[\"color\"],\n",
    "                    \"severity\": note[\"severity\"]\n",
    "                })\n",
    "            return output.getvalue()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create generator instance\n",
    "    generator = SyntheticFigJamGenerator(seed=42)\n",
    "    \n",
    "    # Generate a board for usability testing scenario\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=75,\n",
    "        clustering_strength=0.8\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Generated board with {board['note_count']} notes\")\n",
    "    print(f\"Scenario: {board['scenario']}\")\n",
    "    print(f\"Sentiment: {board['metadata']['dominant_sentiment']}\")\n",
    "    print(f\"Themes: {board['metadata']['key_themes']}\")\n",
    "    print(f\"Severity distribution: {board['metadata']['severity_distribution']}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_output = generator.export_board(board, format=\"json\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"synthetic_figjam_board.json\", \"w\") as f:\n",
    "        f.write(json_output)\n",
    "    \n",
    "    print(\"\\nBoard exported to synthetic_figjam_board.json\")\n",
    "```\n",
    "\n",
    "You should see:\n",
    "```\n",
    "Generated board with 75 notes\n",
    "Scenario: usability_testing\n",
    "...\n",
    "Board exported to synthetic_figjam_board.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cb4a66-03e1-401b-983d-b4fff61b787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated board with 73 notes\n",
      "Scenario: usability_testing\n",
      "Sentiment: critical\n",
      "Themes: ['opportunity', 'user_quote']\n",
      "Severity distribution: {'low': 50, 'medium': 22, 'high': 1}\n",
      "\n",
      "Board exported to synthetic_figjam_board.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "\n",
    "class StickyNoteType(Enum):\n",
    "    \"\"\"Standardized sticky note categories\"\"\"\n",
    "    PAIN_POINT = \"pain_point\"\n",
    "    OPPORTUNITY = \"opportunity\"\n",
    "    OBSERVATION = \"observation\"\n",
    "    QUOTE = \"quote\"\n",
    "    IDEA = \"idea\"\n",
    "    QUESTION = \"question\"\n",
    "    TASK = \"task\"\n",
    "    DECISION = \"decision\"\n",
    "\n",
    "class ResearchScenario(Enum):\n",
    "    \"\"\"Different research contexts to generate\"\"\"\n",
    "    USABILITY_TEST = \"usability_testing\"\n",
    "    USER_INTERVIEW = \"user_interview\"\n",
    "    JOURNEY_MAP = \"journey_mapping\"\n",
    "    COMPETITIVE_ANALYSIS = \"competitive_analysis\"\n",
    "    AFFINITY_MAP = \"affinity_mapping\"\n",
    "    WORKSHOP = \"design_workshop\"\n",
    "    SURVEY_SYNTHESIS = \"survey_synthesis\"\n",
    "\n",
    "class FigJamColorScheme:\n",
    "    \"\"\"Standard FigJam color meanings\"\"\"\n",
    "    COLORS = {\n",
    "        \"red\": {\"hex\": \"#FF6B6B\", \"typical_use\": [\"pain_point\", \"critical\", \"blocker\"]},\n",
    "        \"yellow\": {\"hex\": \"#FFD93D\", \"typical_use\": [\"observation\", \"neutral\", \"note\"]},\n",
    "        \"green\": {\"hex\": \"#6BCF7F\", \"typical_use\": [\"opportunity\", \"positive\", \"success\"]},\n",
    "        \"blue\": {\"hex\": \"#4A90E2\", \"typical_use\": [\"question\", \"information\", \"process\"]},\n",
    "        \"purple\": {\"hex\": \"#9B59B6\", \"typical_use\": [\"idea\", \"innovation\", \"concept\"]},\n",
    "        \"orange\": {\"hex\": \"#FF8C42\", \"typical_use\": [\"warning\", \"attention\", \"important\"]},\n",
    "        \"pink\": {\"hex\": \"#FF69B4\", \"typical_use\": [\"quote\", \"user_voice\", \"feedback\"]},\n",
    "        \"gray\": {\"hex\": \"#95A5A6\", \"typical_use\": [\"context\", \"background\", \"archived\"]}\n",
    "    }\n",
    "\n",
    "class SyntheticFigJamGenerator:\n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        \"\"\"Initialize the generator with optional seed for reproducibility\"\"\"\n",
    "        if seed:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.faker = Faker()\n",
    "        if seed:\n",
    "            Faker.seed(seed)\n",
    "        \n",
    "        # Research-specific vocabulary\n",
    "        self.ux_vocabulary = {\n",
    "            \"pain_points\": [\n",
    "                \"confusing navigation\", \"slow loading times\", \"unclear messaging\",\n",
    "                \"missing features\", \"broken flow\", \"inconsistent design\",\n",
    "                \"poor mobile experience\", \"accessibility issues\", \"data loss\",\n",
    "                \"complex onboarding\", \"hidden functionality\", \"error handling\"\n",
    "            ],\n",
    "            \"opportunities\": [\n",
    "                \"streamline process\", \"add shortcuts\", \"improve clarity\",\n",
    "                \"personalization\", \"automation potential\", \"better feedback\",\n",
    "                \"progressive disclosure\", \"gamification\", \"social features\",\n",
    "                \"AI assistance\", \"predictive features\", \"contextual help\"\n",
    "            ],\n",
    "            \"user_emotions\": [\n",
    "                \"frustrated\", \"confused\", \"delighted\", \"anxious\", \"confident\",\n",
    "                \"overwhelmed\", \"satisfied\", \"disappointed\", \"curious\", \"engaged\"\n",
    "            ],\n",
    "            \"actions\": [\n",
    "                \"clicked\", \"scrolled\", \"searched\", \"filtered\", \"uploaded\",\n",
    "                \"shared\", \"saved\", \"deleted\", \"exported\", \"configured\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def generate_sticky_note(self, \n",
    "                            note_type: StickyNoteType,\n",
    "                            scenario: ResearchScenario,\n",
    "                            severity: float = 0.5) -> Dict:\n",
    "        \"\"\"Generate a single sticky note with realistic UX research content\"\"\"\n",
    "        \n",
    "        # Select appropriate color based on note type\n",
    "        color = self._select_color(note_type, severity)\n",
    "        \n",
    "        # Generate contextual content\n",
    "        content = self._generate_content(note_type, scenario, severity)\n",
    "        \n",
    "        # Add metadata\n",
    "        note = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"type\": note_type.value,\n",
    "            \"content\": content,\n",
    "            \"color\": color,\n",
    "            \"position\": {\"x\": 0, \"y\": 0},  # Will be set by layout algorithm\n",
    "            \"severity\": severity,\n",
    "            \"confidence\": random.uniform(0.6, 1.0),\n",
    "            \"word_count\": len(content.split()),\n",
    "            \"has_question\": \"?\" in content,\n",
    "            \"has_quote\": '\"' in content,\n",
    "            \"timestamp\": self.faker.date_time_between(\"-30d\", \"now\").isoformat(),\n",
    "            \"tags\": self._extract_tags(content)\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _select_color(self, note_type: StickyNoteType, severity: float) -> str:\n",
    "        \"\"\"Select appropriate color based on type and severity\"\"\"\n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            return \"red\" if severity > 0.7 else \"orange\"\n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            return \"green\"\n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            return \"pink\"\n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return \"blue\"\n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return \"purple\"\n",
    "        else:\n",
    "            return \"yellow\"\n",
    "    \n",
    "    def _generate_content(self, \n",
    "                         note_type: StickyNoteType, \n",
    "                         scenario: ResearchScenario,\n",
    "                         severity: float) -> str:\n",
    "        \"\"\"Generate realistic content based on type and scenario\"\"\"\n",
    "        \n",
    "        if note_type == StickyNoteType.PAIN_POINT:\n",
    "            pain = random.choice(self.ux_vocabulary[\"pain_points\"])\n",
    "            emotion = random.choice(self.ux_vocabulary[\"user_emotions\"][:5])  # Negative emotions\n",
    "            if severity > 0.7:\n",
    "                return f\"CRITICAL: Users {emotion} - {pain}\"\n",
    "            return f\"User reported: {pain}\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.OPPORTUNITY:\n",
    "            opp = random.choice(self.ux_vocabulary[\"opportunities\"])\n",
    "            return f\"Opportunity: {opp} - could improve user satisfaction\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUOTE:\n",
    "            action = random.choice(self.ux_vocabulary[\"actions\"])\n",
    "            return f'\"I {action} but couldn\\'t find what I needed\" - P{random.randint(1,12)}'\n",
    "            \n",
    "        elif note_type == StickyNoteType.QUESTION:\n",
    "            return f\"Why do users {random.choice(self.ux_vocabulary['actions'])} here?\"\n",
    "            \n",
    "        elif note_type == StickyNoteType.IDEA:\n",
    "            return f\"Idea: {random.choice(self.ux_vocabulary['opportunities'])}\"\n",
    "            \n",
    "        else:\n",
    "            return f\"Observed: User {random.choice(self.ux_vocabulary['actions'])} {random.randint(2,8)} times\"\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract relevant tags from content\"\"\"\n",
    "        tags = []\n",
    "        \n",
    "        # Check for keywords\n",
    "        for keyword in [\"critical\", \"blocker\", \"opportunity\", \"idea\"]:\n",
    "            if keyword.lower() in content.lower():\n",
    "                tags.append(keyword)\n",
    "        \n",
    "        # Check for user references\n",
    "        if \"P\" in content and any(char.isdigit() for char in content):\n",
    "            tags.append(\"user_quote\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def generate_board(self,\n",
    "                      scenario: ResearchScenario,\n",
    "                      num_notes: int = 50,\n",
    "                      clustering_strength: float = 0.7) -> Dict:\n",
    "        \"\"\"Generate a complete FigJam-like board with clustered notes\"\"\"\n",
    "        \n",
    "        # Determine note distribution based on scenario\n",
    "        distribution = self._get_note_distribution(scenario)\n",
    "        \n",
    "        # Generate notes\n",
    "        notes = []\n",
    "        for note_type, count in distribution.items():\n",
    "            actual_count = int(num_notes * count)\n",
    "            for _ in range(actual_count):\n",
    "                severity = np.random.beta(2, 5)  # Skewed towards lower severity\n",
    "                note = self.generate_sticky_note(note_type, scenario, severity)\n",
    "                notes.append(note)\n",
    "        \n",
    "        # Create spatial clusters\n",
    "        clusters = self._create_clusters(notes, clustering_strength)\n",
    "        \n",
    "        # Apply layout algorithm\n",
    "        self._apply_layout(notes, clusters)\n",
    "        \n",
    "        # Generate connections\n",
    "        connections = self._generate_connections(notes, clusters)\n",
    "        \n",
    "        # Create board metadata\n",
    "        board = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"scenario\": scenario.value,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"note_count\": len(notes),\n",
    "            \"notes\": notes,\n",
    "            \"clusters\": clusters,\n",
    "            \"connections\": connections,\n",
    "            \"metadata\": {\n",
    "                \"clustering_strength\": clustering_strength,\n",
    "                \"dominant_sentiment\": self._calculate_sentiment(notes),\n",
    "                \"key_themes\": self._extract_themes(notes),\n",
    "                \"severity_distribution\": self._severity_distribution(notes)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _get_note_distribution(self, scenario: ResearchScenario) -> Dict[StickyNoteType, float]:\n",
    "        \"\"\"Define note type distribution based on research scenario\"\"\"\n",
    "        distributions = {\n",
    "            ResearchScenario.USABILITY_TEST: {\n",
    "                StickyNoteType.PAIN_POINT: 0.35,\n",
    "                StickyNoteType.OBSERVATION: 0.25,\n",
    "                StickyNoteType.QUOTE: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.15,\n",
    "                StickyNoteType.QUESTION: 0.05\n",
    "            },\n",
    "            ResearchScenario.USER_INTERVIEW: {\n",
    "                StickyNoteType.QUOTE: 0.40,\n",
    "                StickyNoteType.OBSERVATION: 0.20,\n",
    "                StickyNoteType.PAIN_POINT: 0.20,\n",
    "                StickyNoteType.OPPORTUNITY: 0.10,\n",
    "                StickyNoteType.IDEA: 0.10\n",
    "            },\n",
    "            ResearchScenario.JOURNEY_MAP: {\n",
    "                StickyNoteType.OBSERVATION: 0.30,\n",
    "                StickyNoteType.PAIN_POINT: 0.25,\n",
    "                StickyNoteType.OPPORTUNITY: 0.20,\n",
    "                StickyNoteType.QUOTE: 0.15,\n",
    "                StickyNoteType.TASK: 0.10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return distributions.get(scenario, {\n",
    "            StickyNoteType.OBSERVATION: 0.30,\n",
    "            StickyNoteType.PAIN_POINT: 0.25,\n",
    "            StickyNoteType.OPPORTUNITY: 0.20,\n",
    "            StickyNoteType.QUOTE: 0.15,\n",
    "            StickyNoteType.IDEA: 0.10\n",
    "        })\n",
    "    \n",
    "    def _create_clusters(self, notes: List[Dict], strength: float) -> List[Dict]:\n",
    "        \"\"\"Group related notes into clusters\"\"\"\n",
    "        # Simplified clustering - group by note type and severity\n",
    "        clusters = []\n",
    "        \n",
    "        # Group notes by type\n",
    "        type_groups = {}\n",
    "        for note in notes:\n",
    "            note_type = note[\"type\"]\n",
    "            if note_type not in type_groups:\n",
    "                type_groups[note_type] = []\n",
    "            type_groups[note_type].append(note[\"id\"])\n",
    "        \n",
    "        # Create cluster definitions\n",
    "        for cluster_type, note_ids in type_groups.items():\n",
    "            clusters.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": cluster_type,\n",
    "                \"note_ids\": note_ids,\n",
    "                \"center\": {\"x\": random.randint(100, 900), \"y\": random.randint(100, 700)},\n",
    "                \"radius\": 150 + len(note_ids) * 10\n",
    "            })\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _apply_layout(self, notes: List[Dict], clusters: List[Dict]):\n",
    "        \"\"\"Apply spatial layout to notes based on clusters\"\"\"\n",
    "        for cluster in clusters:\n",
    "            center_x = cluster[\"center\"][\"x\"]\n",
    "            center_y = cluster[\"center\"][\"y\"]\n",
    "            radius = cluster[\"radius\"]\n",
    "            \n",
    "            cluster_notes = [n for n in notes if n[\"id\"] in cluster[\"note_ids\"]]\n",
    "            \n",
    "            # Arrange notes in cluster\n",
    "            for i, note in enumerate(cluster_notes):\n",
    "                angle = (2 * np.pi * i) / len(cluster_notes)\n",
    "                r = radius * (0.5 + 0.5 * random.random())  # Vary radius\n",
    "                \n",
    "                note[\"position\"][\"x\"] = int(center_x + r * np.cos(angle))\n",
    "                note[\"position\"][\"y\"] = int(center_y + r * np.sin(angle))\n",
    "    \n",
    "    def _generate_connections(self, notes: List[Dict], clusters: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate meaningful connections between related notes\"\"\"\n",
    "        connections = []\n",
    "        \n",
    "        # Connect high-severity pain points to opportunities\n",
    "        pain_points = [n for n in notes if n[\"type\"] == \"pain_point\" and n[\"severity\"] > 0.6]\n",
    "        opportunities = [n for n in notes if n[\"type\"] == \"opportunity\"]\n",
    "        \n",
    "        for pain in pain_points[:5]:  # Limit connections\n",
    "            if opportunities:\n",
    "                opp = random.choice(opportunities)\n",
    "                connections.append({\n",
    "                    \"from\": pain[\"id\"],\n",
    "                    \"to\": opp[\"id\"],\n",
    "                    \"type\": \"addresses\",\n",
    "                    \"strength\": pain[\"severity\"]\n",
    "                })\n",
    "        \n",
    "        return connections\n",
    "    \n",
    "    def _calculate_sentiment(self, notes: List[Dict]) -> str:\n",
    "        \"\"\"Calculate overall board sentiment\"\"\"\n",
    "        pain_count = sum(1 for n in notes if n[\"type\"] == \"pain_point\")\n",
    "        opp_count = sum(1 for n in notes if n[\"type\"] == \"opportunity\")\n",
    "        \n",
    "        if pain_count > opp_count * 1.5:\n",
    "            return \"critical\"\n",
    "        elif opp_count > pain_count:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    \n",
    "    def _extract_themes(self, notes: List[Dict]) -> List[str]:\n",
    "        \"\"\"Extract key themes from all notes\"\"\"\n",
    "        themes = set()\n",
    "        for note in notes:\n",
    "            themes.update(note.get(\"tags\", []))\n",
    "        return list(themes)[:5]  # Top 5 themes\n",
    "    \n",
    "    def _severity_distribution(self, notes: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate severity distribution\"\"\"\n",
    "        dist = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "        for note in notes:\n",
    "            sev = note.get(\"severity\", 0.5)\n",
    "            if sev < 0.33:\n",
    "                dist[\"low\"] += 1\n",
    "            elif sev < 0.67:\n",
    "                dist[\"medium\"] += 1\n",
    "            else:\n",
    "                dist[\"high\"] += 1\n",
    "        return dist\n",
    "    \n",
    "    def export_board(self, board: Dict, format: str = \"json\") -> str:\n",
    "        \"\"\"Export board to specified format\"\"\"\n",
    "        if format == \"json\":\n",
    "            return json.dumps(board, indent=2)\n",
    "        elif format == \"csv\":\n",
    "            # Simplified CSV export of notes only\n",
    "            import csv\n",
    "            import io\n",
    "            output = io.StringIO()\n",
    "            writer = csv.DictWriter(output, \n",
    "                fieldnames=[\"id\", \"type\", \"content\", \"color\", \"severity\"])\n",
    "            writer.writeheader()\n",
    "            for note in board[\"notes\"]:\n",
    "                writer.writerow({\n",
    "                    \"id\": note[\"id\"],\n",
    "                    \"type\": note[\"type\"],\n",
    "                    \"content\": note[\"content\"],\n",
    "                    \"color\": note[\"color\"],\n",
    "                    \"severity\": note[\"severity\"]\n",
    "                })\n",
    "            return output.getvalue()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create generator instance\n",
    "    generator = SyntheticFigJamGenerator(seed=42)\n",
    "    \n",
    "    # Generate a board for usability testing scenario\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=75,\n",
    "        clustering_strength=0.8\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Generated board with {board['note_count']} notes\")\n",
    "    print(f\"Scenario: {board['scenario']}\")\n",
    "    print(f\"Sentiment: {board['metadata']['dominant_sentiment']}\")\n",
    "    print(f\"Themes: {board['metadata']['key_themes']}\")\n",
    "    print(f\"Severity distribution: {board['metadata']['severity_distribution']}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_output = generator.export_board(board, format=\"json\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"synthetic_figjam_board.json\", \"w\") as f:\n",
    "        f.write(json_output)\n",
    "    \n",
    "    print(\"\\nBoard exported to synthetic_figjam_board.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7a3ac9-af60-4be4-8ebf-8939bbc23a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generator loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Quick test to make sure generator is loaded\n",
    "generator = SyntheticFigJamGenerator(seed=42)\n",
    "print(\"âœ“ Generator loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8abcad-8693-48df-9f91-b01bb8307c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 3 boards with 144 total notes\n"
     ]
    }
   ],
   "source": [
    "# Generate boards for testing\n",
    "boards = []\n",
    "for i in range(3):\n",
    "    board = generator.generate_board(\n",
    "        scenario=ResearchScenario.USABILITY_TEST,\n",
    "        num_notes=50,\n",
    "        clustering_strength=0.7\n",
    "    )\n",
    "    boards.append(board)\n",
    "print(f\"âœ“ Created {len(boards)} boards with {sum(b['note_count'] for b in boards)} total notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3fca19-c4ed-473b-ac61-fcea433afad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 10) (3129766246.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"ðŸ”´ Found {len(pain_points)} pain\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's test PRISM without Gradio for now\n",
    "test_board = boards[0]\n",
    "\n",
    "# Simple analysis\n",
    "pain_points = [n for n in test_board['notes'] if n['type'] == 'pain_point']\n",
    "opportunities = [n for n in test_board['notes'] if n['type'] == 'opportunity']\n",
    "\n",
    "print(\"ðŸŽ‰ SUCCESS! PRISM is working!\")\n",
    "print(f\"ðŸ“Š Analyzed board with {test_board['note_count']} notes\")\n",
    "print(f\"ðŸ”´ Found {len(pain_points)} pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3a8e78-0237-470c-b3b8-843256f23160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ SUCCESS! PRISM is working!\n",
      "ðŸ“Š Analyzed board with 48 notes\n",
      "ðŸ”´ Found 17 pain points\n",
      "ðŸ’š Found 7 opportunities\n"
     ]
    }
   ],
   "source": [
    "# Let's test PRISM without Gradio for now\n",
    "test_board = boards[0]\n",
    "\n",
    "# Simple analysis\n",
    "pain_points = [n for n in test_board['notes'] if n['type'] == 'pain_point']\n",
    "opportunities = [n for n in test_board['notes'] if n['type'] == 'opportunity']\n",
    "\n",
    "print(\"ðŸŽ‰ SUCCESS! PRISM is working!\")\n",
    "print(f\"ðŸ“Š Analyzed board with {test_board['note_count']} notes\")\n",
    "print(f\"ðŸ”´ Found {len(pain_points)} pain points\")\n",
    "print(f\"ðŸ’š Found {len(opportunities)} opportunities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcf33ff-845b-4ea6-9a4a-e5f6476979ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training data saved!\n"
     ]
    }
   ],
   "source": [
    "# Save your current progress\n",
    "import pickle\n",
    "\n",
    "# Save the boards you generated\n",
    "with open('training_boards.pkl', 'wb') as f:\n",
    "    pickle.dump(boards, f)\n",
    "\n",
    "print(\"âœ“ Training data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccea046f-3ac6-40f8-9244-7055f23540db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PRISM Architecture saved!\n"
     ]
    }
   ],
   "source": [
    "# Save your dual-AI architecture plan\n",
    "architecture = {\n",
    "    \"version\": \"2.0\",\n",
    "    \"name\": \"PRISM - Pattern Recognition & Insight Structure Module\",\n",
    "    \"core_ais\": {\n",
    "        \"AI_1_GENERATOR\": {\n",
    "            \"name\": \"Synthetic Data Engine\",\n",
    "            \"purpose\": \"Generate increasingly sophisticated training data\",\n",
    "            \"current_state\": \"âœ“ COMPLETE - Basic generation working\",\n",
    "            \"next_evolution\": [\n",
    "                \"Add context awareness (yellow = default vs yellow = concern)\",\n",
    "                \"Generate messy vs organized boards\",\n",
    "                \"Simulate real research progression over time\",\n",
    "                \"Add cultural/industry variations\"\n",
    "            ]\n",
    "        },\n",
    "        \"AI_2_LEARNER\": {\n",
    "            \"name\": \"Deep Pattern Recognition Engine\", \n",
    "            \"purpose\": \"Learn nuanced patterns from synthetic data\",\n",
    "            \"current_state\": \"âš¡ IN PROGRESS - Basic detection working\",\n",
    "            \"next_evolution\": [\n",
    "                \"Context-aware color interpretation\",\n",
    "                \"Spatial relationship learning\",\n",
    "                \"Temporal pattern recognition\",\n",
    "                \"Confidence scoring for ambiguous patterns\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"feedback_loop\": {\n",
    "        \"mechanism\": \"AI_2 failures â†’ AI_1 generates harder examples â†’ AI_2 learns\",\n",
    "        \"current_status\": \"Not implemented\",\n",
    "        \"implementation\": \"Version 3.0\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the architecture\n",
    "with open('prism_architecture.json', 'w') as f:\n",
    "    json.dump(architecture, f, indent=2)\n",
    "\n",
    "print(\"âœ“ PRISM Architecture saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a48c834-3234-40fa-9f29-1ac71af9e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSyntheticGenerator(SyntheticFigJamGenerator):\n",
    "    \"\"\"Version 2: Context-aware generation\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed)\n",
    "        \n",
    "        # Add context rules\n",
    "        self.color_contexts = {\n",
    "            \"yellow\": {\n",
    "                \"default\": 0.4,  # 40% of yellow is just default\n",
    "                \"concern\": 0.3,  # 30% actually means concern\n",
    "                \"neutral\": 0.3   # 30% is neutral observation\n",
    "            },\n",
    "            \"red\": {\n",
    "                \"pain_point\": 0.5,  # Only 50% of red is pain points\n",
    "                \"urgent\": 0.25,     # 25% means urgent/important\n",
    "                \"action\": 0.25      # 25% means action items\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def generate_contextual_note(self, color, position_context):\n",
    "        \"\"\"Generate notes where color doesn't always mean the obvious\"\"\"\n",
    "        \n",
    "        # Sometimes yellow is just the default sticky note\n",
    "        if color == \"yellow\" and random.random() < 0.4:\n",
    "            return {\n",
    "                \"color\": \"yellow\",\n",
    "                \"type\": random.choice([\"observation\", \"idea\", \"quote\"]),\n",
    "                \"content\": self._generate_neutral_content(),\n",
    "                \"is_default\": True  # Meta tag for training\n",
    "            }\n",
    "        \n",
    "        # Sometimes red is for \"IMPORTANT\" not \"PROBLEM\"\n",
    "        if color == \"red\" and random.random() < 0.25:\n",
    "            return {\n",
    "                \"color\": \"red\",\n",
    "                \"type\": \"action_item\",\n",
    "                \"content\": f\"ACTION: {random.choice(self.ux_vocabulary['actions'])}\",\n",
    "                \"is_pain_point\": False  # Meta tag for training\n",
    "            }\n",
    "            \n",
    "    def generate_realistic_board(self, scenario, messiness_level=0.5):\n",
    "        \"\"\"Generate boards with realistic human messiness\"\"\"\n",
    "        \n",
    "        board = self.generate_board(scenario)\n",
    "        \n",
    "        # Add realistic human patterns\n",
    "        if messiness_level > 0.7:\n",
    "            # Messy board: poor clustering, random colors\n",
    "            self._add_chaos(board)\n",
    "        elif messiness_level < 0.3:\n",
    "            # Hyper-organized: perfect clusters, consistent colors\n",
    "            self._add_organization(board)\n",
    "            \n",
    "        # Add meta-patterns for AI #2 to learn\n",
    "        board[\"meta_patterns\"] = {\n",
    "            \"color_consistency\": self._calculate_color_consistency(board),\n",
    "            \"spatial_organization\": 1 - messiness_level,\n",
    "            \"has_default_colors\": True\n",
    "        }\n",
    "        \n",
    "        return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b6a608c-29aa-43c2-b584-060dc97a905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PRISMDeepLearner:\n",
    "    \"\"\"Version 2: Context-aware pattern recognition\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.context_model = None\n",
    "        self.pattern_memory = {}\n",
    "        \n",
    "    def learn_color_context(self, boards):\n",
    "        \"\"\"Learn that colors don't always mean what they seem\"\"\"\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for board in boards:\n",
    "            for note in board['notes']:\n",
    "                features = {\n",
    "                    'color': note['color'],\n",
    "                    'position': note['position'],\n",
    "                    'nearby_colors': self._get_nearby_colors(note, board),\n",
    "                    'text_sentiment': self._analyze_text_sentiment(note['content']),\n",
    "                    'has_action_words': any(word in note['content'].lower() \n",
    "                                          for word in ['do', 'action', 'must', 'need'])\n",
    "                }\n",
    "                \n",
    "                # True label from meta tags\n",
    "                true_type = note.get('type', 'unknown')\n",
    "                \n",
    "                training_data.append((features, true_type))\n",
    "        \n",
    "        # Train context model\n",
    "        self.context_model = RandomForestClassifier()\n",
    "        # ... training logic\n",
    "        \n",
    "        print(f\"âœ“ Learned from {len(training_data)} examples\")\n",
    "        print(f\"âœ“ Now I know: Yellow isn't always concern!\")\n",
    "        print(f\"âœ“ Now I know: Red can mean urgent OR problem!\")\n",
    "        \n",
    "    def analyze_with_context(self, board):\n",
    "        \"\"\"Smarter analysis using learned context\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            'true_pain_points': [],  # Not just red notes\n",
    "            'hidden_opportunities': [],  # Found in any color\n",
    "            'action_items': [],  # Separated from pain points\n",
    "            'confidence_scores': {}\n",
    "        }\n",
    "        \n",
    "        for note in board['notes']:\n",
    "            # Use learned context, not just color\n",
    "            predicted_type = self._predict_with_context(note, board)\n",
    "            confidence = self._calculate_confidence(note, board)\n",
    "            \n",
    "            if predicted_type == 'pain_point' and confidence > 0.7:\n",
    "                results['true_pain_points'].append(note)\n",
    "            elif predicted_type == 'opportunity':\n",
    "                results['hidden_opportunities'].append(note)\n",
    "            elif predicted_type == 'action_item':\n",
    "                results['action_items'].append(note)\n",
    "                \n",
    "            results['confidence_scores'][note['id']] = confidence\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def identify_patterns_beyond_color(self, board):\n",
    "        \"\"\"Find patterns humans might miss\"\"\"\n",
    "        \n",
    "        patterns = {\n",
    "            'spatial_clusters': self._find_spatial_patterns(board),\n",
    "            'temporal_flow': self._detect_research_progression(board),\n",
    "            'hidden_themes': self._extract_deep_themes(board),\n",
    "            'researcher_style': self._identify_organization_style(board)\n",
    "        }\n",
    "        \n",
    "        return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097dd273-770c-4a87-99ba-e72481344f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRISMFeedbackLoop:\n",
    "    \"\"\"Connect AI #1 and AI #2 for continuous improvement\"\"\"\n",
    "    \n",
    "    def __init__(self, generator, learner):\n",
    "        self.generator = generator\n",
    "        self.learner = learner\n",
    "        self.improvement_history = []\n",
    "        \n",
    "    def train_cycle(self, iterations=10):\n",
    "        \"\"\"Run a complete training cycle\"\"\"\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f\"\\nðŸ”„ Training Cycle {i+1}\")\n",
    "            \n",
    "            # 1. Generate training data\n",
    "            boards = []\n",
    "            for _ in range(20):\n",
    "                board = self.generator.generate_realistic_board(\n",
    "                    scenario=random.choice(list(ResearchScenario)),\n",
    "                    messiness_level=random.random()\n",
    "                )\n",
    "                boards.append(board)\n",
    "            \n",
    "            # 2. Train learner\n",
    "            self.learner.learn_color_context(boards)\n",
    "            \n",
    "            # 3. Test and find failures\n",
    "            test_board = self.generator.generate_realistic_board(\n",
    "                ResearchScenario.USABILITY_TEST\n",
    "            )\n",
    "            results = self.learner.analyze_with_context(test_board)\n",
    "            \n",
    "            # 4. Identify what learner got wrong\n",
    "            failures = self._identify_failures(test_board, results)\n",
    "            \n",
    "            # 5. Tell generator to make harder examples\n",
    "            if failures:\n",
    "                print(f\"  âš ï¸ Found {len(failures)} failures\")\n",
    "                self.generator.difficulty_level += 0.1\n",
    "                self.generator.focus_areas = failures\n",
    "            else:\n",
    "                print(f\"  âœ… No failures - increasing complexity\")\n",
    "                self.generator.add_new_pattern_type()\n",
    "            \n",
    "            self.improvement_history.append({\n",
    "                'cycle': i+1,\n",
    "                'accuracy': 1 - (len(failures) / test_board['note_count']),\n",
    "                'patterns_learned': len(self.learner.pattern_memory)\n",
    "            })\n",
    "        \n",
    "        return self.improvement_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8808d566-a606-4da0-81e7-b7ff805f76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRISMFeedbackLoop:\n",
    "    \"\"\"Connect AI #1 and AI #2 for continuous improvement\"\"\"\n",
    "    \n",
    "    def __init__(self, generator, learner):\n",
    "        self.generator = generator\n",
    "        self.learner = learner\n",
    "        self.improvement_history = []\n",
    "        \n",
    "    def train_cycle(self, iterations=10):\n",
    "        \"\"\"Run a complete training cycle\"\"\"\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f\"\\nðŸ”„ Training Cycle {i+1}\")\n",
    "            \n",
    "            # 1. Generate training data\n",
    "            boards = []\n",
    "            for _ in range(20):\n",
    "                board = self.generator.generate_realistic_board(\n",
    "                    scenario=random.choice(list(ResearchScenario)),\n",
    "                    messiness_level=random.random()\n",
    "                )\n",
    "                boards.append(board)\n",
    "            \n",
    "            # 2. Train learner\n",
    "            self.learner.learn_color_context(boards)\n",
    "            \n",
    "            # 3. Test and find failures\n",
    "            test_board = self.generator.generate_realistic_board(\n",
    "                ResearchScenario.USABILITY_TEST\n",
    "            )\n",
    "            results = self.learner.analyze_with_context(test_board)\n",
    "            \n",
    "            # 4. Identify what learner got wrong\n",
    "            failures = self._identify_failures(test_board, results)\n",
    "            \n",
    "            # 5. Tell generator to make harder examples\n",
    "            if failures:\n",
    "                print(f\"  âš ï¸ Found {len(failures)} failures\")\n",
    "                self.generator.difficulty_level += 0.1\n",
    "                self.generator.focus_areas = failures\n",
    "            else:\n",
    "                print(f\"  âœ… No failures - increasing complexity\")\n",
    "                self.generator.add_new_pattern_type()\n",
    "            \n",
    "            self.improvement_history.append({\n",
    "                'cycle': i+1,\n",
    "                'accuracy': 1 - (len(failures) / test_board['note_count']),\n",
    "                'patterns_learned': len(self.learner.pattern_memory)\n",
    "            })\n",
    "        \n",
    "        return self.improvement_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c03a9d-9133-409a-9712-1593cbe46c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PRISM v2 structure created!\n",
      "\n",
      "ðŸ“Š Your 4-AI Architecture:\n",
      "1. Generator AI âœ… (Working - Expanding)\n",
      "2. Learner AI âš¡ (In Progress - Building)\n",
      "3. Figma Integration ðŸ“ (Planned)\n",
      "4. Visual Output ðŸŽ¨ (Future)\n"
     ]
    }
   ],
   "source": [
    "# Create project structure\n",
    "import os\n",
    "\n",
    "project_structure = {\n",
    "    \"prism_v2/\": {\n",
    "        \"ai_1_generator/\": [\"generator.py\", \"training_data/\", \"configs/\"],\n",
    "        \"ai_2_learner/\": [\"models/\", \"pattern_library/\", \"context_rules/\"],\n",
    "        \"feedback_loop/\": [\"training_history/\", \"failure_analysis/\"],\n",
    "        \"future_integration/\": [\"figma_connector.py\", \"visual_output.py\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for folder in [\"prism_v2/ai_1_generator\", \"prism_v2/ai_2_learner\", \n",
    "               \"prism_v2/feedback_loop\", \"prism_v2/future_integration\"]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ PRISM v2 structure created!\")\n",
    "print(\"\\nðŸ“Š Your 4-AI Architecture:\")\n",
    "print(\"1. Generator AI âœ… (Working - Expanding)\")\n",
    "print(\"2. Learner AI âš¡ (In Progress - Building)\")\n",
    "print(\"3. Figma Integration ðŸ“ (Planned)\")\n",
    "print(\"4. Visual Output ðŸŽ¨ (Future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64455b48-d8e2-4f11-b520-4e2a831aeb6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangdetect\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGlobalResearchGenerator\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langdetect'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "import langdetect\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class GlobalResearchGenerator:\n",
    "    \"\"\"Generate diverse, realistic research data from around the world\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Multilingual content\n",
    "        self.languages = {\n",
    "            'en': {'weight': 0.4, 'direction': 'ltr'},\n",
    "            'es': {'weight': 0.15, 'direction': 'ltr'},\n",
    "            'zh': {'weight': 0.1, 'direction': 'ltr'},\n",
    "            'ar': {'weight': 0.05, 'direction': 'rtl'},\n",
    "            'hi': {'weight': 0.05, 'direction': 'ltr'},\n",
    "            'ja': {'weight': 0.05, 'direction': 'ltr'},\n",
    "            'pt': {'weight': 0.05, 'direction': 'ltr'},\n",
    "            'mixed': {'weight': 0.15, 'direction': 'mixed'}  # Code-switching\n",
    "        }\n",
    "        \n",
    "        # Industry-specific patterns\n",
    "        self.industries = {\n",
    "            'healthcare': {\n",
    "                'jargon': ['patient journey', 'HIPAA', 'EMR', 'clinical workflow', 'triage'],\n",
    "                'pain_points': ['wait times', 'insurance complexity', 'medication adherence'],\n",
    "                'note_style': 'formal',\n",
    "                'color_usage': 'conservative'  # Blues and greens mostly\n",
    "            },\n",
    "            'gaming': {\n",
    "                'jargon': ['player retention', 'rage quit', 'grind', 'RNG', 'meta'],\n",
    "                'pain_points': ['pay to win', 'server lag', 'toxic community', 'balance'],\n",
    "                'note_style': 'casual',\n",
    "                'color_usage': 'vibrant'  # All colors, heavy on purple/pink\n",
    "            },\n",
    "            'finance': {\n",
    "                'jargon': ['KYC', 'onboarding friction', 'conversion', 'churn', 'compliance'],\n",
    "                'pain_points': ['trust issues', 'complex forms', 'security concerns'],\n",
    "                'note_style': 'bullet_points',\n",
    "                'color_usage': 'systematic'  # Strict color coding\n",
    "            },\n",
    "            'education': {\n",
    "                'jargon': ['engagement', 'scaffolding', 'differentiation', 'LMS', 'async'],\n",
    "                'pain_points': ['attention span', 'tech barriers', 'motivation'],\n",
    "                'note_style': 'verbose',\n",
    "                'color_usage': 'rainbow'  # Teachers love colors\n",
    "            },\n",
    "            'ecommerce': {\n",
    "                'jargon': ['cart abandonment', 'checkout flow', 'SKU', 'conversion funnel'],\n",
    "                'pain_points': ['shipping costs', 'return process', 'product discovery'],\n",
    "                'note_style': 'metrics_focused',\n",
    "                'color_usage': 'hierarchical'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Human behavior patterns\n",
    "        self.researcher_personas = {\n",
    "            'meticulous_martha': {\n",
    "                'organization': 0.95,\n",
    "                'color_consistency': 0.9,\n",
    "                'note_length': 'consistent',\n",
    "                'typos': 0.01,\n",
    "                'uses_templates': True\n",
    "            },\n",
    "            'chaotic_carlos': {\n",
    "                'organization': 0.2,\n",
    "                'color_consistency': 0.1,\n",
    "                'note_length': 'random',\n",
    "                'typos': 0.15,\n",
    "                'uses_templates': False\n",
    "            },\n",
    "            'visual_victor': {\n",
    "                'organization': 0.7,\n",
    "                'uses_emoji': True,\n",
    "                'draws_connections': True,\n",
    "                'groups_spatially': True,\n",
    "                'note_length': 'short'\n",
    "            },\n",
    "            'analytical_aisha': {\n",
    "                'organization': 0.8,\n",
    "                'uses_numbers': True,\n",
    "                'categories_everything': True,\n",
    "                'note_length': 'long',\n",
    "                'includes_evidence': True\n",
    "            },\n",
    "            'collaborative_kim': {\n",
    "                'includes_names': True,\n",
    "                'timestamps_everything': True,\n",
    "                'uses_questions': True,\n",
    "                'builds_on_others': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Real-world messiness\n",
    "        self.realistic_errors = {\n",
    "            'typos': ['teh', 'thier', 'doesnt', 'wont', 'cant'],\n",
    "            'incomplete_thoughts': ['...', '???', 'TODO', 'CHECK THIS', '[name?]'],\n",
    "            'emotional_markers': ['!!!', 'ugh', 'YES!', ':)', ':(', 'ðŸ¤”'],\n",
    "            'shortcuts': ['w/', 'b/c', 'esp.', 'vs', 'thru', '&'],\n",
    "            'emphasis': ['IMPORTANT', 'KEY POINT', '****', 'MUST FIX', 'BIG ISSUE']\n",
    "        }\n",
    "        \n",
    "    def generate_multicultural_board(self, scenario_type='mixed'):\n",
    "        \"\"\"Generate boards with cultural and linguistic diversity\"\"\"\n",
    "        \n",
    "        # Pick random industry and persona\n",
    "        industry = random.choice(list(self.industries.keys()))\n",
    "        persona = random.choice(list(self.researcher_personas.keys()))\n",
    "        \n",
    "        board = {\n",
    "            'meta': {\n",
    "                'industry': industry,\n",
    "                'persona': persona,\n",
    "                'languages': [],\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'session_duration': random.randint(15, 180),  # minutes\n",
    "                'participant_count': random.randint(1, 12)\n",
    "            },\n",
    "            'notes': []\n",
    "        }\n",
    "        \n",
    "        # Generate diverse notes\n",
    "        num_notes = random.randint(20, 200)\n",
    "        \n",
    "        for i in range(num_notes):\n",
    "            note = self._generate_realistic_note(industry, persona)\n",
    "            board['notes'].append(note)\n",
    "            \n",
    "        # Add realistic patterns\n",
    "        self._add_temporal_patterns(board)\n",
    "        self._add_spatial_relationships(board)\n",
    "        self._add_human_inconsistencies(board)\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _generate_realistic_note(self, industry, persona):\n",
    "        \"\"\"Create a single realistic note\"\"\"\n",
    "        \n",
    "        persona_traits = self.researcher_personas[persona]\n",
    "        industry_traits = self.industries[industry]\n",
    "        \n",
    "        # Choose language\n",
    "        lang = self._choose_language()\n",
    "        \n",
    "        # Generate content based on all factors\n",
    "        content = self._create_content(lang, industry_traits, persona_traits)\n",
    "        \n",
    "        # Add realistic human touches\n",
    "        if random.random() < persona_traits.get('typos', 0.05):\n",
    "            content = self._add_typos(content)\n",
    "            \n",
    "        if random.random() < 0.3:  # 30% chance of emotional marker\n",
    "            content = self._add_emotion(content)\n",
    "        \n",
    "        # Determine color (not always meaningful!)\n",
    "        color = self._choose_color(industry_traits, persona_traits, content)\n",
    "        \n",
    "        note = {\n",
    "            'content': content,\n",
    "            'color': color,\n",
    "            'language': lang,\n",
    "            'timestamp_offset': random.randint(0, 7200),  # When in session\n",
    "            'author_indicator': self._get_author_indicator(persona_traits),\n",
    "            'confidence': random.uniform(0.3, 1.0),\n",
    "            'is_followup': random.random() < 0.3,  # Builds on another note\n",
    "            'has_action': 'action' in content.lower() or 'todo' in content.lower(),\n",
    "            'formatting': self._get_formatting_style(persona_traits)\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _add_temporal_patterns(self, board):\n",
    "        \"\"\"Add patterns that emerge over time in real sessions\"\"\"\n",
    "        \n",
    "        # Early notes are more exploratory\n",
    "        early_notes = board['notes'][:len(board['notes'])//3]\n",
    "        for note in early_notes:\n",
    "            note['phase'] = 'divergent'\n",
    "            note['certainty'] = random.uniform(0.2, 0.6)\n",
    "        \n",
    "        # Middle notes start clustering\n",
    "        middle_notes = board['notes'][len(board['notes'])//3:2*len(board['notes'])//3]\n",
    "        for note in middle_notes:\n",
    "            note['phase'] = 'clustering'\n",
    "            note['references_earlier'] = random.random() < 0.4\n",
    "        \n",
    "        # Late notes are more conclusive\n",
    "        late_notes = board['notes'][2*len(board['notes'])//3:]\n",
    "        for note in late_notes:\n",
    "            note['phase'] = 'convergent'\n",
    "            note['certainty'] = random.uniform(0.6, 0.9)\n",
    "            note['is_summary'] = random.random() < 0.2\n",
    "    \n",
    "    def _add_human_inconsistencies(self, board):\n",
    "        \"\"\"Add realistic human inconsistencies\"\"\"\n",
    "        \n",
    "        # Sometimes people change color schemes mid-session\n",
    "        if random.random() < 0.3:\n",
    "            switch_point = random.randint(len(board['notes'])//2, len(board['notes']))\n",
    "            for note in board['notes'][switch_point:]:\n",
    "                note['color_scheme_changed'] = True\n",
    "        \n",
    "        # Fatigue effects - later notes might be sloppier\n",
    "        for i, note in enumerate(board['notes']):\n",
    "            fatigue_factor = i / len(board['notes'])\n",
    "            if random.random() < fatigue_factor * 0.5:\n",
    "                note['content'] = note['content'][:len(note['content'])//2] + '...'\n",
    "                note['is_incomplete'] = True\n",
    "        \n",
    "        # Random \"parking lot\" notes that don't fit\n",
    "        num_parking_lot = random.randint(0, 5)\n",
    "        for _ in range(num_parking_lot):\n",
    "            if board['notes']:\n",
    "                random_note = random.choice(board['notes'])\n",
    "                random_note['is_parking_lot'] = True\n",
    "                random_note['position'] = {'x': 900, 'y': random.randint(0, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cad82b-c1d0-4e69-a659-6353ea3a5ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uuid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 294\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# Test it\u001b[39;00m\n\u001b[32m    293\u001b[39m generator = GlobalResearchGenerator()\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m test_board = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_realistic_board\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindustry\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhealthcare\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchaotic_charlie\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomplexity\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedium\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    298\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Generated board for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_board[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mindustry\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Researcher style: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_board[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mresearcher_style\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mGlobalResearchGenerator.generate_realistic_board\u001b[39m\u001b[34m(self, industry, persona, session_type, complexity)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Determine board size based on complexity\u001b[39;00m\n\u001b[32m    115\u001b[39m num_notes = {\n\u001b[32m    116\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msimple\u001b[39m\u001b[33m'\u001b[39m: random.randint(\u001b[32m10\u001b[39m, \u001b[32m30\u001b[39m),\n\u001b[32m    117\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m'\u001b[39m: random.randint(\u001b[32m30\u001b[39m, \u001b[32m80\u001b[39m),\n\u001b[32m    118\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcomplex\u001b[39m\u001b[33m'\u001b[39m: random.randint(\u001b[32m80\u001b[39m, \u001b[32m200\u001b[39m),\n\u001b[32m    119\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moverwhelming\u001b[39m\u001b[33m'\u001b[39m: random.randint(\u001b[32m200\u001b[39m, \u001b[32m500\u001b[39m)\n\u001b[32m    120\u001b[39m }.get(complexity, \u001b[32m50\u001b[39m)\n\u001b[32m    122\u001b[39m board = {\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[43muuid\u001b[49m.uuid4()),\n\u001b[32m    124\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    125\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mindustry\u001b[39m\u001b[33m'\u001b[39m: industry,\n\u001b[32m    126\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mresearcher_style\u001b[39m\u001b[33m'\u001b[39m: persona,\n\u001b[32m    127\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msession_pattern\u001b[39m\u001b[33m'\u001b[39m: session_type,\n\u001b[32m    128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcomplexity\u001b[39m\u001b[33m'\u001b[39m: complexity,\n\u001b[32m    129\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m: datetime.now().isoformat(),\n\u001b[32m    130\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mduration_minutes\u001b[39m\u001b[33m'\u001b[39m: random.randint(\u001b[32m30\u001b[39m, \u001b[32m180\u001b[39m),\n\u001b[32m    131\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mparticipant_count\u001b[39m\u001b[33m'\u001b[39m: random.randint(\u001b[32m1\u001b[39m, \u001b[32m8\u001b[39m)\n\u001b[32m    132\u001b[39m     },\n\u001b[32m    133\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m    134\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpatterns\u001b[39m\u001b[33m'\u001b[39m: {}\n\u001b[32m    135\u001b[39m }\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Generate notes with realistic progression\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_notes):\n",
      "\u001b[31mNameError\u001b[39m: name 'uuid' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class GlobalResearchGenerator:\n",
    "    \"\"\"Generate diverse, realistic research data without external dependencies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Industry-specific patterns (what you really need)\n",
    "        self.industries = {\n",
    "            'healthcare': {\n",
    "                'jargon': ['patient journey', 'HIPAA', 'EMR', 'clinical workflow', 'triage', 'diagnosis'],\n",
    "                'pain_points': ['wait times', 'insurance complexity', 'medication adherence', 'communication gaps'],\n",
    "                'note_style': 'formal',\n",
    "                'color_patterns': {'red': 'critical', 'yellow': 'caution', 'green': 'working'}\n",
    "            },\n",
    "            'gaming': {\n",
    "                'jargon': ['player retention', 'rage quit', 'grind', 'RNG', 'meta', 'nerf', 'buff'],\n",
    "                'pain_points': ['pay to win', 'server lag', 'toxic community', 'balance', 'bugs'],\n",
    "                'note_style': 'casual',\n",
    "                'color_patterns': {'red': 'broken', 'yellow': 'meh', 'purple': 'idea', 'green': 'fun'}\n",
    "            },\n",
    "            'finance': {\n",
    "                'jargon': ['KYC', 'onboarding', 'conversion', 'churn', 'compliance', 'fraud'],\n",
    "                'pain_points': ['trust issues', 'complex forms', 'security concerns', 'fees'],\n",
    "                'note_style': 'bullet_points',\n",
    "                'color_patterns': {'red': 'risk', 'yellow': 'review', 'green': 'approved'}\n",
    "            },\n",
    "            'education': {\n",
    "                'jargon': ['engagement', 'scaffolding', 'differentiation', 'LMS', 'async', 'rubric'],\n",
    "                'pain_points': ['attention span', 'tech barriers', 'motivation', 'assessment'],\n",
    "                'note_style': 'verbose',\n",
    "                'color_patterns': {'red': 'struggling', 'yellow': 'needs support', 'green': 'mastered'}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Different types of researchers (this is KEY for realism)\n",
    "        self.researcher_personas = {\n",
    "            'organized_olivia': {\n",
    "                'consistency': 0.95,  # Uses same color for same type\n",
    "                'completeness': 0.9,  # Finishes thoughts\n",
    "                'clustering': 0.9,    # Groups related items\n",
    "                'style': 'structured'\n",
    "            },\n",
    "            'chaotic_charlie': {\n",
    "                'consistency': 0.2,   # Random colors\n",
    "                'completeness': 0.5,  # Half-finished thoughts\n",
    "                'clustering': 0.1,    # Notes everywhere\n",
    "                'style': 'stream_of_consciousness'\n",
    "            },\n",
    "            'visual_vera': {\n",
    "                'consistency': 0.7,\n",
    "                'completeness': 0.6,\n",
    "                'clustering': 0.8,    # Visual groupings\n",
    "                'style': 'spatial',\n",
    "                'uses_arrows': True,\n",
    "                'uses_emoji': True\n",
    "            },\n",
    "            'analytical_alex': {\n",
    "                'consistency': 0.85,\n",
    "                'completeness': 0.95,\n",
    "                'clustering': 0.75,\n",
    "                'style': 'data_driven',\n",
    "                'uses_numbers': True,\n",
    "                'uses_metrics': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Real patterns that happen in research sessions\n",
    "        self.session_patterns = {\n",
    "            'divergent_start': {\n",
    "                'description': 'Starts broad, exploring all angles',\n",
    "                'note_distribution': 'scattered',\n",
    "                'confidence': 'low_to_high'\n",
    "            },\n",
    "            'convergent_end': {\n",
    "                'description': 'Ends with synthesis and conclusions',\n",
    "                'note_distribution': 'clustered',\n",
    "                'confidence': 'high'\n",
    "            },\n",
    "            'aha_moment': {\n",
    "                'description': 'Sudden realization midway',\n",
    "                'note_distribution': 'explosion',\n",
    "                'confidence': 'spike'\n",
    "            },\n",
    "            'building_consensus': {\n",
    "                'description': 'Multiple people adding and agreeing',\n",
    "                'note_distribution': 'layered',\n",
    "                'confidence': 'increasing'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_realistic_board(self, \n",
    "                                industry='random', \n",
    "                                persona='random',\n",
    "                                session_type='random',\n",
    "                                complexity='medium'):\n",
    "        \"\"\"Generate a board with realistic human patterns\"\"\"\n",
    "        \n",
    "        # Select random parameters if not specified\n",
    "        if industry == 'random':\n",
    "            industry = random.choice(list(self.industries.keys()))\n",
    "        if persona == 'random':\n",
    "            persona = random.choice(list(self.researcher_personas.keys()))\n",
    "        if session_type == 'random':\n",
    "            session_type = random.choice(list(self.session_patterns.keys()))\n",
    "            \n",
    "        industry_data = self.industries[industry]\n",
    "        persona_data = self.researcher_personas[persona]\n",
    "        session_data = self.session_patterns[session_type]\n",
    "        \n",
    "        # Determine board size based on complexity\n",
    "        num_notes = {\n",
    "            'simple': random.randint(10, 30),\n",
    "            'medium': random.randint(30, 80),\n",
    "            'complex': random.randint(80, 200),\n",
    "            'overwhelming': random.randint(200, 500)\n",
    "        }.get(complexity, 50)\n",
    "        \n",
    "        board = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'metadata': {\n",
    "                'industry': industry,\n",
    "                'researcher_style': persona,\n",
    "                'session_pattern': session_type,\n",
    "                'complexity': complexity,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'duration_minutes': random.randint(30, 180),\n",
    "                'participant_count': random.randint(1, 8)\n",
    "            },\n",
    "            'notes': [],\n",
    "            'patterns': {}\n",
    "        }\n",
    "        \n",
    "        # Generate notes with realistic progression\n",
    "        for i in range(num_notes):\n",
    "            note = self._generate_contextual_note(\n",
    "                i, num_notes, industry_data, persona_data, session_data\n",
    "            )\n",
    "            board['notes'].append(note)\n",
    "        \n",
    "        # Add realistic human patterns\n",
    "        self._add_human_patterns(board, persona_data)\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _generate_contextual_note(self, index, total, industry, persona, session):\n",
    "        \"\"\"Generate a note that fits the context\"\"\"\n",
    "        \n",
    "        # Determine phase of research (beginning, middle, end)\n",
    "        progress = index / total\n",
    "        \n",
    "        # Early notes are exploratory\n",
    "        if progress < 0.3:\n",
    "            content_type = random.choice(['question', 'observation', 'pain_point'])\n",
    "            confidence = random.uniform(0.3, 0.6)\n",
    "        # Middle notes are analytical\n",
    "        elif progress < 0.7:\n",
    "            content_type = random.choice(['pain_point', 'observation', 'idea'])\n",
    "            confidence = random.uniform(0.5, 0.8)\n",
    "        # Late notes are conclusive\n",
    "        else:\n",
    "            content_type = random.choice(['opportunity', 'action', 'summary'])\n",
    "            confidence = random.uniform(0.7, 1.0)\n",
    "        \n",
    "        # Generate content based on industry\n",
    "        if content_type == 'pain_point':\n",
    "            content = f\"{random.choice(['User:', 'Problem:', 'Issue:', ''])} {random.choice(industry['pain_points'])}\"\n",
    "        elif content_type == 'question':\n",
    "            content = f\"Why {random.choice(['do users', 'does this', 'can\\'t we'])} {random.choice(industry['jargon'])}?\"\n",
    "        elif content_type == 'observation':\n",
    "            content = f\"Noticed: {random.choice(industry['jargon'])} {random.choice(['happening', 'not working', 'confusing users'])}\"\n",
    "        elif content_type == 'opportunity':\n",
    "            content = f\"Opportunity: {random.choice(['Improve', 'Streamline', 'Simplify'])} {random.choice(industry['jargon'])}\"\n",
    "        elif content_type == 'action':\n",
    "            content = f\"TODO: {random.choice(['Research', 'Test', 'Implement'])} {random.choice(industry['jargon'])}\"\n",
    "        else:  # summary\n",
    "            content = f\"Key insight: {random.choice(industry['pain_points'])} affects {random.choice(industry['jargon'])}\"\n",
    "        \n",
    "        # Add persona-specific styling\n",
    "        if persona.get('uses_emoji') and random.random() < 0.3:\n",
    "            content = random.choice(['ðŸ”´', 'ðŸ’¡', 'âš ï¸', 'âœ…', 'ðŸ¤”']) + ' ' + content\n",
    "        \n",
    "        if persona.get('uses_numbers') and random.random() < 0.4:\n",
    "            content = f\"{random.randint(1,10)}. {content}\"\n",
    "            \n",
    "        # Determine color (NOT always meaningful!)\n",
    "        if persona['consistency'] > 0.7:\n",
    "            # Consistent personas use colors meaningfully\n",
    "            color = self._get_meaningful_color(content_type)\n",
    "        else:\n",
    "            # Chaotic personas use random colors\n",
    "            color = random.choice(['red', 'yellow', 'green', 'blue', 'purple', 'orange', 'pink'])\n",
    "        \n",
    "        # Sometimes people just use default yellow\n",
    "        if random.random() < 0.3:  # 30% chance of default\n",
    "            color = 'yellow'\n",
    "            \n",
    "        note = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'content': content,\n",
    "            'color': color,\n",
    "            'type': content_type,\n",
    "            'confidence': confidence,\n",
    "            'position': self._calculate_position(index, total, session),\n",
    "            'timestamp_offset': index * random.randint(10, 60),  # seconds\n",
    "            'author_hint': f\"P{random.randint(1, 8)}\" if random.random() < 0.2 else None\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _get_meaningful_color(self, content_type):\n",
    "        \"\"\"Get a color that actually means something\"\"\"\n",
    "        color_map = {\n",
    "            'pain_point': 'red',\n",
    "            'opportunity': 'green',\n",
    "            'question': 'blue',\n",
    "            'observation': 'yellow',\n",
    "            'idea': 'purple',\n",
    "            'action': 'orange',\n",
    "            'summary': 'pink'\n",
    "        }\n",
    "        return color_map.get(content_type, 'yellow')\n",
    "    \n",
    "    def _calculate_position(self, index, total, session):\n",
    "        \"\"\"Calculate realistic note positions based on session pattern\"\"\"\n",
    "        \n",
    "        if session['note_distribution'] == 'scattered':\n",
    "            # Random positions\n",
    "            return {\n",
    "                'x': random.randint(50, 950),\n",
    "                'y': random.randint(50, 750)\n",
    "            }\n",
    "        elif session['note_distribution'] == 'clustered':\n",
    "            # Grouped positions\n",
    "            cluster = index // 10\n",
    "            base_x = (cluster * 200) % 800 + 100\n",
    "            base_y = (cluster * 150) % 600 + 100\n",
    "            return {\n",
    "                'x': base_x + random.randint(-50, 50),\n",
    "                'y': base_y + random.randint(-50, 50)\n",
    "            }\n",
    "        elif session['note_distribution'] == 'layered':\n",
    "            # Timeline-like\n",
    "            return {\n",
    "                'x': 100 + (index / total) * 800,\n",
    "                'y': 400 + random.randint(-200, 200)\n",
    "            }\n",
    "        else:\n",
    "            # Default grid\n",
    "            return {\n",
    "                'x': (index * 100) % 900 + 50,\n",
    "                'y': ((index // 9) * 100) % 700 + 50\n",
    "            }\n",
    "    \n",
    "    def _add_human_patterns(self, board, persona):\n",
    "        \"\"\"Add realistic human messiness\"\"\"\n",
    "        \n",
    "        # Sometimes people abandon notes\n",
    "        if random.random() < 0.2:\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                if board['notes']:\n",
    "                    random_note = random.choice(board['notes'])\n",
    "                    random_note['abandoned'] = True\n",
    "                    random_note['content'] = random_note['content'][:20] + '...'\n",
    "        \n",
    "        # Fatigue effect - later notes get sloppier\n",
    "        if persona['completeness'] < 0.7:\n",
    "            for note in board['notes'][-10:]:\n",
    "                if random.random() < 0.3:\n",
    "                    note['content'] = note['content'].lower()\n",
    "                    note['typo'] = True\n",
    "        \n",
    "        # Add relationships between notes\n",
    "        connections = []\n",
    "        for i in range(min(10, len(board['notes']) // 5)):\n",
    "            note1 = random.choice(board['notes'])\n",
    "            note2 = random.choice(board['notes'])\n",
    "            if note1['id'] != note2['id']:\n",
    "                connections.append({\n",
    "                    'from': note1['id'],\n",
    "                    'to': note2['id'],\n",
    "                    'type': random.choice(['builds_on', 'contradicts', 'relates_to'])\n",
    "                })\n",
    "        \n",
    "        board['connections'] = connections\n",
    "        \n",
    "        return board\n",
    "\n",
    "# Test it\n",
    "generator = GlobalResearchGenerator()\n",
    "test_board = generator.generate_realistic_board(\n",
    "    industry='healthcare',\n",
    "    persona='chaotic_charlie',\n",
    "    complexity='medium'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated board for {test_board['metadata']['industry']}\")\n",
    "print(f\"âœ“ Researcher style: {test_board['metadata']['researcher_style']}\")\n",
    "print(f\"âœ“ Notes: {len(test_board['notes'])}\")\n",
    "print(f\"âœ“ This simulates real human research behavior!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef979bf-790c-4560-9fd7-713f3e86749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated board for healthcare\n",
      "âœ“ Researcher style: chaotic_charlie\n",
      "âœ“ Notes: 30\n",
      "âœ“ This simulates real human research behavior!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "import uuid  # This was missing!\n",
    "\n",
    "class GlobalResearchGenerator:\n",
    "    \"\"\"Generate diverse, realistic research data without external dependencies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Industry-specific patterns (what you really need)\n",
    "        self.industries = {\n",
    "            'healthcare': {\n",
    "                'jargon': ['patient journey', 'HIPAA', 'EMR', 'clinical workflow', 'triage', 'diagnosis'],\n",
    "                'pain_points': ['wait times', 'insurance complexity', 'medication adherence', 'communication gaps'],\n",
    "                'note_style': 'formal',\n",
    "                'color_patterns': {'red': 'critical', 'yellow': 'caution', 'green': 'working'}\n",
    "            },\n",
    "            'gaming': {\n",
    "                'jargon': ['player retention', 'rage quit', 'grind', 'RNG', 'meta', 'nerf', 'buff'],\n",
    "                'pain_points': ['pay to win', 'server lag', 'toxic community', 'balance', 'bugs'],\n",
    "                'note_style': 'casual',\n",
    "                'color_patterns': {'red': 'broken', 'yellow': 'meh', 'purple': 'idea', 'green': 'fun'}\n",
    "            },\n",
    "            'finance': {\n",
    "                'jargon': ['KYC', 'onboarding', 'conversion', 'churn', 'compliance', 'fraud'],\n",
    "                'pain_points': ['trust issues', 'complex forms', 'security concerns', 'fees'],\n",
    "                'note_style': 'bullet_points',\n",
    "                'color_patterns': {'red': 'risk', 'yellow': 'review', 'green': 'approved'}\n",
    "            },\n",
    "            'education': {\n",
    "                'jargon': ['engagement', 'scaffolding', 'differentiation', 'LMS', 'async', 'rubric'],\n",
    "                'pain_points': ['attention span', 'tech barriers', 'motivation', 'assessment'],\n",
    "                'note_style': 'verbose',\n",
    "                'color_patterns': {'red': 'struggling', 'yellow': 'needs support', 'green': 'mastered'}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Different types of researchers (this is KEY for realism)\n",
    "        self.researcher_personas = {\n",
    "            'organized_olivia': {\n",
    "                'consistency': 0.95,  # Uses same color for same type\n",
    "                'completeness': 0.9,  # Finishes thoughts\n",
    "                'clustering': 0.9,    # Groups related items\n",
    "                'style': 'structured'\n",
    "            },\n",
    "            'chaotic_charlie': {\n",
    "                'consistency': 0.2,   # Random colors\n",
    "                'completeness': 0.5,  # Half-finished thoughts\n",
    "                'clustering': 0.1,    # Notes everywhere\n",
    "                'style': 'stream_of_consciousness'\n",
    "            },\n",
    "            'visual_vera': {\n",
    "                'consistency': 0.7,\n",
    "                'completeness': 0.6,\n",
    "                'clustering': 0.8,    # Visual groupings\n",
    "                'style': 'spatial',\n",
    "                'uses_arrows': True,\n",
    "                'uses_emoji': True\n",
    "            },\n",
    "            'analytical_alex': {\n",
    "                'consistency': 0.85,\n",
    "                'completeness': 0.95,\n",
    "                'clustering': 0.75,\n",
    "                'style': 'data_driven',\n",
    "                'uses_numbers': True,\n",
    "                'uses_metrics': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Real patterns that happen in research sessions\n",
    "        self.session_patterns = {\n",
    "            'divergent_start': {\n",
    "                'description': 'Starts broad, exploring all angles',\n",
    "                'note_distribution': 'scattered',\n",
    "                'confidence': 'low_to_high'\n",
    "            },\n",
    "            'convergent_end': {\n",
    "                'description': 'Ends with synthesis and conclusions',\n",
    "                'note_distribution': 'clustered',\n",
    "                'confidence': 'high'\n",
    "            },\n",
    "            'aha_moment': {\n",
    "                'description': 'Sudden realization midway',\n",
    "                'note_distribution': 'explosion',\n",
    "                'confidence': 'spike'\n",
    "            },\n",
    "            'building_consensus': {\n",
    "                'description': 'Multiple people adding and agreeing',\n",
    "                'note_distribution': 'layered',\n",
    "                'confidence': 'increasing'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_realistic_board(self, \n",
    "                                industry='random', \n",
    "                                persona='random',\n",
    "                                session_type='random',\n",
    "                                complexity='medium'):\n",
    "        \"\"\"Generate a board with realistic human patterns\"\"\"\n",
    "        \n",
    "        # Select random parameters if not specified\n",
    "        if industry == 'random':\n",
    "            industry = random.choice(list(self.industries.keys()))\n",
    "        if persona == 'random':\n",
    "            persona = random.choice(list(self.researcher_personas.keys()))\n",
    "        if session_type == 'random':\n",
    "            session_type = random.choice(list(self.session_patterns.keys()))\n",
    "            \n",
    "        industry_data = self.industries[industry]\n",
    "        persona_data = self.researcher_personas[persona]\n",
    "        session_data = self.session_patterns[session_type]\n",
    "        \n",
    "        # Determine board size based on complexity\n",
    "        num_notes = {\n",
    "            'simple': random.randint(10, 30),\n",
    "            'medium': random.randint(30, 80),\n",
    "            'complex': random.randint(80, 200),\n",
    "            'overwhelming': random.randint(200, 500)\n",
    "        }.get(complexity, 50)\n",
    "        \n",
    "        board = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'metadata': {\n",
    "                'industry': industry,\n",
    "                'researcher_style': persona,\n",
    "                'session_pattern': session_type,\n",
    "                'complexity': complexity,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'duration_minutes': random.randint(30, 180),\n",
    "                'participant_count': random.randint(1, 8)\n",
    "            },\n",
    "            'notes': [],\n",
    "            'patterns': {}\n",
    "        }\n",
    "        \n",
    "        # Generate notes with realistic progression\n",
    "        for i in range(num_notes):\n",
    "            note = self._generate_contextual_note(\n",
    "                i, num_notes, industry_data, persona_data, session_data\n",
    "            )\n",
    "            board['notes'].append(note)\n",
    "        \n",
    "        # Add realistic human patterns\n",
    "        self._add_human_patterns(board, persona_data)\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def _generate_contextual_note(self, index, total, industry, persona, session):\n",
    "        \"\"\"Generate a note that fits the context\"\"\"\n",
    "        \n",
    "        # Determine phase of research (beginning, middle, end)\n",
    "        progress = index / total\n",
    "        \n",
    "        # Early notes are exploratory\n",
    "        if progress < 0.3:\n",
    "            content_type = random.choice(['question', 'observation', 'pain_point'])\n",
    "            confidence = random.uniform(0.3, 0.6)\n",
    "        # Middle notes are analytical\n",
    "        elif progress < 0.7:\n",
    "            content_type = random.choice(['pain_point', 'observation', 'idea'])\n",
    "            confidence = random.uniform(0.5, 0.8)\n",
    "        # Late notes are conclusive\n",
    "        else:\n",
    "            content_type = random.choice(['opportunity', 'action', 'summary'])\n",
    "            confidence = random.uniform(0.7, 1.0)\n",
    "        \n",
    "        # Generate content based on industry\n",
    "        if content_type == 'pain_point':\n",
    "            content = f\"{random.choice(['User:', 'Problem:', 'Issue:', ''])} {random.choice(industry['pain_points'])}\"\n",
    "        elif content_type == 'question':\n",
    "            content = f\"Why {random.choice(['do users', 'does this', 'can\\'t we'])} {random.choice(industry['jargon'])}?\"\n",
    "        elif content_type == 'observation':\n",
    "            content = f\"Noticed: {random.choice(industry['jargon'])} {random.choice(['happening', 'not working', 'confusing users'])}\"\n",
    "        elif content_type == 'opportunity':\n",
    "            content = f\"Opportunity: {random.choice(['Improve', 'Streamline', 'Simplify'])} {random.choice(industry['jargon'])}\"\n",
    "        elif content_type == 'action':\n",
    "            content = f\"TODO: {random.choice(['Research', 'Test', 'Implement'])} {random.choice(industry['jargon'])}\"\n",
    "        else:  # summary\n",
    "            content = f\"Key insight: {random.choice(industry['pain_points'])} affects {random.choice(industry['jargon'])}\"\n",
    "        \n",
    "        # Add persona-specific styling\n",
    "        if persona.get('uses_emoji') and random.random() < 0.3:\n",
    "            content = random.choice(['ðŸ”´', 'ðŸ’¡', 'âš ï¸', 'âœ…', 'ðŸ¤”']) + ' ' + content\n",
    "        \n",
    "        if persona.get('uses_numbers') and random.random() < 0.4:\n",
    "            content = f\"{random.randint(1,10)}. {content}\"\n",
    "            \n",
    "        # Determine color (NOT always meaningful!)\n",
    "        if persona['consistency'] > 0.7:\n",
    "            # Consistent personas use colors meaningfully\n",
    "            color = self._get_meaningful_color(content_type)\n",
    "        else:\n",
    "            # Chaotic personas use random colors\n",
    "            color = random.choice(['red', 'yellow', 'green', 'blue', 'purple', 'orange', 'pink'])\n",
    "        \n",
    "        # Sometimes people just use default yellow\n",
    "        if random.random() < 0.3:  # 30% chance of default\n",
    "            color = 'yellow'\n",
    "            \n",
    "        note = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'content': content,\n",
    "            'color': color,\n",
    "            'type': content_type,\n",
    "            'confidence': confidence,\n",
    "            'position': self._calculate_position(index, total, session),\n",
    "            'timestamp_offset': index * random.randint(10, 60),  # seconds\n",
    "            'author_hint': f\"P{random.randint(1, 8)}\" if random.random() < 0.2 else None\n",
    "        }\n",
    "        \n",
    "        return note\n",
    "    \n",
    "    def _get_meaningful_color(self, content_type):\n",
    "        \"\"\"Get a color that actually means something\"\"\"\n",
    "        color_map = {\n",
    "            'pain_point': 'red',\n",
    "            'opportunity': 'green',\n",
    "            'question': 'blue',\n",
    "            'observation': 'yellow',\n",
    "            'idea': 'purple',\n",
    "            'action': 'orange',\n",
    "            'summary': 'pink'\n",
    "        }\n",
    "        return color_map.get(content_type, 'yellow')\n",
    "    \n",
    "    def _calculate_position(self, index, total, session):\n",
    "        \"\"\"Calculate realistic note positions based on session pattern\"\"\"\n",
    "        \n",
    "        if session['note_distribution'] == 'scattered':\n",
    "            # Random positions\n",
    "            return {\n",
    "                'x': random.randint(50, 950),\n",
    "                'y': random.randint(50, 750)\n",
    "            }\n",
    "        elif session['note_distribution'] == 'clustered':\n",
    "            # Grouped positions\n",
    "            cluster = index // 10\n",
    "            base_x = (cluster * 200) % 800 + 100\n",
    "            base_y = (cluster * 150) % 600 + 100\n",
    "            return {\n",
    "                'x': base_x + random.randint(-50, 50),\n",
    "                'y': base_y + random.randint(-50, 50)\n",
    "            }\n",
    "        elif session['note_distribution'] == 'layered':\n",
    "            # Timeline-like\n",
    "            return {\n",
    "                'x': 100 + (index / total) * 800,\n",
    "                'y': 400 + random.randint(-200, 200)\n",
    "            }\n",
    "        else:\n",
    "            # Default grid\n",
    "            return {\n",
    "                'x': (index * 100) % 900 + 50,\n",
    "                'y': ((index // 9) * 100) % 700 + 50\n",
    "            }\n",
    "    \n",
    "    def _add_human_patterns(self, board, persona):\n",
    "        \"\"\"Add realistic human messiness\"\"\"\n",
    "        \n",
    "        # Sometimes people abandon notes\n",
    "        if random.random() < 0.2:\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                if board['notes']:\n",
    "                    random_note = random.choice(board['notes'])\n",
    "                    random_note['abandoned'] = True\n",
    "                    random_note['content'] = random_note['content'][:20] + '...'\n",
    "        \n",
    "        # Fatigue effect - later notes get sloppier\n",
    "        if persona['completeness'] < 0.7:\n",
    "            for note in board['notes'][-10:]:\n",
    "                if random.random() < 0.3:\n",
    "                    note['content'] = note['content'].lower()\n",
    "                    note['typo'] = True\n",
    "        \n",
    "        # Add relationships between notes\n",
    "        connections = []\n",
    "        for i in range(min(10, len(board['notes']) // 5)):\n",
    "            note1 = random.choice(board['notes'])\n",
    "            note2 = random.choice(board['notes'])\n",
    "            if note1['id'] != note2['id']:\n",
    "                connections.append({\n",
    "                    'from': note1['id'],\n",
    "                    'to': note2['id'],\n",
    "                    'type': random.choice(['builds_on', 'contradicts', 'relates_to'])\n",
    "                })\n",
    "        \n",
    "        board['connections'] = connections\n",
    "        \n",
    "        return board\n",
    "\n",
    "# Test it\n",
    "generator = GlobalResearchGenerator()\n",
    "test_board = generator.generate_realistic_board(\n",
    "    industry='healthcare',\n",
    "    persona='chaotic_charlie',\n",
    "    complexity='medium'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated board for {test_board['metadata']['industry']}\")\n",
    "print(f\"âœ“ Researcher style: {test_board['metadata']['researcher_style']}\")\n",
    "print(f\"âœ“ Notes: {len(test_board['notes'])}\")\n",
    "print(f\"âœ“ This simulates real human research behavior!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da73caa-e6cb-4bf3-8187-68baf2e64195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
