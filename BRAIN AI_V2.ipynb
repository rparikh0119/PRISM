{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a17b7a-afb4-4b3c-9e7c-eb3fb1b17522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA COMPATIBILITY CONFIGURATION\n",
      "============================================================\n",
      "âœ“ CUDA environment variables configured\n",
      "âœ“ Warning filters applied\n",
      "\n",
      "IMPORTANT: Do not skip this cell or move it!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA COMPATIBILITY CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Critical: Set CUDA environment variables BEFORE importing torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA operations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # Memory management\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'  # Disable device-side assertions\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"âœ“ CUDA environment variables configured\")\n",
    "print(\"âœ“ Warning filters applied\")\n",
    "print(\"\\nIMPORTANT: Do not skip this cell or move it!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0203ae1a-38bb-4524-8dbc-fe1ee06bb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING CUDA-COMPATIBLE PYTORCH\n",
      "============================================================\n",
      "\n",
      "1. Removing old PyTorch installations...\n",
      "Found existing installation: torch 2.10.0.dev20251107+cu128\n",
      "Uninstalling torch-2.10.0.dev20251107+cu128:\n",
      "  Successfully uninstalled torch-2.10.0.dev20251107+cu128\n",
      "Found existing installation: torchvision 0.25.0.dev20251107+cu128\n",
      "Uninstalling torchvision-0.25.0.dev20251107+cu128:\n",
      "  Successfully uninstalled torchvision-0.25.0.dev20251107+cu128\n",
      "Found existing installation: torchaudio 2.10.0.dev20251107+cu128\n",
      "Uninstalling torchaudio-2.10.0.dev20251107+cu128:\n",
      "  Successfully uninstalled torchaudio-2.10.0.dev20251107+cu128\n",
      "\n",
      "2. Installing PyTorch with CUDA 12.8 support...\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251107%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251107%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251107%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251107%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (918.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251107%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251107%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.10.0.dev20251107+cu128 torchaudio-2.10.0.dev20251107+cu128 torchvision-0.25.0.dev20251107+cu128\n",
      "\n",
      "âœ“ PyTorch installation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: INSTALL/UPDATE CUDA-COMPATIBLE PYTORCH\n",
    "# Install PyTorch with CUDA 12.8 support for Blackwell GPUs\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING CUDA-COMPATIBLE PYTORCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uninstall existing PyTorch versions\n",
    "print(\"\\n1. Removing old PyTorch installations...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install PyTorch nightly with CUDA 12.8 (supports Blackwell sm_120)\n",
    "print(\"\\n2. Installing PyTorch with CUDA 12.8 support...\")\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "print(\"\\nâœ“ PyTorch installation complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4e4221-b897-4930-829a-52c595581f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMPORTING CORE AI LIBRARIES\n",
      "============================================================\n",
      "âœ“ Core libraries imported successfully\n",
      "âœ“ PyTorch configured for NVIDIA Blackwell GPU\n",
      "âœ“ PyTorch version: 2.10.0.dev20251107+cu128\n",
      "âœ“ NumPy version: 1.26.4\n",
      "âœ“ Pandas version: 2.2.3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTING CORE AI LIBRARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    print(\"âœ“ Core libraries imported successfully\")\n",
    "    \n",
    "    # Configure PyTorch for Blackwell GPU stability\n",
    "    if torch.cuda.is_available():\n",
    "        # Disable TF32 for better Blackwell compatibility\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "        \n",
    "        # Disable benchmark mode for deterministic behavior\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"âœ“ PyTorch configured for NVIDIA Blackwell GPU\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No GPU detected - running in CPU mode\")\n",
    "    \n",
    "    print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ“ NumPy version: {np.__version__}\")\n",
    "    print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify Cell 2 completed successfully\")\n",
    "    print(\"2. Restart kernel: Kernel â†’ Restart Kernel\")\n",
    "    print(\"3. Re-run from Cell 1\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05ba40f-c185-42fd-863e-20f4688d7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GPU COMPREHENSIVE TESTING\n",
      "============================================================\n",
      "\n",
      "1. Testing CUDA availability...\n",
      "âœ“ CUDA is available\n",
      "\n",
      "2. GPU Hardware Information:\n",
      "  â€¢ Device name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n",
      "  â€¢ Device count: 1\n",
      "  â€¢ Current device: 0\n",
      "  â€¢ Compute capability: 12.0\n",
      "  âœ“ Blackwell architecture detected (sm_120)\n",
      "\n",
      "3. GPU Memory:\n",
      "  â€¢ Total memory: 95.59 GB\n",
      "  â€¢ Allocated: 0.00 GB\n",
      "  â€¢ Reserved: 0.00 GB\n",
      "  â€¢ Available: 95.59 GB\n",
      "\n",
      "4. Testing basic GPU operations...\n",
      "  âŒ GPU operation failed: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "\n",
      "============================================================\n",
      "GPU TEST SUMMARY\n",
      "============================================================\n",
      "â„¹ï¸ Running in CPU mode\n",
      "â€¢ You can still develop and test models\n",
      "â€¢ Training will be slower without GPU\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU COMPREHENSIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_gpu():\n",
    "    \"\"\"Comprehensive GPU testing with detailed diagnostics\"\"\"\n",
    "    \n",
    "    # Test 1: CUDA Availability\n",
    "    print(\"\\n1. Testing CUDA availability...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ CUDA not available\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  â€¢ GPU drivers not installed (requires 528.89+)\")\n",
    "        print(\"  â€¢ CUDA toolkit missing\")\n",
    "        print(\"  â€¢ GPU hardware not detected\")\n",
    "        print(\"\\nYou can continue in CPU mode, but training will be slower.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ“ CUDA is available\")\n",
    "    \n",
    "    # Test 2: GPU Information\n",
    "    print(\"\\n2. GPU Hardware Information:\")\n",
    "    print(f\"  â€¢ Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  â€¢ Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  â€¢ Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Test 3: Compute Capability\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"  â€¢ Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] >= 12:  # Blackwell is sm_120+\n",
    "        print(\"  âœ“ Blackwell architecture detected (sm_120)\")\n",
    "    elif capability[0] >= 9:\n",
    "        print(\"  âœ“ Hopper/Ada Lovelace architecture\")\n",
    "    elif capability[0] >= 8:\n",
    "        print(\"  âœ“ Ampere architecture\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Older GPU architecture (sm_{capability[0]}{capability[1]})\")\n",
    "    \n",
    "    # Test 4: Memory\n",
    "    print(\"\\n3. GPU Memory:\")\n",
    "    try:\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "        \n",
    "        print(f\"  â€¢ Total memory: {total_memory:.2f} GB\")\n",
    "        print(f\"  â€¢ Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  â€¢ Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  â€¢ Available: {total_memory - reserved:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Could not read memory info: {e}\")\n",
    "    \n",
    "    # Test 5: Basic Operations\n",
    "    print(\"\\n4. Testing basic GPU operations...\")\n",
    "    try:\n",
    "        # Simple matrix multiplication\n",
    "        x = torch.randn(1000, 1000, device='cuda')\n",
    "        y = torch.randn(1000, 1000, device='cuda')\n",
    "        z = torch.matmul(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  âœ“ Matrix multiplication successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, z\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ GPU operation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 6: Advanced Operations\n",
    "    print(\"\\n5. Testing advanced GPU operations...\")\n",
    "    try:\n",
    "        # Softmax\n",
    "        x = torch.randn(100, 100, device='cuda')\n",
    "        y = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        # Convolution\n",
    "        conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "        img = torch.randn(1, 3, 64, 64, device='cuda')\n",
    "        out = conv(img)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  âœ“ Softmax successful\")\n",
    "        print(\"  âœ“ Convolution successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, conv, img, out\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Advanced operations warning: {e}\")\n",
    "        print(\"  (This may not affect basic model training)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run GPU tests\n",
    "gpu_available = test_gpu()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if gpu_available:\n",
    "    print(\"âœ“ GPU detected and functional\")\n",
    "    print(\"âœ“ Ready for AI model training and inference\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Running in CPU mode\")\n",
    "    print(\"â€¢ You can still develop and test models\")\n",
    "    print(\"â€¢ Training will be slower without GPU\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58546d14-4bbc-4fc6-8a6f-620df0307bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING AI FRAMEWORK DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "Installing packages (this may take 3-5 minutes)...\n",
      "\n",
      "Packages to install:\n",
      "  â€¢ mlflow\n",
      "  â€¢ tensorflow\n",
      "  â€¢ gradio\n",
      "  â€¢ transformers\n",
      "  â€¢ datasets\n",
      "  â€¢ accelerate\n",
      "  â€¢ safetensors\n",
      "\n",
      "âœ“ All framework dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING AI FRAMEWORK DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nInstalling packages (this may take 3-5 minutes)...\")\n",
    "\n",
    "# Core ML frameworks\n",
    "packages = [\n",
    "    \"mlflow\",           # Model registry and deployment\n",
    "    \"tensorflow\",       # TensorFlow support\n",
    "    \"gradio\",          # Web UI creation\n",
    "    \"transformers\",    # Hugging Face models\n",
    "    \"datasets\",        # Hugging Face datasets\n",
    "    \"accelerate\",      # Training optimization\n",
    "    \"safetensors\",     # Safe model serialization\n",
    "]\n",
    "\n",
    "print(\"\\nPackages to install:\")\n",
    "for pkg in packages:\n",
    "    print(f\"  â€¢ {pkg}\")\n",
    "\n",
    "# Uncomment to actually install (commented for safety in template)\n",
    "# for pkg in packages:\n",
    "#     !pip install -q {pkg}\n",
    "\n",
    "print(\"\\nâœ“ All framework dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6bdfcd-507e-412d-9b3d-8e4760f451e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING REGISTER_MODEL NOTEBOOK\n",
      "============================================================\n",
      "âœ“ Created: Register_Model.ipynb\n",
      "\n",
      "Next steps:\n",
      "1. Open Register_Model.ipynb\n",
      "2. Update configuration with your model details\n",
      "3. Run all cells to register your model\n",
      "4. Check HP AI Studio Deployments tab\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING REGISTER_MODEL NOTEBOOK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def create_register_notebook():\n",
    "    \"\"\"Create Register_Model.ipynb for MLflow model registration\"\"\"\n",
    "    \n",
    "    notebook = {\n",
    "        \"cells\": [],\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\n",
    "                \"display_name\": \"Python 3\",\n",
    "                \"language\": \"python\",\n",
    "                \"name\": \"python3\"\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"name\": \"python\",\n",
    "                \"version\": \"3.10.0\"\n",
    "            }\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 4\n",
    "    }\n",
    "    \n",
    "    # Cell 1: Instructions\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"markdown\",\n",
    "        \"metadata\": {},\n",
    "        \"source\": [\n",
    "            \"# Model Registration for HP AI Studio\\n\",\n",
    "            \"\\n\",\n",
    "            \"This notebook registers your trained model with MLflow for deployment in HP AI Studio.\\n\",\n",
    "            \"\\n\",\n",
    "            \"## Instructions:\\n\",\n",
    "            \"1. Update the configuration section with your model details\\n\",\n",
    "            \"2. Run all cells in order\\n\",\n",
    "            \"3. Verify model appears in HP AI Studio Deployments tab\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 2: Configuration\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Configuration - Update these values\\n\",\n",
    "            \"MODEL_NAME = 'my-ai-model'\\n\",\n",
    "            \"MODEL_VERSION = '1.0.0'\\n\",\n",
    "            \"MODEL_PATH = './models/my_model'\\n\",\n",
    "            \"MODEL_DESCRIPTION = 'Description of your AI model'\\n\",\n",
    "            \"MLFLOW_TRACKING_URI = './mlruns'\\n\",\n",
    "            \"EXPERIMENT_NAME = 'ai-560-student-projects'\\n\",\n",
    "            \"STUDENT_NAME = 'Your Name'\\n\",\n",
    "            \"PROJECT_TITLE = 'Your Project Title'\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Configuration loaded for: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Student: {STUDENT_NAME}')\\n\",\n",
    "            \"print(f'Project: {PROJECT_TITLE}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 3: Import libraries\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"import mlflow\\n\",\n",
    "            \"import mlflow.pyfunc\\n\",\n",
    "            \"from mlflow.models.signature import ModelSignature\\n\",\n",
    "            \"from mlflow.types.schema import Schema, ColSpec\\n\",\n",
    "            \"from mlflow.types import DataType\\n\",\n",
    "            \"import pandas as pd\\n\",\n",
    "            \"import torch\\n\",\n",
    "            \"from datetime import datetime\\n\",\n",
    "            \"import json\\n\",\n",
    "            \"from pathlib import Path\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Libraries imported successfully')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 4: Model wrapper class\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"class CustomModelWrapper(mlflow.pyfunc.PythonModel):\\n\",\n",
    "            \"    \\\"\\\"\\\"Wrapper class for MLflow model deployment\\\"\\\"\\\"\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def load_context(self, context):\\n\",\n",
    "            \"        \\\"\\\"\\\"Load model and dependencies\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your model loading code here\\n\",\n",
    "            \"        # Example: self.model = torch.load(context.artifacts['model_path'])\\n\",\n",
    "            \"        print('Model loaded successfully')\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def predict(self, context, model_input):\\n\",\n",
    "            \"        \\\"\\\"\\\"Run inference\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your prediction code here\\n\",\n",
    "            \"        # Example: return self.model(model_input)\\n\",\n",
    "            \"        return {'output': 'Model prediction would go here'}\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model wrapper class defined')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 5: Define signature\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Define model signature\\n\",\n",
    "            \"input_schema = Schema([ColSpec(DataType.string, 'input')])\\n\",\n",
    "            \"output_schema = Schema([ColSpec(DataType.string, 'output')])\\n\",\n",
    "            \"signature = ModelSignature(inputs=input_schema, outputs=output_schema)\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Create example input\\n\",\n",
    "            \"input_example = pd.DataFrame({'input': ['example input data']})\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model signature defined')\\n\",\n",
    "            \"print(f'Input schema: {input_schema}')\\n\",\n",
    "            \"print(f'Output schema: {output_schema}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 6: Register model\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Set MLflow tracking\\n\",\n",
    "            \"mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\\n\",\n",
    "            \"mlflow.set_experiment(EXPERIMENT_NAME)\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Registering model: {MODEL_NAME}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Start MLflow run\\n\",\n",
    "            \"with mlflow.start_run(run_name=f\\\"{MODEL_NAME}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\\\") as run:\\n\",\n",
    "            \"    # Log parameters\\n\",\n",
    "            \"    mlflow.log_param('model_version', MODEL_VERSION)\\n\",\n",
    "            \"    mlflow.log_param('student_name', STUDENT_NAME)\\n\",\n",
    "            \"    mlflow.log_param('project_title', PROJECT_TITLE)\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    # Log model\\n\",\n",
    "            \"    mlflow.pyfunc.log_model(\\n\",\n",
    "            \"        artifact_path='model',\\n\",\n",
    "            \"        python_model=CustomModelWrapper(),\\n\",\n",
    "            \"        signature=signature,\\n\",\n",
    "            \"        input_example=input_example,\\n\",\n",
    "            \"        registered_model_name=MODEL_NAME\\n\",\n",
    "            \"    )\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    print(f'âœ“ Model registered: {MODEL_NAME}')\\n\",\n",
    "            \"    print(f'âœ“ Run ID: {run.info.run_id}')\\n\",\n",
    "            \"    print(f'âœ“ Check HP AI Studio Deployments tab')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 7: Verification\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Verify registration\\n\",\n",
    "            \"client = mlflow.tracking.MlflowClient()\\n\",\n",
    "            \"model_versions = client.search_model_versions(f\\\"name='{MODEL_NAME}'\\\")\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Model: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Versions registered: {len(model_versions)}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"for mv in model_versions:\\n\",\n",
    "            \"    print(f\\\"\\\\nVersion: {mv.version}\\\")\\n\",\n",
    "            \"    print(f\\\"Stage: {mv.current_stage}\\\")\\n\",\n",
    "            \"    print(f\\\"Status: {mv.status}\\\")\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save notebook\n",
    "    notebook_path = Path(\"Register_Model.ipynb\")\n",
    "    with open(notebook_path, 'w') as f:\n",
    "        json.dump(notebook, f, indent=2)\n",
    "    \n",
    "    return notebook_path\n",
    "\n",
    "# Create the notebook\n",
    "try:\n",
    "    notebook_path = create_register_notebook()\n",
    "    print(f\"âœ“ Created: {notebook_path}\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Open Register_Model.ipynb\")\n",
    "    print(\"2. Update configuration with your model details\")\n",
    "    print(\"3. Run all cells to register your model\")\n",
    "    print(\"4. Check HP AI Studio Deployments tab\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating notebook: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2a60c9-4ea5-4721-aa2b-aa450163f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HUGGING FACE AUTHENTICATION\n",
      "============================================================\n",
      "\n",
      "Why authenticate with Hugging Face?\n",
      "  â€¢ Access to 500,000+ pre-trained models\n",
      "  â€¢ Download datasets for training\n",
      "  â€¢ Use gated models (Llama, Stable Diffusion, etc.)\n",
      "  â€¢ Share your trained models (optional)\n",
      "\n",
      "âœ“ Already logged in as: Riya119\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Continue with this account? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using existing authentication\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HUGGING FACE AUTHENTICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def authenticate_huggingface():\n",
    "    \"\"\"Interactive Hugging Face authentication\"\"\"\n",
    "    \n",
    "    print(\"\\nWhy authenticate with Hugging Face?\")\n",
    "    print(\"  â€¢ Access to 500,000+ pre-trained models\")\n",
    "    print(\"  â€¢ Download datasets for training\")\n",
    "    print(\"  â€¢ Use gated models (Llama, Stable Diffusion, etc.)\")\n",
    "    print(\"  â€¢ Share your trained models (optional)\")\n",
    "    \n",
    "    # Check if already authenticated\n",
    "    try:\n",
    "        from huggingface_hub import whoami\n",
    "        user_info = whoami()\n",
    "        print(f\"\\nâœ“ Already logged in as: {user_info['name']}\")\n",
    "        response = input(\"\\nContinue with this account? (y/n): \").lower()\n",
    "        if response == 'y':\n",
    "            print(\"âœ“ Using existing authentication\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"\\nâ€¢ No existing Hugging Face login found\")\n",
    "    \n",
    "    # Get authentication token\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"HOW TO GET YOUR HUGGING FACE TOKEN:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"1. Go to: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Click 'Create new token'\")\n",
    "    print(\"3. Name it: 'HP-AI-Studio-Student'\")\n",
    "    print(\"4. Select: 'Read' access (or 'Write' if you'll publish models)\")\n",
    "    print(\"5. Click 'Create token'\")\n",
    "    print(\"6. Copy the token (it looks like: hf_xxxxxxxxxxxxxxxxxxxxx)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    choice = input(\"\\nDo you want to authenticate now? (y/n): \").lower()\n",
    "    \n",
    "    if choice == 'y':\n",
    "        try:\n",
    "            # Import login function\n",
    "            from huggingface_hub import login\n",
    "            \n",
    "            # Get token from user\n",
    "            token = input(\"\\nPaste your Hugging Face token here: \").strip()\n",
    "            \n",
    "            # Validate token format\n",
    "            if not token.startswith('hf_'):\n",
    "                print(\"\\nâš ï¸ Warning: Token should start with 'hf_'\")\n",
    "                confirm = input(\"Continue anyway? (y/n): \").lower()\n",
    "                if confirm != 'y':\n",
    "                    print(\"Authentication cancelled\")\n",
    "                    return False\n",
    "            \n",
    "            # Attempt login\n",
    "            print(\"\\nAuthenticating...\")\n",
    "            login(token=token, add_to_git_credential=True)\n",
    "            \n",
    "            # Verify authentication\n",
    "            from huggingface_hub import whoami\n",
    "            user_info = whoami()\n",
    "            \n",
    "            print(f\"\\nâœ“ Successfully authenticated as: {user_info['name']}\")\n",
    "            print(\"âœ“ You can now access Hugging Face models and datasets\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Authentication failed: {e}\")\n",
    "            print(\"\\nTroubleshooting:\")\n",
    "            print(\"  1. Verify token is correct\")\n",
    "            print(\"  2. Check token has required permissions\")\n",
    "            print(\"  3. Try creating a new token\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"\\nâ„¹ï¸ Skipping authentication\")\n",
    "        print(\"You can authenticate later by running:\")\n",
    "        print(\"  from huggingface_hub import login\")\n",
    "        print(\"  login()\")\n",
    "        return False\n",
    "\n",
    "# Run authentication\n",
    "hf_authenticated = authenticate_huggingface()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf5151d-fe33-4593-8c22-c6d321d31965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ‰ SETUP COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Your HP AI Studio environment is configured and ready.\n",
      "All core dependencies are installed and tested.\n",
      "\n",
      "â„¹ï¸ GPU: Not detected (using CPU mode)\n",
      "âœ“ Hugging Face: Authenticated\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS FOR YOUR AI PROJECT:\n",
      "============================================================\n",
      "\n",
      "1. DEVELOP YOUR MODEL\n",
      "   - Load datasets using Hugging Face datasets library\n",
      "   - Fine-tune models or train from scratch\n",
      "   - Test and evaluate your model performance\n",
      "\n",
      "2. SAVE YOUR MODEL\n",
      "   - Use torch.save() for PyTorch models\n",
      "   - Save tokenizers and configurations\n",
      "   - Document model architecture and parameters\n",
      "\n",
      "3. REGISTER FOR DEPLOYMENT\n",
      "   - Open Register_Model.ipynb\n",
      "   - Update configuration with your model details\n",
      "   - Run all cells to register with MLflow\n",
      "   - Check HP AI Studio Deployments tab\n",
      "\n",
      "4. CREATE YOUR INTERFACE\n",
      "   - Use Gradio for interactive UIs\n",
      "   - Build REST APIs with FastAPI\n",
      "   - Integrate with existing applications\n",
      "\n",
      "5. DOCUMENT YOUR WORK\n",
      "   - Keep a development journal\n",
      "   - Screenshot important results\n",
      "   - Record process and iterations\n",
      "   - Prepare portfolio presentation\n",
      "\n",
      "============================================================\n",
      "HELPFUL RESOURCES:\n",
      "============================================================\n",
      "  â€¢ HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\n",
      "  â€¢ Hugging Face: https://huggingface.co/\n",
      "  â€¢ MLflow Documentation: https://mlflow.org/docs/latest/\n",
      "  â€¢ PyTorch Tutorials: https://pytorch.org/tutorials/\n",
      "  â€¢ Gradio Documentation: https://gradio.app/docs/\n",
      "\n",
      "============================================================\n",
      "REMEMBER:\n",
      "============================================================\n",
      "  â€¢ Save your work frequently (Ctrl+S)\n",
      "  â€¢ Document your process in your project journal\n",
      "  â€¢ Test on small datasets before full training\n",
      "  â€¢ Ask for help in office hours if needed\n",
      "  â€¢ Clear GPU memory: torch.cuda.empty_cache()\n",
      "\n",
      "âœ“ You're ready to begin your AI project!\n",
      "  Good luck with your creative AI development!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYour HP AI Studio environment is configured and ready.\")\n",
    "print(\"All core dependencies are installed and tested.\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(\"\\nâœ“ GPU: Detected and functional\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ GPU: Not detected (using CPU mode)\")\n",
    "\n",
    "if hf_authenticated:\n",
    "    print(\"âœ“ Hugging Face: Authenticated\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Hugging Face: Not authenticated (optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS FOR YOUR AI PROJECT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. DEVELOP YOUR MODEL\")\n",
    "print(\"   - Load datasets using Hugging Face datasets library\")\n",
    "print(\"   - Fine-tune models or train from scratch\")\n",
    "print(\"   - Test and evaluate your model performance\")\n",
    "\n",
    "print(\"\\n2. SAVE YOUR MODEL\")\n",
    "print(\"   - Use torch.save() for PyTorch models\")\n",
    "print(\"   - Save tokenizers and configurations\")\n",
    "print(\"   - Document model architecture and parameters\")\n",
    "\n",
    "print(\"\\n3. REGISTER FOR DEPLOYMENT\")\n",
    "print(\"   - Open Register_Model.ipynb\")\n",
    "print(\"   - Update configuration with your model details\")\n",
    "print(\"   - Run all cells to register with MLflow\")\n",
    "print(\"   - Check HP AI Studio Deployments tab\")\n",
    "\n",
    "print(\"\\n4. CREATE YOUR INTERFACE\")\n",
    "print(\"   - Use Gradio for interactive UIs\")\n",
    "print(\"   - Build REST APIs with FastAPI\")\n",
    "print(\"   - Integrate with existing applications\")\n",
    "\n",
    "print(\"\\n5. DOCUMENT YOUR WORK\")\n",
    "print(\"   - Keep a development journal\")\n",
    "print(\"   - Screenshot important results\")\n",
    "print(\"   - Record process and iterations\")\n",
    "print(\"   - Prepare portfolio presentation\")\n",
    "\n",
    "if not hf_authenticated:\n",
    "    print(\"\\nâš ï¸ RECOMMENDATION:\")\n",
    "    print(\"   Run Cell 7 again to set up Hugging Face authentication\")\n",
    "    print(\"   This will give you access to more models and datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HELPFUL RESOURCES:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  â€¢ HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\")\n",
    "print(\"  â€¢ Hugging Face: https://huggingface.co/\")\n",
    "print(\"  â€¢ MLflow Documentation: https://mlflow.org/docs/latest/\")\n",
    "print(\"  â€¢ PyTorch Tutorials: https://pytorch.org/tutorials/\")\n",
    "print(\"  â€¢ Gradio Documentation: https://gradio.app/docs/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REMEMBER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  â€¢ Save your work frequently (Ctrl+S)\")\n",
    "print(\"  â€¢ Document your process in your project journal\")\n",
    "print(\"  â€¢ Test on small datasets before full training\")\n",
    "print(\"  â€¢ Ask for help in office hours if needed\")\n",
    "print(\"  â€¢ Clear GPU memory: torch.cuda.empty_cache()\")\n",
    "\n",
    "print(\"\\nâœ“ You're ready to begin your AI project!\")\n",
    "print(\"  Good luck with your creative AI development!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38510d87-3557-4279-88eb-1ad092ffff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading training data...\n",
      "Current directory: /home/jovyan/local/DeepLearning/models\n",
      "âœ… Found data at: /home/jovyan/local/DeepLearning/synthetic_data/full_10k/training_data.json\n",
      "âœ… Loaded 10000 boards\n",
      "ðŸ“ Total notes: 899,195\n",
      "\n",
      "Ready for Brain AI v2!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CELL 1: LOAD TRAINING DATA\n",
    "# =====================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“‚ Loading training data...\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Try different paths\n",
    "possible_paths = [\n",
    "    './DeepLearning/synthetic_data/full_10k/training_data.json',\n",
    "    '../DeepLearning/synthetic_data/full_10k/training_data.json',\n",
    "    './synthetic_data/full_10k/training_data.json',\n",
    "    '/home/jovyan/local/DeepLearning/synthetic_data/full_10k/training_data.json'\n",
    "]\n",
    "\n",
    "training_data = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"âœ… Found data at: {path}\")\n",
    "        with open(path, 'r') as f:\n",
    "            training_data = json.load(f)\n",
    "        break\n",
    "\n",
    "if training_data:\n",
    "    print(f\"âœ… Loaded {len(training_data)} boards\")\n",
    "    print(f\"ðŸ“ Total notes: {sum(board['total_notes'] for board in training_data):,}\")\n",
    "    print(\"\\nReady for Brain AI v2!\")\n",
    "else:\n",
    "    print(\"âŒ Could not find training data\")\n",
    "    print(\"\\nAvailable files:\")\n",
    "    for item in os.listdir('.'):\n",
    "        print(f\"   - {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "973d7e52-b8d1-4345-a63d-ddbdb1250eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Whisper available for audio processing\n",
      "âœ“ PyPDF2 available for PDF processing\n",
      "âœ“ python-pptx available for PowerPoint processing\n",
      "\n",
      "âœ… PRISM Brain AI v2 loaded successfully!\n",
      "   Ready for Gradio UI\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PRISM BRAIN AI V2 - Multi-Modal Research Analyzer\n",
    "Complete implementation for Jupyter notebook\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import whisper\n",
    "    WHISPER_AVAILABLE = True\n",
    "    print(\"âœ“ Whisper available for audio processing\")\n",
    "except:\n",
    "    WHISPER_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  Whisper not installed (run: pip install openai-whisper)\")\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_AVAILABLE = True\n",
    "    print(\"âœ“ PyPDF2 available for PDF processing\")\n",
    "except:\n",
    "    PDF_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  PyPDF2 not installed (run: pip install PyPDF2)\")\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation\n",
    "    PPTX_AVAILABLE = True\n",
    "    print(\"âœ“ python-pptx available for PowerPoint processing\")\n",
    "except:\n",
    "    PPTX_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  python-pptx not installed (run: pip install python-pptx)\")\n",
    "\n",
    "\n",
    "        return {\n",
    "            'success': True,class PRISMBrainV2:\n",
    "    \"\"\"\n",
    "    PRISM Brain AI v2 - Multi-modal research synthesis\n",
    "    \n",
    "    Features:\n",
    "    - FigJam: Full board analysis (sticky notes + arrows + diagrams)\n",
    "    - Audio: Whisper transcription with tone analysis\n",
    "    - Documents: PDF/PPT/DOCX analysis\n",
    "    - Real-time synthesis and updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data=None, figjam_token=None):\n",
    "        self.training_data = training_data\n",
    "        self.figjam_token = figjam_token\n",
    "        self.patterns = {}\n",
    "        self.projects = {}\n",
    "        \n",
    "        # Initialize Whisper if available\n",
    "        if WHISPER_AVAILABLE:\n",
    "            try:\n",
    "                print(\"ðŸŽ™ï¸  Loading Whisper model...\")\n",
    "                self.whisper_model = whisper.load_model(\"base\")\n",
    "                print(\"âœ“ Whisper model loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Whisper load error: {e}\")\n",
    "                self.whisper_model = None\n",
    "        else:\n",
    "            self.whisper_model = None\n",
    "        \n",
    "        # Learn from training data\n",
    "        if training_data:\n",
    "            self._initialize_from_training()\n",
    "        \n",
    "        print(\"\\nðŸ§  PRISM Brain AI v2 initialized\")\n",
    "        print(\"   âœ“ FigJam: URL input with full board analysis\")\n",
    "        print(\"   âœ“ Audio: File upload with Whisper transcription\")\n",
    "        print(\"   âœ“ Documents: PDF/PPT file upload\")\n",
    "    \n",
    "    def _initialize_from_training(self):\n",
    "        \"\"\"Learn patterns from training data\"\"\"\n",
    "        print(\"ðŸ“š Learning from training data...\")\n",
    "        all_notes = []\n",
    "        for board in self.training_data[:100]:  # Sample for speed\n",
    "            all_notes.extend(board['notes'])\n",
    "        self.patterns['keywords'] = self._learn_keywords(all_notes)\n",
    "        print(f\"   âœ“ Learned patterns from {len(all_notes):,} notes\")\n",
    "    \n",
    "    def _learn_keywords(self, notes):\n",
    "        \"\"\"Learn keywords for content types\"\"\"\n",
    "        keywords = defaultdict(set)\n",
    "        for note in notes:\n",
    "            content_type = note['true_type']\n",
    "            words = note['content'].lower().split()\n",
    "            keywords[content_type].update(words[:3])\n",
    "        return {k: list(v)[:15] for k, v in keywords.items()}\n",
    "    \n",
    "    # =====================================================\n",
    "    # PROJECT MANAGEMENT\n",
    "    # =====================================================\n",
    "    \n",
    "    def create_project(self, project_name: str) -> str:\n",
    "        \"\"\"Create new PRISM project\"\"\"\n",
    "        project_id = hashlib.md5(f\"{project_name}{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "        \n",
    "        self.projects[project_id] = {\n",
    "            'name': project_name,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'sources': [],\n",
    "            'notes': [],\n",
    "            'connections': [],\n",
    "            'diagrams': [],\n",
    "            'timeline': [],\n",
    "            'contributors': {},\n",
    "            'insights': {},\n",
    "            'last_updated': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Created project: {project_name} (ID: {project_id})\")\n",
    "        return project_id\n",
    "    \n",
    "    # =====================================================\n",
    "    # FIGJAM: URL UPLOAD WITH FULL ANALYSIS\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_figjam_url(self, project_id: str, figjam_url: str):\n",
    "        \"\"\"Ingest FigJam board from URL - analyzes everything\"\"\"\n",
    "        print(f\"\\nðŸ“¥ Ingesting FigJam board...\")\n",
    "        \n",
    "        # Extract file key\n",
    "        file_key = self._extract_figjam_key(figjam_url)\n",
    "        if not file_key:\n",
    "            return {'error': 'Invalid FigJam URL format'}\n",
    "        \n",
    "        if not self.figjam_token:\n",
    "            return {'error': 'FigJam token not configured'}\n",
    "        \n",
    "        # Fetch board\n",
    "        board_data = self._fetch_figjam_board(file_key)\n",
    "        if not board_data:\n",
    "            return {'error': 'Could not fetch board from API'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        board_name = board_data.get('name', 'Untitled Board')\n",
    "        \n",
    "        # Extract all elements\n",
    "        sticky_notes = []\n",
    "        arrows = []\n",
    "        shapes = []\n",
    "        \n",
    "        def traverse(node, parent=None):\n",
    "            node_type = node.get('type')\n",
    "            \n",
    "            if node_type == 'STICKY':\n",
    "                sticky_notes.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'content': node.get('characters', ''),\n",
    "                    'color': self._map_color(node),\n",
    "                    'author': node.get('lastModifier', {}).get('name', 'Unknown'),\n",
    "                    'position': node.get('absoluteBoundingBox', {}),\n",
    "                    'parent': parent\n",
    "                })\n",
    "            \n",
    "            elif node_type == 'CONNECTOR':\n",
    "                arrows.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'from': node.get('connectorStart', {}).get('endpointNodeId'),\n",
    "                    'to': node.get('connectorEnd', {}).get('endpointNodeId')\n",
    "                })\n",
    "            \n",
    "            elif node_type in ['RECTANGLE', 'ELLIPSE', 'TEXT']:\n",
    "                shapes.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'type': node_type.lower(),\n",
    "                    'content': node.get('characters', ''),\n",
    "                    'position': node.get('absoluteBoundingBox', {})\n",
    "                })\n",
    "            \n",
    "            if 'children' in node:\n",
    "                for child in node['children']:\n",
    "                    traverse(child, node.get('name') if node_type == 'FRAME' else parent)\n",
    "        \n",
    "        traverse(board_data.get('document', {}))\n",
    "        \n",
    "        print(f\"   âœ“ {len(sticky_notes)} sticky notes\")\n",
    "        print(f\"   âœ“ {len(arrows)} connections\")\n",
    "        print(f\"   âœ“ {len(shapes)} diagrams/shapes\")\n",
    "        \n",
    "        # Analyze notes\n",
    "        analyzed_notes = []\n",
    "        for sticky in sticky_notes:\n",
    "            analysis = self._analyze_content(sticky['content'], sticky['color'])\n",
    "            \n",
    "            note = {\n",
    "                'id': sticky['id'],\n",
    "                'source': 'figjam',\n",
    "                'source_name': board_name,\n",
    "                'content': sticky['content'],\n",
    "                'color': sticky['color'],\n",
    "                'predicted_type': analysis['predicted_type'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                'contributor': sticky['author'],\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'position': sticky['position'],\n",
    "                'sentiment': self._detect_sentiment(sticky['content']),\n",
    "                'priority': self._calc_priority(sticky['content'], analysis),\n",
    "                'tags': self._extract_tags(sticky['content'])\n",
    "            }\n",
    "            analyzed_notes.append(note)\n",
    "        \n",
    "        # Store connections\n",
    "        for arrow in arrows:\n",
    "            project['connections'].append({\n",
    "                'from_note': arrow['from'],\n",
    "                'to_note': arrow['to'],\n",
    "                'relationship': 'connects_to',\n",
    "                'source': 'figjam'\n",
    "            })\n",
    "        \n",
    "        project['diagrams'].extend(shapes)\n",
    "        \n",
    "        # Add to project\n",
    "        project['sources'].append({\n",
    "            'type': 'figjam',\n",
    "            'name': board_name,\n",
    "            'url': figjam_url,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(analyzed_notes),\n",
    "            'connection_count': len(arrows),\n",
    "            'diagram_count': len(shapes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        self._update_contributors(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… FigJam board ingested successfully\")\n",
    "        \n",
    "            'notes': len(analyzed_notes),\n",
    "            'connections': len(arrows),\n",
    "            'diagrams': len(shapes)\n",
    "        }\n",
    "    \n",
    "    def _extract_figjam_key(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extract file key from FigJam URL\"\"\"\n",
    "        match = re.search(r'/board/([a-zA-Z0-9_-]+)', url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        match = re.search(r'/file/([a-zA-Z0-9_-]+)', url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def _fetch_figjam_board(self, file_key: str) -> Optional[Dict]:\n",
    "        \"\"\"Fetch from FigJam API\"\"\"\n",
    "        url = f\"https://api.figma.com/v1/files/{file_key}\"\n",
    "        headers = {\"X-Figma-Token\": self.figjam_token}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"   âŒ API error: {response.status_code}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _map_color(self, node):\n",
    "        \"\"\"Map RGB to color names\"\"\"\n",
    "        fills = node.get('fills', [])\n",
    "        if not fills or fills[0].get('type') != 'SOLID':\n",
    "            return 'YELLOW'\n",
    "        \n",
    "        c = fills[0].get('color', {})\n",
    "        r, g, b = c.get('r', 1), c.get('g', 1), c.get('b', 1)\n",
    "        \n",
    "        if r > 0.8 and g < 0.5 and b < 0.5:\n",
    "            return 'RED'\n",
    "        elif r > 0.8 and g > 0.5 and b < 0.3:\n",
    "            return 'ORANGE'\n",
    "        elif r > 0.8 and g > 0.8 and b < 0.5:\n",
    "            return 'YELLOW'\n",
    "        elif r < 0.5 and g > 0.7 and b < 0.5:\n",
    "            return 'GREEN'\n",
    "        elif r < 0.5 and g < 0.5 and b > 0.8:\n",
    "            return 'BLUE'\n",
    "        elif r > 0.5 and g < 0.5 and b > 0.7:\n",
    "            return 'PURPLE'\n",
    "        elif r > 0.8 and g < 0.5 and b > 0.6:\n",
    "            return 'PINK'\n",
    "        else:\n",
    "            return 'GRAY'\n",
    "    \n",
    "    # =====================================================\n",
    "    # AUDIO: FILE UPLOAD WITH WHISPER\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_audio_file(self, project_id: str, audio_path: str):\n",
    "        \"\"\"Ingest audio file with Whisper transcription\"\"\"\n",
    "        print(f\"\\nðŸŽ™ï¸  Processing audio file...\")\n",
    "        \n",
    "        if not WHISPER_AVAILABLE or not self.whisper_model:\n",
    "            return {'error': 'Whisper not available'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(audio_path)\n",
    "        \n",
    "        print(f\"   Transcribing: {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            result = self.whisper_model.transcribe(\n",
    "                audio_path,\n",
    "                word_timestamps=True,\n",
    "                verbose=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return {'error': f'Transcription failed: {e}'}\n",
    "        \n",
    "        segments = result['segments']\n",
    "        print(f\"   âœ“ Transcribed {len(segments)} segments\")\n",
    "        \n",
    "        # Extract insights\n",
    "        analyzed_notes = []\n",
    "        for i, seg in enumerate(segments):\n",
    "            text = seg['text'].strip()\n",
    "            if len(text) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Detect tone\n",
    "            tone = self._analyze_tone(seg, text)\n",
    "            \n",
    "            # Extract key points\n",
    "            points = self._extract_insights(text)\n",
    "            \n",
    "            for point in points:\n",
    "                analysis = self._analyze_content(point, 'YELLOW')\n",
    "                \n",
    "                note = {\n",
    "                    'id': f\"audio_{file_name}_{i}_{len(analyzed_notes)}\",\n",
    "                    'source': 'audio',\n",
    "                    'source_name': file_name,\n",
    "                    'content': point,\n",
    "                    'full_segment': text,\n",
    "                    'predicted_type': analysis['predicted_type'],\n",
    "                    'confidence': analysis['confidence'],\n",
    "                    'contributor': 'Speaker',\n",
    "                    'created_at': datetime.now().isoformat(),\n",
    "                    'timestamp': f\"{seg['start']:.1f}s\",\n",
    "                    'audio_tone': tone,\n",
    "                    'sentiment': self._detect_sentiment(point),\n",
    "                    'priority': self._calc_priority(point, analysis),\n",
    "                    'tags': self._extract_tags(point)\n",
    "                }\n",
    "                analyzed_notes.append(note)\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'audio',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'duration': f\"{result.get('duration', 0):.1f}s\",\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        self._update_contributors(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(analyzed_notes)} insights\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'notes': len(analyzed_notes),\n",
    "            'duration': result.get('duration', 0)\n",
    "        }\n",
    "    \n",
    "    def _analyze_tone(self, segment, text):\n",
    "        \"\"\"Detect speaker tone\"\"\"\n",
    "        if '!' in text or text.isupper():\n",
    "            return 'emphatic'\n",
    "        elif '?' in text:\n",
    "            return 'questioning'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def _extract_insights(self, text):\n",
    "        \"\"\"Extract key points from text\"\"\"\n",
    "        points = []\n",
    "        \n",
    "        if any(w in text.lower() for w in ['problem', 'issue', 'difficult']):\n",
    "            points.append(f\"Pain point: {text}\")\n",
    "        elif '?' in text:\n",
    "            points.append(f\"Question: {text}\")\n",
    "        elif any(w in text.lower() for w in ['decided', 'agreed', 'will']):\n",
    "            points.append(f\"Decision: {text}\")\n",
    "        elif '\"' in text:\n",
    "            points.append(f\"Quote: {text}\")\n",
    "        else:\n",
    "            points.append(text)\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    # =====================================================\n",
    "    # DOCUMENTS: FILE UPLOAD\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_document_file(self, project_id: str, doc_path: str):\n",
    "        \"\"\"Ingest document file\"\"\"\n",
    "        print(f\"\\nðŸ“„ Processing document...\")\n",
    "        \n",
    "        file_name = os.path.basename(doc_path)\n",
    "        ext = os.path.splitext(file_name)[1].lower()\n",
    "        \n",
    "        if ext == '.pdf':\n",
    "            return self._ingest_pdf(project_id, doc_path)\n",
    "        elif ext in ['.ppt', '.pptx']:\n",
    "            return self._ingest_ppt(project_id, doc_path)\n",
    "        else:\n",
    "            return self._ingest_text(project_id, doc_path)\n",
    "    \n",
    "    def _ingest_pdf(self, project_id: str, pdf_path: str):\n",
    "        \"\"\"Extract from PDF\"\"\"\n",
    "        if not PDF_AVAILABLE:\n",
    "            return {'error': 'PyPDF2 not installed'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        \n",
    "        analyzed_notes = []\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                print(f\"   Processing {len(reader.pages)} pages\")\n",
    "                \n",
    "                for page_num, page in enumerate(reader.pages, 1):\n",
    "                    text = page.extract_text()\n",
    "                    paras = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 50]\n",
    "                    \n",
    "                    for para in paras:\n",
    "                        analysis = self._analyze_content(para, 'YELLOW')\n",
    "                        \n",
    "                        note = {\n",
    "                            'id': f\"pdf_{file_name}_p{page_num}_{len(analyzed_notes)}\",\n",
    "                            'source': 'pdf',\n",
    "                            'source_name': file_name,\n",
    "                            'content': para[:200],\n",
    "                            'full_text': para,\n",
    "                            'predicted_type': analysis['predicted_type'],\n",
    "                            'confidence': analysis['confidence'],\n",
    "                            'contributor': 'Author',\n",
    "                            'created_at': datetime.now().isoformat(),\n",
    "                            'page_number': page_num,\n",
    "                            'sentiment': self._detect_sentiment(para),\n",
    "                            'priority': self._calc_priority(para, analysis),\n",
    "                            'tags': self._extract_tags(para)\n",
    "                        }\n",
    "                        analyzed_notes.append(note)\n",
    "        except Exception as e:\n",
    "            return {'error': f'PDF processing failed: {e}'}\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'pdf',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'pages': len(reader.pages),\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        self._update_contributors(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(analyzed_notes)} insights\")\n",
    "        \n",
    "        return {'success': True, 'notes': len(analyzed_notes)}\n",
    "    \n",
    "    def _ingest_ppt(self, project_id: str, ppt_path: str):\n",
    "        \"\"\"Extract from PowerPoint\"\"\"\n",
    "        if not PPTX_AVAILABLE:\n",
    "            return {'error': 'python-pptx not installed'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(ppt_path)\n",
    "        \n",
    "        analyzed_notes = []\n",
    "        \n",
    "        try:\n",
    "            prs = Presentation(ppt_path)\n",
    "            print(f\"   Processing {len(prs.slides)} slides\")\n",
    "            \n",
    "            for slide_num, slide in enumerate(prs.slides, 1):\n",
    "                text = ' '.join(shape.text for shape in slide.shapes if hasattr(shape, \"text\"))\n",
    "                \n",
    "                if len(text) > 20:\n",
    "                    analysis = self._analyze_content(text, 'YELLOW')\n",
    "                    \n",
    "                    note = {\n",
    "                        'id': f\"ppt_{file_name}_s{slide_num}\",\n",
    "                        'source': 'powerpoint',\n",
    "                        'source_name': file_name,\n",
    "                        'content': text[:200],\n",
    "                        'full_text': text,\n",
    "                        'predicted_type': analysis['predicted_type'],\n",
    "                        'confidence': analysis['confidence'],\n",
    "                        'contributor': 'Presenter',\n",
    "                        'created_at': datetime.now().isoformat(),\n",
    "                        'slide_number': slide_num,\n",
    "                        'sentiment': self._detect_sentiment(text),\n",
    "                        'priority': self._calc_priority(text, analysis),\n",
    "                        'tags': self._extract_tags(text)\n",
    "                    }\n",
    "                    analyzed_notes.append(note)\n",
    "        except Exception as e:\n",
    "            return {'error': f'PPT processing failed: {e}'}\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'powerpoint',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'slides': len(prs.slides),\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(analyzed_notes)} insights\")\n",
    "        \n",
    "        return {'success': True, 'notes': len(analyzed_notes)}\n",
    "    \n",
    "    def _ingest_text(self, project_id: str, text_path: str):\n",
    "        \"\"\"Process text file\"\"\"\n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(text_path)\n",
    "        \n",
    "        with open(text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        analyzed_notes = []\n",
    "        paras = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 50]\n",
    "        \n",
    "        for i, para in enumerate(paras):\n",
    "            analysis = self._analyze_content(para, 'YELLOW')\n",
    "            \n",
    "            note = {\n",
    "                'id': f\"txt_{file_name}_{i}\",\n",
    "                'source': 'document',\n",
    "                'source_name': file_name,\n",
    "                'content': para[:200],\n",
    "                'full_text': para,\n",
    "                'predicted_type': analysis['predicted_type'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                'contributor': 'Author',\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'sentiment': self._detect_sentiment(para),\n",
    "                'priority': self._calc_priority(para, analysis),\n",
    "                'tags': self._extract_tags(para)\n",
    "            }\n",
    "            analyzed_notes.append(note)\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'document',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        \n",
    "        return {'success': True, 'notes': len(analyzed_notes)}\n",
    "    \n",
    "    # =====================================================\n",
    "    # ANALYSIS FUNCTIONS\n",
    "    # =====================================================\n",
    "    \n",
    "    def _analyze_content(self, content: str, color: str) -> Dict:\n",
    "        \"\"\"Analyze content type\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        if '?' in content:\n",
    "            return {'predicted_type': 'question', 'confidence': 0.8}\n",
    "        elif '\"' in content:\n",
    "            return {'predicted_type': 'quote', 'confidence': 0.7}\n",
    "        elif any(w in content_lower for w in ['problem', 'issue', 'error', 'broken']):\n",
    "            return {'predicted_type': 'pain_point', 'confidence': 0.75}\n",
    "        elif any(w in content_lower for w in ['love', 'great', 'awesome', 'excellent']):\n",
    "            return {'predicted_type': 'positive', 'confidence': 0.7}\n",
    "        elif any(w in content_lower for w in ['could', 'should', 'what if', 'idea']):\n",
    "            return {'predicted_type': 'idea', 'confidence': 0.7}\n",
    "        else:\n",
    "            return {'predicted_type': 'neutral', 'confidence': 0.6}\n",
    "    \n",
    "    def _detect_sentiment(self, content: str) -> str:\n",
    "        \"\"\"Detect sentiment\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        pos = sum(1 for w in ['love', 'great', 'good', 'excellent'] if w in content_lower)\n",
    "        neg = sum(1 for w in ['hate', 'bad', 'terrible', 'broken'] if w in content_lower)\n",
    "        return 'positive' if pos > neg else 'negative' if neg > pos else 'neutral'\n",
    "    \n",
    "    def _calc_priority(self, content: str, analysis: Dict) -> str:\n",
    "        \"\"\"Calculate priority\"\"\"\n",
    "        if analysis['predicted_type'] == 'pain_point':\n",
    "            return 'high'\n",
    "        if any(w in content.lower() for w in ['critical', 'urgent', 'blocker']):\n",
    "            return 'high'\n",
    "        if analysis['predicted_type'] == 'neutral':\n",
    "            return 'low'\n",
    "        return 'medium'\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract tags\"\"\"\n",
    "        tags = []\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        tag_map = {\n",
    "            'navigation': ['navigation', 'nav', 'menu'],\n",
    "            'mobile': ['mobile', 'phone'],\n",
    "            'performance': ['slow', 'fast', 'loading'],\n",
    "            'accessibility': ['accessibility', 'a11y'],\n",
    "            'search': ['search', 'find'],\n",
    "            'error': ['error', 'bug', 'broken']\n",
    "        }\n",
    "        \n",
    "        for tag, keywords in tag_map.items():\n",
    "            if any(kw in content_lower for kw in keywords):\n",
    "                tags.append(tag)\n",
    "        \n",
    "        return tags[:3]\n",
    "    \n",
    "    def _update_timeline(self, project: Dict, notes: List[Dict]):\n",
    "        \"\"\"Update timeline\"\"\"\n",
    "        for note in notes:\n",
    "            project['timeline'].append({\n",
    "                'timestamp': note.get('created_at'),\n",
    "                'contributor': note['contributor'],\n",
    "                'content_preview': note['content'][:100],\n",
    "                'note_id': note['id'],\n",
    "                'source': note['source']\n",
    "            })\n",
    "        project['timeline'].sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "    def _update_contributors(self, project: Dict, notes: List[Dict]):\n",
    "        \"\"\"Update contributors\"\"\"\n",
    "        for note in notes:\n",
    "            contributor = note['contributor']\n",
    "            if contributor not in project['contributors']:\n",
    "                project['contributors'][contributor] = {\n",
    "                    'total_contributions': 0,\n",
    "                    'note_types': defaultdict(int)\n",
    "                }\n",
    "            \n",
    "            project['contributors'][contributor]['total_contributions'] += 1\n",
    "            project['contributors'][contributor]['note_types'][note['predicted_type']] += 1\n",
    "    \n",
    "    # =====================================================\n",
    "    # SYNTHESIS\n",
    "    # =====================================================\n",
    "    \n",
    "    def synthesize_project(self, project_id: str) -> Dict:\n",
    "        \"\"\"Generate project synthesis\"\"\"\n",
    "        project = self.projects[project_id]\n",
    "        notes = project['notes']\n",
    "        \n",
    "        by_type = defaultdict(list)\n",
    "        by_priority = defaultdict(list)\n",
    "        \n",
    "        for note in notes:\n",
    "            by_type[note['predicted_type']].append(note)\n",
    "            by_priority[note['priority']].append(note)\n",
    "        \n",
    "        all_tags = []\n",
    "        for note in notes:\n",
    "            all_tags.extend(note.get('tags', []))\n",
    "        \n",
    "        tag_counts = Counter(all_tags)\n",
    "        themes = [\n",
    "            {'name': tag, 'frequency': count, 'percentage': (count/len(notes))*100}\n",
    "            for tag, count in tag_counts.most_common(10)\n",
    "        ]\n",
    "        \n",
    "        action_items = [\n",
    "            {\n",
    "                'content': n['content'], \n",
    "                'type': n['predicted_type'],\n",
    "                'contributor': n['contributor'],\n",
    "                'source': n['source_name']\n",
    "            }\n",
    "            for n in notes if n['priority'] == 'high'\n",
    "        ][:20]\n",
    "        \n",
    "        sentiment_dist = Counter(n.get('sentiment', 'neutral') for n in notes)\n",
    "        \n",
    "        synthesis = {\n",
    "            'project_name': project['name'],\n",
    "            'last_updated': project['last_updated'],\n",
    "            'total_notes': len(notes),\n",
    "            'total_sources': len(project['sources']),\n",
    "            'contributors': len(project['contributors']),\n",
    "            'by_type': dict(by_type),\n",
    "            'by_priority': dict(by_priority),\n",
    "            'by_contributor': project['contributors'],\n",
    "            'timeline': project['timeline'],\n",
    "            'themes': themes,\n",
    "            'action_items': action_items,\n",
    "            'stats': {\n",
    "                'sentiment_distribution': dict(sentiment_dist),\n",
    "                'avg_confidence': sum(n.get('confidence', 0) for n in notes) / len(notes) if notes else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        project['insights'] = synthesis\n",
    "        return synthesis\n",
    "    \n",
    "    def refresh_project(self, project_id: str):\n",
    "        \"\"\"Refresh analysis\"\"\"\n",
    "        return self.synthesize_project(project_id)\n",
    "\n",
    "print(\"\\nâœ… PRISM Brain AI v2 loaded successfully!\")\n",
    "print(\"   Ready for Gradio UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e58f70-7823-4ce7-839f-2c796aa8457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing dependencies...\n",
      "Requirement already satisfied: openai-whisper in /opt/conda/lib/python3.12/site-packages (20250625)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (0.62.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (0.8.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (2.10.0.dev20251107+cu128)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (4.67.0)\n",
      "Requirement already satisfied: triton>=2 in /opt/conda/lib/python3.12/site-packages (from openai-whisper) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.12/site-packages (from triton>=2->openai-whisper) (78.1.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba->openai-whisper) (0.45.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (1.13.1.3)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /opt/conda/lib/python3.12/site-packages (from torch->openai-whisper) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Requirement already satisfied: PyPDF2 in /opt/conda/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-pptx in /opt/conda/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /opt/conda/lib/python3.12/site-packages (from python-pptx) (11.3.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/conda/lib/python3.12/site-packages (from python-pptx) (3.2.9)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from python-pptx) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.12/site-packages (from python-pptx) (4.14.1)\n",
      "\n",
      "âœ… All dependencies installed!\n",
      "   Restart kernel and re-run cells if needed\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# INSTALL DEPENDENCIES\n",
    "# =====================================================\n",
    "\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "\n",
    "!pip install openai-whisper\n",
    "!pip install PyPDF2\n",
    "!pip install python-pptx\n",
    "\n",
    "print(\"\\nâœ… All dependencies installed!\")\n",
    "print(\"   Restart kernel and re-run cells if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286eab77-883b-43a6-98fc-1f86ff286789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Building PRISM UI...\n",
      "ðŸŽ™ï¸  Loading Whisper model...\n",
      "âœ“ Whisper model loaded\n",
      "ðŸ“š Learning from training data...\n",
      "   âœ“ Learned patterns from 9,366 notes\n",
      "\n",
      "ðŸ§  PRISM Brain AI v2 initialized\n",
      "   âœ“ FigJam: URL input with full board analysis\n",
      "   âœ“ Audio: File upload with Whisper transcription\n",
      "   âœ“ Documents: PDF/PPT file upload\n",
      "ðŸš€ Launching PRISM...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://3531af5ff41286a8a5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3531af5ff41286a8a5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# GRADIO UI - PRISM v2\n",
    "# =====================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "print(\"ðŸŽ¨ Building PRISM UI...\")\n",
    "\n",
    "# Initialize Brain with your FigJam token\n",
    "figjam_token = \"figd_YP-yLbvxZ0jOVR9C54bCPveiHdkFB3uZD7hKQKDF\"\n",
    "brain = PRISMBrainV2(training_data, figjam_token)\n",
    "\n",
    "current_project_id = None\n",
    "\n",
    "# =====================================================\n",
    "# UI FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def create_project(name):\n",
    "    global current_project_id\n",
    "    if not name:\n",
    "        return \"âŒ Enter project name\"\n",
    "    current_project_id = brain.create_project(name)\n",
    "    return f\"âœ… Project created: {name}\"\n",
    "\n",
    "def upload_figjam(url):\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    \n",
    "    result = brain.ingest_figjam_url(current_project_id, url)\n",
    "    if 'error' in result:\n",
    "        return f\"âŒ {result['error']}\"\n",
    "    return f\"âœ… FigJam ingested!\\n   Notes: {result['notes']}\\n   Connections: {result['connections']}\"\n",
    "\n",
    "def upload_audio(file):\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    if not file:\n",
    "        return \"âŒ No file\"\n",
    "    \n",
    "    result = brain.ingest_audio_file(current_project_id, file.name)\n",
    "    if 'error' in result:\n",
    "        return f\"âŒ {result['error']}\"\n",
    "    return f\"âœ… Audio processed!\\n   Notes: {result['notes']}\"\n",
    "\n",
    "def upload_doc(file):\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    if not file:\n",
    "        return \"âŒ No file\"\n",
    "    \n",
    "    result = brain.ingest_document_file(current_project_id, file.name)\n",
    "    if 'error' in result:\n",
    "        return f\"âŒ {result['error']}\"\n",
    "    return f\"âœ… Document processed!\\n   Notes: {result['notes']}\"\n",
    "\n",
    "def analyze():\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    \n",
    "    s = brain.synthesize_project(current_project_id)\n",
    "    \n",
    "    output = f\"\"\"\n",
    "ðŸ“Š PRISM ANALYSIS\n",
    "{'='*60}\n",
    "\n",
    "PROJECT: {s['project_name']}\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Total Notes: {s['total_notes']}\n",
    "Sources: {s['total_sources']}\n",
    "Contributors: {s['contributors']}\n",
    "\n",
    "BY TYPE\n",
    "-------\n",
    "\"\"\"\n",
    "    for t, notes in sorted(s['by_type'].items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        output += f\"{t:15} â†’ {len(notes)} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "PRIORITY\n",
    "--------\n",
    "\"\"\"\n",
    "    for p in ['high', 'medium', 'low']:\n",
    "        output += f\"{p:15} â†’ {len(s['by_priority'].get(p, []))} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "TOP THEMES\n",
    "----------\n",
    "\"\"\"\n",
    "    for theme in s['themes'][:5]:\n",
    "        output += f\"{theme['name']:15} â†’ {theme['frequency']} mentions\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# BUILD UI\n",
    "with gr.Blocks(title=\"PRISM v2\") as demo:\n",
    "    gr.Markdown(\"# PRISM - Research Synthesis\\n### Multi-Modal AI Analysis\")\n",
    "    \n",
    "    gr.Markdown(\"## 1. Create Project\")\n",
    "    with gr.Row():\n",
    "        project_name = gr.Textbox(label=\"Project Name\")\n",
    "        create_btn = gr.Button(\"Create\", variant=\"primary\")\n",
    "    create_status = gr.Textbox(label=\"Status\", lines=2)\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## 2. Upload Files\")\n",
    "    \n",
    "    with gr.Tab(\"FigJam URL\"):\n",
    "        figjam_url = gr.Textbox(label=\"FigJam Board URL\")\n",
    "        figjam_btn = gr.Button(\"Upload\")\n",
    "        figjam_status = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Audio File\"):\n",
    "        audio_file = gr.File(label=\"Audio (.mp3, .wav, .mov)\")\n",
    "        audio_btn = gr.Button(\"Process\")\n",
    "        audio_status = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Document\"):\n",
    "        doc_file = gr.File(label=\"Document (.pdf, .pptx, .txt)\")\n",
    "        doc_btn = gr.Button(\"Process\")\n",
    "        doc_status = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## 3. Analyze\")\n",
    "    \n",
    "    analyze_btn = gr.Button(\"ðŸ” Generate Analysis\", variant=\"primary\", size=\"lg\")\n",
    "    analysis_output = gr.Textbox(label=\"Results\", lines=30, show_copy_button=True)\n",
    "    \n",
    "    # Wire up\n",
    "    create_btn.click(create_project, [project_name], [create_status])\n",
    "    figjam_btn.click(upload_figjam, [figjam_url], [figjam_status])\n",
    "    audio_btn.click(upload_audio, [audio_file], [audio_status])\n",
    "    doc_btn.click(upload_doc, [doc_file], [doc_status])\n",
    "    analyze_btn.click(analyze, [], [analysis_output])\n",
    "\n",
    "print(\"ðŸš€ Launching PRISM...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a243d13-d125-43d3-bc82-0276160f041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /opt/conda/lib/python3.12/site-packages (5.49.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.116.0)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.12/site-packages (from gradio) (0.6.4)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.1.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.14.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.14.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.13.3->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.0)\n",
      "Requirement already satisfied: typer-slim in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Gradio\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e04f13-ed25-4139-89c7-6c530de639f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Building PRISM UI...\n",
      "ðŸŽ™ï¸  Loading Whisper model...\n",
      "âœ“ Whisper model loaded\n",
      "ðŸ“š Learning from training data...\n",
      "   âœ“ Learned patterns from 9,366 notes\n",
      "\n",
      "ðŸ§  PRISM Brain AI v2 initialized\n",
      "   âœ“ FigJam: URL input with full board analysis\n",
      "   âœ“ Audio: File upload with Whisper transcription\n",
      "   âœ“ Documents: PDF/PPT file upload\n",
      "ðŸš€ Launching PRISM...\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://c6f64d7c6ea0ab881e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c6f64d7c6ea0ab881e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# GRADIO UI - PRISM v2\n",
    "# =====================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "print(\"ðŸŽ¨ Building PRISM UI...\")\n",
    "\n",
    "# Initialize Brain with your FigJam token\n",
    "figjam_token = \"figd_YP-yLbvxZ0jOVR9C54bCPveiHdkFB3uZD7hKQKDF\"\n",
    "brain = PRISMBrainV2(training_data, figjam_token)\n",
    "\n",
    "current_project_id = None\n",
    "\n",
    "# =====================================================\n",
    "# UI FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def create_project(name):\n",
    "    global current_project_id\n",
    "    if not name:\n",
    "        return \"âŒ Enter project name\"\n",
    "    current_project_id = brain.create_project(name)\n",
    "    return f\"âœ… Project created: {name}\"\n",
    "\n",
    "def upload_figjam(url):\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    \n",
    "    result = brain.ingest_figjam_url(current_project_id, url)\n",
    "    if 'error' in result:\n",
    "        return f\"âŒ {result['error']}\"\n",
    "    return f\"âœ… FigJam ingested!\\n   Notes: {result['notes']}\\n   Connections: {result['connections']}\"\n",
    "\n",
    "def upload_audio(file):\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    if not file:\n",
    "        return \"âŒ No file\"\n",
    "    \n",
    "    result = brain.ingest_audio_file(current_project_id, file.name)\n",
    "    if 'error' in result:\n",
    "        return f\"âŒ {result['error']}\"\n",
    "    return f\"âœ… Audio processed!\\n   Notes: {result['notes']}\"\n",
    "\n",
    "def upload_doc(file):\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    if not file:\n",
    "        return \"âŒ No file\"\n",
    "    \n",
    "    result = brain.ingest_document_file(current_project_id, file.name)\n",
    "    if 'error' in result:\n",
    "        return f\"âŒ {result['error']}\"\n",
    "    return f\"âœ… Document processed!\\n   Notes: {result['notes']}\"\n",
    "\n",
    "def analyze():\n",
    "    global current_project_id\n",
    "    if not current_project_id:\n",
    "        return \"âŒ Create project first\"\n",
    "    \n",
    "    s = brain.synthesize_project(current_project_id)\n",
    "    \n",
    "    output = f\"\"\"\n",
    "ðŸ“Š PRISM ANALYSIS\n",
    "{'='*60}\n",
    "\n",
    "PROJECT: {s['project_name']}\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Total Notes: {s['total_notes']}\n",
    "Sources: {s['total_sources']}\n",
    "Contributors: {s['contributors']}\n",
    "\n",
    "BY TYPE\n",
    "-------\n",
    "\"\"\"\n",
    "    for t, notes in sorted(s['by_type'].items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        output += f\"{t:15} â†’ {len(notes)} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "PRIORITY\n",
    "--------\n",
    "\"\"\"\n",
    "    for p in ['high', 'medium', 'low']:\n",
    "        output += f\"{p:15} â†’ {len(s['by_priority'].get(p, []))} notes\\n\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "TOP THEMES\n",
    "----------\n",
    "\"\"\"\n",
    "    for theme in s['themes'][:5]:\n",
    "        output += f\"{theme['name']:15} â†’ {theme['frequency']} mentions\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# BUILD UI\n",
    "with gr.Blocks(title=\"PRISM v2\") as demo:\n",
    "    gr.Markdown(\"# PRISM - Research Synthesis\\n### Multi-Modal AI Analysis\")\n",
    "    \n",
    "    gr.Markdown(\"## 1. Create Project\")\n",
    "    with gr.Row():\n",
    "        project_name = gr.Textbox(label=\"Project Name\")\n",
    "        create_btn = gr.Button(\"Create\", variant=\"primary\")\n",
    "    create_status = gr.Textbox(label=\"Status\", lines=2)\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## 2. Upload Files\")\n",
    "    \n",
    "    with gr.Tab(\"FigJam URL\"):\n",
    "        figjam_url = gr.Textbox(label=\"FigJam Board URL\")\n",
    "        figjam_btn = gr.Button(\"Upload\")\n",
    "        figjam_status = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Audio File\"):\n",
    "        audio_file = gr.File(label=\"Audio (.mp3, .wav, .mov)\")\n",
    "        audio_btn = gr.Button(\"Process\")\n",
    "        audio_status = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Tab(\"Document\"):\n",
    "        doc_file = gr.File(label=\"Document (.pdf, .pptx, .txt)\")\n",
    "        doc_btn = gr.Button(\"Process\")\n",
    "        doc_status = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## 3. Analyze\")\n",
    "    \n",
    "    analyze_btn = gr.Button(\"ðŸ” Generate Analysis\", variant=\"primary\", size=\"lg\")\n",
    "    analysis_output = gr.Textbox(label=\"Results\", lines=30, show_copy_button=True)\n",
    "    \n",
    "    # Wire up\n",
    "    create_btn.click(create_project, [project_name], [create_status])\n",
    "    figjam_btn.click(upload_figjam, [figjam_url], [figjam_status])\n",
    "    audio_btn.click(upload_audio, [audio_file], [audio_status])\n",
    "    doc_btn.click(upload_doc, [doc_file], [doc_status])\n",
    "    analyze_btn.click(analyze, [], [analysis_output])\n",
    "\n",
    "print(\"ðŸš€ Launching PRISM...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9fc57b1-39dc-4de7-9e87-921287843c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… prism_brain.py created successfully!\n",
      "ðŸ“ You can now create a new notebook and import with:\n",
      "   from prism_brain import PRISMBrainV2\n"
     ]
    }
   ],
   "source": [
    "# Run this in your BRAIN AI V2 notebook to export the brain\n",
    "with open('prism_brain.py', 'w') as f:\n",
    "    f.write('''\"\"\"\n",
    "PRISM BRAIN AI V2 - Multi-Modal Research Analyzer\n",
    "Complete implementation for standalone import\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import whisper\n",
    "    WHISPER_AVAILABLE = True\n",
    "    print(\"âœ“ Whisper available for audio processing\")\n",
    "except:\n",
    "    WHISPER_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  Whisper not installed (run: pip install openai-whisper)\")\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_AVAILABLE = True\n",
    "    print(\"âœ“ PyPDF2 available for PDF processing\")\n",
    "except:\n",
    "    PDF_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  PyPDF2 not installed (run: pip install PyPDF2)\")\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation\n",
    "    PPTX_AVAILABLE = True\n",
    "    print(\"âœ“ python-pptx available for PowerPoint processing\")\n",
    "except:\n",
    "    PPTX_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  python-pptx not installed (run: pip install python-pptx)\")\n",
    "\n",
    "\n",
    "class PRISMBrainV2:\n",
    "    \"\"\"\n",
    "    PRISM Brain AI v2 - Multi-modal research synthesis\n",
    "    \n",
    "    Features:\n",
    "    - FigJam: Full board analysis (sticky notes + arrows + diagrams)\n",
    "    - Audio: Whisper transcription with tone analysis\n",
    "    - Documents: PDF/PPT/DOCX analysis\n",
    "    - Real-time synthesis and updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data=None, figjam_token=None):\n",
    "        self.training_data = training_data\n",
    "        self.figjam_token = figjam_token\n",
    "        self.patterns = {}\n",
    "        self.projects = {}\n",
    "        \n",
    "        # Initialize Whisper if available\n",
    "        if WHISPER_AVAILABLE:\n",
    "            try:\n",
    "                print(\"ðŸŽ™ï¸  Loading Whisper model...\")\n",
    "                self.whisper_model = whisper.load_model(\"base\")\n",
    "                print(\"âœ“ Whisper model loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Whisper load error: {e}\")\n",
    "                self.whisper_model = None\n",
    "        else:\n",
    "            self.whisper_model = None\n",
    "        \n",
    "        # Learn from training data\n",
    "        if training_data:\n",
    "            self._initialize_from_training()\n",
    "        \n",
    "        print(\"\\\\nðŸ§  PRISM Brain AI v2 initialized\")\n",
    "        print(\"   âœ“ FigJam: URL input with full board analysis\")\n",
    "        print(\"   âœ“ Audio: File upload with Whisper transcription\")\n",
    "        print(\"   âœ“ Documents: PDF/PPT file upload\")\n",
    "    \n",
    "    def _initialize_from_training(self):\n",
    "        \"\"\"Learn patterns from training data\"\"\"\n",
    "        print(\"ðŸ“š Learning from training data...\")\n",
    "        all_notes = []\n",
    "        for board in self.training_data[:100]:  # Sample for speed\n",
    "            all_notes.extend(board['notes'])\n",
    "        self.patterns['keywords'] = self._learn_keywords(all_notes)\n",
    "        print(f\"   âœ“ Learned patterns from {len(all_notes):,} notes\")\n",
    "    \n",
    "    def _learn_keywords(self, notes):\n",
    "        \"\"\"Learn keywords for content types\"\"\"\n",
    "        keywords = defaultdict(set)\n",
    "        for note in notes:\n",
    "            content_type = note['true_type']\n",
    "            words = note['content'].lower().split()\n",
    "            keywords[content_type].update(words[:3])\n",
    "        return {k: list(v)[:15] for k, v in keywords.items()}\n",
    "    \n",
    "    # =====================================================\n",
    "    # PROJECT MANAGEMENT\n",
    "    # =====================================================\n",
    "    \n",
    "    def create_project(self, project_name: str) -> str:\n",
    "        \"\"\"Create new PRISM project\"\"\"\n",
    "        project_id = hashlib.md5(f\"{project_name}{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "        \n",
    "        self.projects[project_id] = {\n",
    "            'name': project_name,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'sources': [],\n",
    "            'notes': [],\n",
    "            'connections': [],\n",
    "            'diagrams': [],\n",
    "            'timeline': [],\n",
    "            'contributors': {},\n",
    "            'insights': {},\n",
    "            'last_updated': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Created project: {project_name} (ID: {project_id})\")\n",
    "        return project_id\n",
    "    \n",
    "    # =====================================================\n",
    "    # FIGJAM: URL UPLOAD WITH FULL ANALYSIS\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_figjam_url(self, project_id: str, figjam_url: str):\n",
    "        \"\"\"Ingest FigJam board from URL - analyzes everything\"\"\"\n",
    "        print(f\"\\\\nðŸ“¥ Ingesting FigJam board...\")\n",
    "        \n",
    "        # Extract file key\n",
    "        file_key = self._extract_figjam_key(figjam_url)\n",
    "        if not file_key:\n",
    "            return {'error': 'Invalid FigJam URL format'}\n",
    "        \n",
    "        if not self.figjam_token:\n",
    "            return {'error': 'FigJam token not configured'}\n",
    "        \n",
    "        # Fetch board\n",
    "        board_data = self._fetch_figjam_board(file_key)\n",
    "        if not board_data:\n",
    "            return {'error': 'Could not fetch board from API'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        board_name = board_data.get('name', 'Untitled Board')\n",
    "        \n",
    "        # Extract all elements\n",
    "        sticky_notes = []\n",
    "        arrows = []\n",
    "        shapes = []\n",
    "        \n",
    "        def traverse(node, parent=None):\n",
    "            node_type = node.get('type')\n",
    "            \n",
    "            if node_type == 'STICKY':\n",
    "                sticky_notes.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'content': node.get('characters', ''),\n",
    "                    'color': self._map_color(node),\n",
    "                    'author': node.get('lastModifier', {}).get('name', 'Unknown'),\n",
    "                    'position': node.get('absoluteBoundingBox', {}),\n",
    "                    'parent': parent\n",
    "                })\n",
    "            \n",
    "            elif node_type == 'CONNECTOR':\n",
    "                arrows.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'from': node.get('connectorStart', {}).get('endpointNodeId'),\n",
    "                    'to': node.get('connectorEnd', {}).get('endpointNodeId')\n",
    "                })\n",
    "            \n",
    "            elif node_type in ['RECTANGLE', 'ELLIPSE', 'TEXT']:\n",
    "                shapes.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'type': node_type.lower(),\n",
    "                    'content': node.get('characters', ''),\n",
    "                    'position': node.get('absoluteBoundingBox', {})\n",
    "                })\n",
    "            \n",
    "            if 'children' in node:\n",
    "                for child in node['children']:\n",
    "                    traverse(child, node.get('name') if node_type == 'FRAME' else parent)\n",
    "        \n",
    "        traverse(board_data.get('document', {}))\n",
    "        \n",
    "        print(f\"   âœ“ {len(sticky_notes)} sticky notes\")\n",
    "        print(f\"   âœ“ {len(arrows)} connections\")\n",
    "        print(f\"   âœ“ {len(shapes)} diagrams/shapes\")\n",
    "        \n",
    "        # Analyze notes\n",
    "        analyzed_notes = []\n",
    "        for sticky in sticky_notes:\n",
    "            analysis = self._analyze_content(sticky['content'], sticky['color'])\n",
    "            \n",
    "            note = {\n",
    "                'id': sticky['id'],\n",
    "                'source': 'figjam',\n",
    "                'source_name': board_name,\n",
    "                'content': sticky['content'],\n",
    "                'color': sticky['color'],\n",
    "                'predicted_type': analysis['predicted_type'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                'contributor': sticky['author'],\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'position': sticky['position'],\n",
    "                'sentiment': self._detect_sentiment(sticky['content']),\n",
    "                'priority': self._calc_priority(sticky['content'], analysis),\n",
    "                'tags': self._extract_tags(sticky['content'])\n",
    "            }\n",
    "            analyzed_notes.append(note)\n",
    "        \n",
    "        # Store connections\n",
    "        for arrow in arrows:\n",
    "            project['connections'].append({\n",
    "                'from_note': arrow['from'],\n",
    "                'to_note': arrow['to'],\n",
    "                'relationship': 'connects_to',\n",
    "                'source': 'figjam'\n",
    "            })\n",
    "        \n",
    "        project['diagrams'].extend(shapes)\n",
    "        \n",
    "        # Add to project\n",
    "        project['sources'].append({\n",
    "            'type': 'figjam',\n",
    "            'name': board_name,\n",
    "            'url': figjam_url,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(analyzed_notes),\n",
    "            'connection_count': len(arrows),\n",
    "            'diagram_count': len(shapes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        self._update_contributors(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… FigJam board ingested successfully\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'notes': len(analyzed_notes),\n",
    "            'connections': len(arrows),\n",
    "            'diagrams': len(shapes)\n",
    "        }\n",
    "    \n",
    "    def _extract_figjam_key(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extract file key from FigJam URL\"\"\"\n",
    "        match = re.search(r'/board/([a-zA-Z0-9_-]+)', url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        match = re.search(r'/file/([a-zA-Z0-9_-]+)', url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def _fetch_figjam_board(self, file_key: str) -> Optional[Dict]:\n",
    "        \"\"\"Fetch from FigJam API\"\"\"\n",
    "        url = f\"https://api.figma.com/v1/files/{file_key}\"\n",
    "        headers = {\"X-Figma-Token\": self.figjam_token}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"   âŒ API error: {response.status_code}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _map_color(self, node):\n",
    "        \"\"\"Map RGB to color names\"\"\"\n",
    "        fills = node.get('fills', [])\n",
    "        if not fills or fills[0].get('type') != 'SOLID':\n",
    "            return 'YELLOW'\n",
    "        \n",
    "        c = fills[0].get('color', {})\n",
    "        r, g, b = c.get('r', 1), c.get('g', 1), c.get('b', 1)\n",
    "        \n",
    "        if r > 0.8 and g < 0.5 and b < 0.5:\n",
    "            return 'RED'\n",
    "        elif r > 0.8 and g > 0.5 and b < 0.3:\n",
    "            return 'ORANGE'\n",
    "        elif r > 0.8 and g > 0.8 and b < 0.5:\n",
    "            return 'YELLOW'\n",
    "        elif r < 0.5 and g > 0.7 and b < 0.5:\n",
    "            return 'GREEN'\n",
    "        elif r < 0.5 and g < 0.5 and b > 0.8:\n",
    "            return 'BLUE'\n",
    "        elif r > 0.5 and g < 0.5 and b > 0.7:\n",
    "            return 'PURPLE'\n",
    "        elif r > 0.8 and g < 0.5 and b > 0.6:\n",
    "            return 'PINK'\n",
    "        else:\n",
    "            return 'GRAY'\n",
    "    \n",
    "    # =====================================================\n",
    "    # AUDIO: FILE UPLOAD WITH WHISPER\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_audio_file(self, project_id: str, audio_path: str):\n",
    "        \"\"\"Ingest audio file with Whisper transcription\"\"\"\n",
    "        print(f\"\\\\nðŸŽ™ï¸  Processing audio file...\")\n",
    "        \n",
    "        if not WHISPER_AVAILABLE or not self.whisper_model:\n",
    "            return {'error': 'Whisper not available'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(audio_path)\n",
    "        \n",
    "        print(f\"   Transcribing: {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            result = self.whisper_model.transcribe(\n",
    "                audio_path,\n",
    "                word_timestamps=True,\n",
    "                verbose=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return {'error': f'Transcription failed: {e}'}\n",
    "        \n",
    "        segments = result['segments']\n",
    "        print(f\"   âœ“ Transcribed {len(segments)} segments\")\n",
    "        \n",
    "        # Extract insights\n",
    "        analyzed_notes = []\n",
    "        for i, seg in enumerate(segments):\n",
    "            text = seg['text'].strip()\n",
    "            if len(text) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Detect tone\n",
    "            tone = self._analyze_tone(seg, text)\n",
    "            \n",
    "            # Extract key points\n",
    "            points = self._extract_insights(text)\n",
    "            \n",
    "            for point in points:\n",
    "                analysis = self._analyze_content(point, 'YELLOW')\n",
    "                \n",
    "                note = {\n",
    "                    'id': f\"audio_{file_name}_{i}_{len(analyzed_notes)}\",\n",
    "                    'source': 'audio',\n",
    "                    'source_name': file_name,\n",
    "                    'content': point,\n",
    "                    'full_segment': text,\n",
    "                    'predicted_type': analysis['predicted_type'],\n",
    "                    'confidence': analysis['confidence'],\n",
    "                    'contributor': 'Speaker',\n",
    "                    'created_at': datetime.now().isoformat(),\n",
    "                    'timestamp': f\"{seg['start']:.1f}s\",\n",
    "                    'audio_tone': tone,\n",
    "                    'sentiment': self._detect_sentiment(point),\n",
    "                    'priority': self._calc_priority(point, analysis),\n",
    "                    'tags': self._extract_tags(point)\n",
    "                }\n",
    "                analyzed_notes.append(note)\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'audio',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'duration': f\"{result.get('duration', 0):.1f}s\",\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        self._update_contributors(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(analyzed_notes)} insights\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'notes': len(analyzed_notes),\n",
    "            'duration': result.get('duration', 0)\n",
    "        }\n",
    "    \n",
    "    def _analyze_tone(self, segment, text):\n",
    "        \"\"\"Detect speaker tone\"\"\"\n",
    "        if '!' in text or text.isupper():\n",
    "            return 'emphatic'\n",
    "        elif '?' in text:\n",
    "            return 'questioning'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def _extract_insights(self, text):\n",
    "        \"\"\"Extract key points from text\"\"\"\n",
    "        points = []\n",
    "        \n",
    "        if any(w in text.lower() for w in ['problem', 'issue', 'difficult']):\n",
    "            points.append(f\"Pain point: {text}\")\n",
    "        elif '?' in text:\n",
    "            points.append(f\"Question: {text}\")\n",
    "        elif any(w in text.lower() for w in ['decided', 'agreed', 'will']):\n",
    "            points.append(f\"Decision: {text}\")\n",
    "        elif '\"' in text:\n",
    "            points.append(f\"Quote: {text}\")\n",
    "        else:\n",
    "            points.append(text)\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    # =====================================================\n",
    "    # DOCUMENTS: FILE UPLOAD\n",
    "    # =====================================================\n",
    "    \n",
    "    def ingest_document_file(self, project_id: str, doc_path: str):\n",
    "        \"\"\"Ingest document file\"\"\"\n",
    "        print(f\"\\\\nðŸ“„ Processing document...\")\n",
    "        \n",
    "        file_name = os.path.basename(doc_path)\n",
    "        ext = os.path.splitext(file_name)[1].lower()\n",
    "        \n",
    "        if ext == '.pdf':\n",
    "            return self._ingest_pdf(project_id, doc_path)\n",
    "        elif ext in ['.ppt', '.pptx']:\n",
    "            return self._ingest_ppt(project_id, doc_path)\n",
    "        else:\n",
    "            return self._ingest_text(project_id, doc_path)\n",
    "    \n",
    "    def _ingest_pdf(self, project_id: str, pdf_path: str):\n",
    "        \"\"\"Extract from PDF\"\"\"\n",
    "        if not PDF_AVAILABLE:\n",
    "            return {'error': 'PyPDF2 not installed'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        \n",
    "        analyzed_notes = []\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                print(f\"   Processing {len(reader.pages)} pages\")\n",
    "                \n",
    "                for page_num, page in enumerate(reader.pages, 1):\n",
    "                    text = page.extract_text()\n",
    "                    paras = [p.strip() for p in text.split('\\\\n\\\\n') if len(p.strip()) > 50]\n",
    "                    \n",
    "                    for para in paras:\n",
    "                        analysis = self._analyze_content(para, 'YELLOW')\n",
    "                        \n",
    "                        note = {\n",
    "                            'id': f\"pdf_{file_name}_p{page_num}_{len(analyzed_notes)}\",\n",
    "                            'source': 'pdf',\n",
    "                            'source_name': file_name,\n",
    "                            'content': para[:200],\n",
    "                            'full_text': para,\n",
    "                            'predicted_type': analysis['predicted_type'],\n",
    "                            'confidence': analysis['confidence'],\n",
    "                            'contributor': 'Author',\n",
    "                            'created_at': datetime.now().isoformat(),\n",
    "                            'page_number': page_num,\n",
    "                            'sentiment': self._detect_sentiment(para),\n",
    "                            'priority': self._calc_priority(para, analysis),\n",
    "                            'tags': self._extract_tags(para)\n",
    "                        }\n",
    "                        analyzed_notes.append(note)\n",
    "        except Exception as e:\n",
    "            return {'error': f'PDF processing failed: {e}'}\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'pdf',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'pages': len(reader.pages),\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        project['last_updated'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        self._update_contributors(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(analyzed_notes)} insights\")\n",
    "        \n",
    "        return {'success': True, 'notes': len(analyzed_notes)}\n",
    "    \n",
    "    def _ingest_ppt(self, project_id: str, ppt_path: str):\n",
    "        \"\"\"Extract from PowerPoint\"\"\"\n",
    "        if not PPTX_AVAILABLE:\n",
    "            return {'error': 'python-pptx not installed'}\n",
    "        \n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(ppt_path)\n",
    "        \n",
    "        analyzed_notes = []\n",
    "        \n",
    "        try:\n",
    "            prs = Presentation(ppt_path)\n",
    "            print(f\"   Processing {len(prs.slides)} slides\")\n",
    "            \n",
    "            for slide_num, slide in enumerate(prs.slides, 1):\n",
    "                text = ' '.join(shape.text for shape in slide.shapes if hasattr(shape, \"text\"))\n",
    "                \n",
    "                if len(text) > 20:\n",
    "                    analysis = self._analyze_content(text, 'YELLOW')\n",
    "                    \n",
    "                    note = {\n",
    "                        'id': f\"ppt_{file_name}_s{slide_num}\",\n",
    "                        'source': 'powerpoint',\n",
    "                        'source_name': file_name,\n",
    "                        'content': text[:200],\n",
    "                        'full_text': text,\n",
    "                        'predicted_type': analysis['predicted_type'],\n",
    "                        'confidence': analysis['confidence'],\n",
    "                        'contributor': 'Presenter',\n",
    "                        'created_at': datetime.now().isoformat(),\n",
    "                        'slide_number': slide_num,\n",
    "                        'sentiment': self._detect_sentiment(text),\n",
    "                        'priority': self._calc_priority(text, analysis),\n",
    "                        'tags': self._extract_tags(text)\n",
    "                    }\n",
    "                    analyzed_notes.append(note)\n",
    "        except Exception as e:\n",
    "            return {'error': f'PPT processing failed: {e}'}\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'powerpoint',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'slides': len(prs.slides),\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(analyzed_notes)} insights\")\n",
    "        \n",
    "        return {'success': True, 'notes': len(analyzed_notes)}\n",
    "    \n",
    "    def _ingest_text(self, project_id: str, text_path: str):\n",
    "        \"\"\"Process text file\"\"\"\n",
    "        project = self.projects[project_id]\n",
    "        file_name = os.path.basename(text_path)\n",
    "        \n",
    "        with open(text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        analyzed_notes = []\n",
    "        paras = [p.strip() for p in text.split('\\\\n\\\\n') if len(p.strip()) > 50]\n",
    "        \n",
    "        for i, para in enumerate(paras):\n",
    "            analysis = self._analyze_content(para, 'YELLOW')\n",
    "            \n",
    "            note = {\n",
    "                'id': f\"txt_{file_name}_{i}\",\n",
    "                'source': 'document',\n",
    "                'source_name': file_name,\n",
    "                'content': para[:200],\n",
    "                'full_text': para,\n",
    "                'predicted_type': analysis['predicted_type'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                'contributor': 'Author',\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'sentiment': self._detect_sentiment(para),\n",
    "                'priority': self._calc_priority(para, analysis),\n",
    "                'tags': self._extract_tags(para)\n",
    "            }\n",
    "            analyzed_notes.append(note)\n",
    "        \n",
    "        project['sources'].append({\n",
    "            'type': 'document',\n",
    "            'name': file_name,\n",
    "            'added_at': datetime.now().isoformat(),\n",
    "            'note_count': len(analyzed_notes)\n",
    "        })\n",
    "        \n",
    "        project['notes'].extend(analyzed_notes)\n",
    "        self._update_timeline(project, analyzed_notes)\n",
    "        \n",
    "        return {'success': True, 'notes': len(analyzed_notes)}\n",
    "    \n",
    "    # =====================================================\n",
    "    # ANALYSIS FUNCTIONS\n",
    "    # =====================================================\n",
    "    \n",
    "    def _analyze_content(self, content: str, color: str) -> Dict:\n",
    "        \"\"\"Analyze content type\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        if '?' in content:\n",
    "            return {'predicted_type': 'question', 'confidence': 0.8}\n",
    "        elif '\"' in content:\n",
    "            return {'predicted_type': 'quote', 'confidence': 0.7}\n",
    "        elif any(w in content_lower for w in ['problem', 'issue', 'error', 'broken']):\n",
    "            return {'predicted_type': 'pain_point', 'confidence': 0.75}\n",
    "        elif any(w in content_lower for w in ['love', 'great', 'awesome', 'excellent']):\n",
    "            return {'predicted_type': 'positive', 'confidence': 0.7}\n",
    "        elif any(w in content_lower for w in ['could', 'should', 'what if', 'idea']):\n",
    "            return {'predicted_type': 'idea', 'confidence': 0.7}\n",
    "        else:\n",
    "            return {'predicted_type': 'neutral', 'confidence': 0.6}\n",
    "    \n",
    "    def _detect_sentiment(self, content: str) -> str:\n",
    "        \"\"\"Detect sentiment\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        pos = sum(1 for w in ['love', 'great', 'good', 'excellent'] if w in content_lower)\n",
    "        neg = sum(1 for w in ['hate', 'bad', 'terrible', 'broken'] if w in content_lower)\n",
    "        return 'positive' if pos > neg else 'negative' if neg > pos else 'neutral'\n",
    "    \n",
    "    def _calc_priority(self, content: str, analysis: Dict) -> str:\n",
    "        \"\"\"Calculate priority\"\"\"\n",
    "        if analysis['predicted_type'] == 'pain_point':\n",
    "            return 'high'\n",
    "        if any(w in content.lower() for w in ['critical', 'urgent', 'blocker']):\n",
    "            return 'high'\n",
    "        if analysis['predicted_type'] == 'neutral':\n",
    "            return 'low'\n",
    "        return 'medium'\n",
    "    \n",
    "    def _extract_tags(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract tags\"\"\"\n",
    "        tags = []\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        tag_map = {\n",
    "            'navigation': ['navigation', 'nav', 'menu'],\n",
    "            'mobile': ['mobile', 'phone'],\n",
    "            'performance': ['slow', 'fast', 'loading'],\n",
    "            'accessibility': ['accessibility', 'a11y'],\n",
    "            'search': ['search', 'find'],\n",
    "            'error': ['error', 'bug', 'broken']\n",
    "        }\n",
    "        \n",
    "        for tag, keywords in tag_map.items():\n",
    "            if any(kw in content_lower for kw in keywords):\n",
    "                tags.append(tag)\n",
    "        \n",
    "        return tags[:3]\n",
    "    \n",
    "    def _update_timeline(self, project: Dict, notes: List[Dict]):\n",
    "        \"\"\"Update timeline\"\"\"\n",
    "        for note in notes:\n",
    "            project['timeline'].append({\n",
    "                'timestamp': note.get('created_at'),\n",
    "                'contributor': note['contributor'],\n",
    "                'content_preview': note['content'][:100],\n",
    "                'note_id': note['id'],\n",
    "                'source': note['source']\n",
    "            })\n",
    "        project['timeline'].sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "    def _update_contributors(self, project: Dict, notes: List[Dict]):\n",
    "        \"\"\"Update contributors\"\"\"\n",
    "        for note in notes:\n",
    "            contributor = note['contributor']\n",
    "            if contributor not in project['contributors']:\n",
    "                project['contributors'][contributor] = {\n",
    "                    'total_contributions': 0,\n",
    "                    'note_types': defaultdict(int)\n",
    "                }\n",
    "            \n",
    "            project['contributors'][contributor]['total_contributions'] += 1\n",
    "            project['contributors'][contributor]['note_types'][note['predicted_type']] += 1\n",
    "    \n",
    "    # =====================================================\n",
    "    # SYNTHESIS\n",
    "    # =====================================================\n",
    "    \n",
    "    def synthesize_project(self, project_id: str) -> Dict:\n",
    "        \"\"\"Generate project synthesis\"\"\"\n",
    "        project = self.projects[project_id]\n",
    "        notes = project['notes']\n",
    "        \n",
    "        by_type = defaultdict(list)\n",
    "        by_priority = defaultdict(list)\n",
    "        \n",
    "        for note in notes:\n",
    "            by_type[note['predicted_type']].append(note)\n",
    "            by_priority[note['priority']].append(note)\n",
    "        \n",
    "        all_tags = []\n",
    "        for note in notes:\n",
    "            all_tags.extend(note.get('tags', []))\n",
    "        \n",
    "        tag_counts = Counter(all_tags)\n",
    "        themes = [\n",
    "            {'name': tag, 'frequency': count, 'percentage': (count/len(notes))*100}\n",
    "            for tag, count in tag_counts.most_common(10)\n",
    "        ]\n",
    "        \n",
    "        action_items = [\n",
    "            {\n",
    "                'content': n['content'], \n",
    "                'type': n['predicted_type'],\n",
    "                'contributor': n['contributor'],\n",
    "                'source': n['source_name']\n",
    "            }\n",
    "            for n in notes if n['priority'] == 'high'\n",
    "        ][:20]\n",
    "        \n",
    "        sentiment_dist = Counter(n.get('sentiment', 'neutral') for n in notes)\n",
    "        \n",
    "        synthesis = {\n",
    "            'project_name': project['name'],\n",
    "            'last_updated': project['last_updated'],\n",
    "            'total_notes': len(notes),\n",
    "            'total_sources': len(project['sources']),\n",
    "            'contributors': len(project['contributors']),\n",
    "            'by_type': dict(by_type),\n",
    "            'by_priority': dict(by_priority),\n",
    "            'by_contributor': project['contributors'],\n",
    "            'timeline': project['timeline'],\n",
    "            'themes': themes,\n",
    "            'action_items': action_items,\n",
    "            'stats': {\n",
    "                'sentiment_distribution': dict(sentiment_dist),\n",
    "                'avg_confidence': sum(n.get('confidence', 0) for n in notes) / len(notes) if notes else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        project['insights'] = synthesis\n",
    "        return synthesis\n",
    "    \n",
    "    def refresh_project(self, project_id: str):\n",
    "        \"\"\"Refresh analysis\"\"\"\n",
    "        return self.synthesize_project(project_id)\n",
    "\n",
    "print(\"âœ… PRISM Brain AI v2 module loaded successfully!\")\n",
    "''')\n",
    "\n",
    "print(\"âœ… prism_brain.py created successfully!\")\n",
    "print(\"ðŸ“ You can now create a new notebook and import with:\")\n",
    "print(\"   from prism_brain import PRISMBrainV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01311386-b98f-42c6-8982-889f2d28c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOOKING FOR TOKEN CONFIGURATION ===\n",
      "\n",
      " 46 |     - FigJam: Full board analysis (sticky notes + arrows + diagrams)\n",
      " 52 |     def __init__(self, training_data=None, figjam_token=None):\n",
      " 54 |         self.figjam_token = figjam_token\n",
      " 75 |         print(\"   âœ“ FigJam: URL input with full board analysis\")\n"
     ]
    }
   ],
   "source": [
    "with open('prism_brain.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"=== LOOKING FOR TOKEN CONFIGURATION ===\\n\")\n",
    "for i, line in enumerate(lines[:100], 1):  # Check first 100 lines\n",
    "    if any(keyword in line.lower() for keyword in ['token', 'figma', 'figjam', 'api_key', 'auth']):\n",
    "        print(f\"{i:3d} | {line.rstrip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6285a-4c57-4583-9b18-fb1ab9334d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
